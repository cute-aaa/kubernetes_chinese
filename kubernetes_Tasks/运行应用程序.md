# 运行应用程序

## 使用部署(Deployment)运行无状态应用程序

本文介绍如何通过Kubernetes Deployment对象去运行一个应用.

### 教程目标

-   创建一个nginx deployment.
-   使用kubectl列举关于deployment信息.
-   更新deployment.

### 准备工作

你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

### 创建和探究一个nginx deployment

你可以通过创建一个Kubernetes Deployment对象来运行一个应用, 可以在一个YAML文件中描述Deployment. 例如, 下面这个YAML文件描述了一个运行nginx:1.7.9 Docker镜像的Deployment:

[`deployment.yaml docs/tasks/run-application`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/run-application/deployment.yaml)

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2 # tells deployment to run 2 pods matching the template
  template: # create pods using pod definition in this template
    metadata:
      # unlike pod-nginx.yaml, the name is not included in the meta data as a unique name is
      # generated from the deployment name
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

1.  通过YAML文件创建一个Deployment:

    kubectl create -f <https://k8s.io/docs/tasks/run-application/deployment.yaml>

2.  展示Deployment相关信息:

    kubectl describe deployment nginx-deployment

    ```
    user@computer:~/website$ kubectl describe deployment nginx-deployment
    Name:     nginx-deployment
    Namespace:    default
    CreationTimestamp:  Tue, 30 Aug 2016 18:11:37 -0700
    Labels:     app=nginx
    Annotations:    deployment.kubernetes.io/revision=1
    Selector:   app=nginx
    Replicas:   2 desired | 2 updated | 2 total | 2 available | 0 unavailable
    StrategyType:   RollingUpdate
    MinReadySeconds:  0
    RollingUpdateStrategy:  1 max unavailable, 1 max surge
    Pod Template:
      Labels:       app=nginx
      Containers:
       nginx:
        Image:              nginx:1.7.9
        Port:               80/TCP
        Environment:        <none>
        Mounts:             <none>
      Volumes:              <none>
    Conditions:
      Type          Status  Reason
      ----          ------  ------
      Available     True    MinimumReplicasAvailable
      Progressing   True    NewReplicaSetAvailable
    OldReplicaSets:   <none>
    NewReplicaSet:    nginx-deployment-1771418926 (2/2 replicas created)
    No events.
    ```

3.  列出deployment创建的pods:

    kubectl get pods -l app=nginx

    ```
    NAME                                READY     STATUS    RESTARTS   AGE
    nginx-deployment-1771418926-7o5ns   1/1       Running   0          16h
    nginx-deployment-1771418926-r18az   1/1       Running   0          16h
    ```

4.  展示某一个pod信息:

    kubectl describe pod

    该处 `<pod-name>` 指某一pod的名称.

### 更新deployment

你可以通过更新一个新的YAML文件来更新deployment. 下面的YAML文件指定该deployment镜像更新为nginx 1.8.

[`deployment-update.yaml docs/tasks/run-application`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/run-application/deployment-update.yaml)

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.8 # Update the version of nginx from 1.7.9 to 1.8
        ports:
        - containerPort: 80
```

1.  应用新的YAML:

    kubectl apply -f <https://k8s.io/docs/tutorials/stateless-application/deployment-update.yaml>

2.  查看该deployment创建的pods以新的名称同时删除旧的pods:

    kubectl get pods -l app=nginx

### 通过增加副本数来弹缩应用

你可以通过应用新的YAML文件来增加Deployment中pods的数量. 该YAML文件将`replicas`设置为4, 指定该Deployment应有4个pods:

[`deployment-scale.yaml docs/tasks/run-application`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/run-application/deployment-scale.yaml)

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 4 # Update the replicas from 2 to 4
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.8
        ports:
        - containerPort: 80
```

1.  应用新的YAML文件:

    kubectl apply -f <https://k8s.io/docs/tutorials/stateless-application/deployment-scale.yaml>

2.  验证Deployment有4个pods:

    kubectl get pods -l app=nginx

    输出的结果类似于:

    ```
    NAME                               READY     STATUS    RESTARTS   AGE
    nginx-deployment-148880595-4zdqq   1/1       Running   0          25s
    nginx-deployment-148880595-6zgi1   1/1       Running   0          25s
    nginx-deployment-148880595-fxcez   1/1       Running   0          2m
    nginx-deployment-148880595-rwovn   1/1       Running   0          2m
    ```

### 删除deployment

通过名称删除deployment:

```
kubectl delete deployment nginx-deployment
```

### ReplicationControllers – 旧的方式

创建一个多副本应用首选方法是使用Deployment,反过来使用ReplicaSet. 在Deployment和ReplicaSet加入到Kubernetes之前, 多副本应用通过[ReplicationController](https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/)来配置.

### 接下来

-   了解更多 [Deployment objects](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/).





## 运行一个单例有状态应用程序

本文介绍在Kubernetes中使用PersistentVolume和Deployment如何运行一个单实例有状态应用. 该应用是MySQL.

### 教程目标

-   在环境中通过磁盘创建一个PersistentVolume.
-   创建一个MySQL Deployment.
-   在集群内以一个已知的DNS名将MySQL暴露给其他pods.

### 准备工作

-   一个集群。

-   为了数据持久性我们将在环境上通过磁盘创建一个持久卷. 环境支持的类型见这里[here](https://kubernetes.io/docs/user-guide/persistent-volumes/#types-of-persistent-volumes). 本篇文档将介绍 `GCEPersistentDisk` . `GCEPersistentDisk`卷只能工作在Google Compute Engine平台上.

### 在环境中设置一个磁盘

你可以为有状态的应用使用任何类型的持久卷. 有关支持环境的磁盘列表，请参考持久卷类型[Types of Persistent Volumes](https://kubernetes.io/docs/user-guide/persistent-volumes/#types-of-persistent-volumes). 对于Google Compute Engine, 请运行:

```
gcloud compute disks create --size=20GB mysql-disk
```

接下来创建一个指向刚创建的 `mysql-disk`磁盘的PersistentVolume. 下面是一个PersistentVolume的配置文件，它指向上面创建的Compute Engine磁盘:

[`gce-volume.yaml docs/tasks/run-application`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/run-application/gce-volume.yaml)

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: mysql-disk
    fsType: ext4
```

注意`pdName: mysql-disk` 这行与Compute Engine环境中的磁盘名称相匹配. 有关为其 他环境编写PersistentVolume配置文件的详细信息，请参见持久卷[Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/).

创建持久卷:

```
kubectl create -f https://k8s.io/docs/tasks/run-application/gce-volume.yaml
```

### 部署MySQL

通过创建Kubernetes Deployment并使用PersistentVolumeClaim将其连接到现已存在的PersistentVolume上来运行一个有状态的应用. 例如, 下面这个YAML文件描述了一个运行MySQL 并引用PersistentVolumeClaim的Deployment. 该文件定义了一个volume其挂载目录为/var/lib/mysql, 然后创建一个内存为20G的卷的PersistentVolumeClaim. 此申领可以通过任 何符合需求的卷来满足, 在本例中满足上面创建的卷.

注意: 在配置的yaml文件中定义密码的做法是不安全的. 具体安全解决方案请参考 [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/).

[`mysql-deployment.yaml docs/tasks/run-application`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/run-application/mysql-deployment.yaml)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    app: mysql
  clusterIP: None
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ""
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: mysql
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
          # Use secret in real usage
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
```

1.  部署YAML文件中定义的内容:

    kubectl create -f <https://k8s.io/docs/tasks/run-application/mysql-deployment.yaml>

2.  展示Deployment相关信息:

    kubectl describe deployment mysql

    ```
    Name:                 mysql
    Namespace:            default
    CreationTimestamp:    Tue, 01 Nov 2016 11:18:45 -0700
    Labels:               app=mysql
    Annotations:          deployment.kubernetes.io/revision=1
    Selector:             app=mysql
    Replicas:             1 desired | 1 updated | 1 total | 0 available | 1 unavailable
    StrategyType:         Recreate
    MinReadySeconds:      0
    Pod Template:
      Labels:       app=mysql
      Containers:
       mysql:
        Image:      mysql:5.6
        Port:       3306/TCP
        Environment:
          MYSQL_ROOT_PASSWORD:      password
        Mounts:
          /var/lib/mysql from mysql-persistent-storage (rw)
      Volumes:
       mysql-persistent-storage:
        Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
        ClaimName:  mysql-pv-claim
        ReadOnly:   false
    Conditions:
      Type          Status  Reason
      ----          ------  ------
      Available     False   MinimumReplicasUnavailable
      Progressing   True    ReplicaSetUpdated
    OldReplicaSets:       <none>
    NewReplicaSet:        mysql-63082529 (1/1 replicas created)
    Events:
      FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
      ---------    --------    -----    ----                -------------    --------    ------            -------
      33s          33s         1        {deployment-controller }             Normal      ScalingReplicaSet Scaled up replica set mysql-63082529 to 1
    ```

3.  列举出Deployment创建的pods:

    kubectl get pods -l app=mysql

    ```
    NAME                   READY     STATUS    RESTARTS   AGE
    mysql-63082529-2z3ki   1/1       Running   0          3m
    ```

4.  查看持久卷:

    kubectl describe pv mysql-pv

    ```
    Name:            mysql-pv
    Labels:          <none>
    Status:          Bound
    Claim:           default/mysql-pv-claim
    Reclaim Policy:  Retain
    Access Modes:    RWO
    Capacity:        20Gi
    Message:
    Source:
        Type:        GCEPersistentDisk (a Persistent Disk resource in Google Compute Engine)
        PDName:      mysql-disk
        FSType:      ext4
        Partition:   0
        ReadOnly:    false
    No events.
    ```

5.  查看PersistentVolumeClaim:

    kubectl describe pvc mysql-pv-claim

    ```
    Name:         mysql-pv-claim
    Namespace:    default
    Status:       Bound
    Volume:       mysql-pv
    Labels:       <none>
    Capacity:     20Gi
    Access Modes: RWO
    No events.
    ```

### 访问MySQL实例

前面YAML文件中创建了一个允许集群内其他pods访问数据库的服务. 该服务中选项 `clusterIP: None` 让服务DNS名称直接解析为Pod的IP地址. 当在一个服务下只有一个pod 并且不打算增加pods的数量这是最好的.

运行MySQL客户端以连接到服务器:

```
kubectl run -it --rm --image=mysql:5.6 mysql-client -- mysql -h <pod-ip> -p <password>
```

此命令在集群内创建一个新的Pod并运行MySQL客户端,并通过服务将其连接到服务器.如果连接成功,你就知道有状态的MySQL database正处于运行状态.

```
Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false
If you don't see a command prompt, try pressing enter.

mysql>
```

### 更新

Deployment中镜像或其他部分同往常一样可以通过 `kubectl apply` 命令更新. 以下是 特定于有状态应用的一些注意事项:

-   不要弹性伸缩. 弹性伸缩仅适用于单实例应用. 下层的PersistentVolume仅只能挂载一个pod. 对于集群级有状态应用, 请参考StatefulSet文档 [StatefulSet documentation](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/).
-   在Deployment的YAML文件中使用 `strategy:` `type: Recreate` . 该选项指示Kubernetes不使用滚动升级. 滚动升级将无法工作, 由于一次不能运行多个pod. 在更新配置文件 创建一个新的pod前 `Recreate`策略将先停止第一个pod.

### 删除deployment

通过名称删除部署的对象:

```
kubectl delete deployment,svc mysql
kubectl delete pvc mysql-pv-claim
kubectl delete pv mysql-pv
```

如果使用Compute Engine磁盘，也可以使用如下命令:

```
gcloud compute disks delete mysql-disk
```

### 接下来

-   了解更多Deployment对象请参考 [Deployment objects](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/).
-   了解更多Deployment应用请参考 [Deploying applications](https://kubernetes.io/docs/user-guide/deploying-applications/)
-   kubectl run文档请参考[kubectl run documentation](https://kubernetes.io/docs/user-guide/kubectl/v1.6/#run)
-   卷和持久卷请参考[Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) and [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)





## （未翻译）运行副本的有状态应用程序

This page shows how to run a replicated stateful application using a [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) controller. The example is a MySQL single-master topology with multiple slaves running asynchronous replication.

Note that **this is not a production configuration**. In particular, MySQL settings remain on insecure defaults to keep the focus on general patterns for running stateful applications in Kubernetes.

### Objectives

-   Deploy a replicated MySQL topology with a StatefulSet controller.
-   Send MySQL client traffic.
-   Observe resistance to downtime.
-   Scale the StatefulSet up and down.

### Before you begin

-   You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using [Minikube](https://kubernetes.io/docs/setup/minikube), or you can use one of these Kubernetes playgrounds:

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

-   You need to either have a dynamic PersistentVolume provisioner with a default[StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/), or [statically provision PersistentVolumes](https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioning) yourself to satisfy the [PersistentVolumeClaims](https://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims) used here.
-   This tutorial assumes you are familiar with [PersistentVolumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) and [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/), as well as other core concepts like [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod/), [Services](https://kubernetes.io/docs/concepts/services-networking/service/), and [ConfigMaps](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/).
-   Some familiarity with MySQL helps, but this tutorial aims to present general patterns that should be useful for other systems.

### Deploy MySQL

The example MySQL deployment consists of a ConfigMap, two Services, and a StatefulSet.

#### ConfigMap

Create the ConfigMap from the following YAML configuration file:



```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:
  master.cnf: |
    # Apply this config only on the master.
    [mysqld]
    log-bin
  slave.cnf: |
    # Apply this config only on slaves.
    [mysqld]
    super-read-only
```

```shell
kubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml
```

This ConfigMap provides `my.cnf` overrides that let you independently control configuration on the MySQL master and slaves. In this case, you want the master to be able to serve replication logs to slaves and you want slaves to reject any writes that don’t come via replication.

There’s nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it’s initializing, based on information provided by the StatefulSet controller.

#### Services

Create the Services from the following YAML configuration file:



```yaml
# Headless service for stable DNS entries of StatefulSet members.
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
# Client service for connecting to any MySQL instance for reads.
# For writes, you must instead connect to the master: mysql-0.mysql.
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql
```

```shell
kubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml
```

The Headless Service provides a home for the DNS entries that the StatefulSet controller creates for each Pod that’s part of the set. Because the Headless Service is named `mysql`, the Pods are accessible by resolving `<pod-name>.mysql` from within any other Pod in the same Kubernetes cluster and namespace.

The Client Service, called `mysql-read`, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the MySQL master and all slaves.

Note that only read queries can use the load-balanced Client Service. Because there is only one MySQL master, clients should connect directly to the MySQL master Pod (through its DNS entry within the Headless Service) to execute writes.

#### StatefulSet

Finally, create the StatefulSet from the following YAML configuration file:



```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Skip the clone on master (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # Check we can execute queries over TCP (skip-networking is off).
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql

          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info && "x$(<xtrabackup_slave_info)" != "x" ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing slave. (Need to remove the tailing semicolon!)
            cat xtrabackup_slave_info | sed -E 's/;$//g' > change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_slave_info xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from master. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm -f xtrabackup_binlog_info xtrabackup_slave_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi

          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

            echo "Initializing replication from clone position"
            mysql -h 127.0.0.1 \
                  -e "$(<change_master_to.sql.in), \
                          MASTER_HOST='mysql-0.mysql', \
                          MASTER_USER='root', \
                          MASTER_PASSWORD='', \
                          MASTER_CONNECT_RETRY=10; \
                        START SLAVE;" || exit 1
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
          fi

          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

```shell
kubectl apply -f https://k8s.io/examples/application/mysql/mysql-statefulset.yaml
```

You can watch the startup progress by running:

```shell
kubectl get pods -l app=mysql --watch
```

After a while, you should see all 3 Pods become Running:

```
NAME      READY     STATUS    RESTARTS   AGE
mysql-0   2/2       Running   0          2m
mysql-1   2/2       Running   0          1m
mysql-2   2/2       Running   0          1m
```

Press **Ctrl+C** to cancel the watch. If you don’t see any progress, make sure you have a dynamic PersistentVolume provisioner enabled as mentioned in the [prerequisites](https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#before-you-begin).

This manifest uses a variety of techniques for managing stateful Pods as part of a StatefulSet. The next section highlights some of these techniques to explain what happens as the StatefulSet creates Pods.

### Understanding stateful Pod initialization

The StatefulSet controller starts Pods one at a time, in order by their ordinal index. It waits until each Pod reports being Ready before starting the next one.

In addition, the controller assigns each Pod a unique, stable name of the form `<statefulset-name>-<ordinal-index>`, which results in Pods named `mysql-0`, `mysql-1`, and `mysql-2`.

The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication.

#### Generating configuration

Before starting any of the containers in the Pod spec, the Pod first runs any [Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) in the order defined.

The first Init Container, named `init-mysql`, generates special MySQL config files based on the ordinal index.

The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the `hostname` command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called `server-id.cnf` in the MySQL `conf.d`directory. This translates the unique, stable identity provided by the StatefulSet controller into the domain of MySQL server IDs, which require the same properties.

The script in the `init-mysql` container also applies either `master.cnf` or `slave.cnf` from the ConfigMap by copying the contents into `conf.d`. Because the example topology consists of a single MySQL master and any number of slaves, the script simply assigns ordinal `0` to be the master, and everyone else to be slaves. Combined with the StatefulSet controller’s[deployment order guarantee](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees/), this ensures the MySQL master is Ready before creating slaves, so they can begin replicating.

#### Cloning existing data

In general, when a new Pod joins the set as a slave, it must assume the MySQL master might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size.

The second Init Container, named `clone-mysql`, performs a clone operation on a slave Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the master.

MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the MySQL master, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod `N` is Ready before starting Pod `N+1`.

#### Starting replication

After the Init Containers complete successfully, the regular containers run. The MySQL Pods consist of a `mysql` container that runs the actual `mysqld` server, and an `xtrabackup`container that acts as a [sidecar](https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns).

The `xtrabackup` sidecar looks at the cloned data files and determines if it’s necessary to initialize MySQL replication on the slave. If so, it waits for `mysqld` to be ready and then executes the `CHANGE MASTER TO` and `START SLAVE` commands with replication parameters extracted from the XtraBackup clone files.

Once a slave begins replication, it remembers its MySQL master and reconnects automatically if the server restarts or the connection dies. Also, because slaves look for the master at its stable DNS name (`mysql-0.mysql`), they automatically find the master even if it gets a new Pod IP due to being rescheduled.

Lastly, after starting replication, the `xtrabackup` container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.

### Sending client traffic

You can send test queries to the MySQL master (hostname `mysql-0.mysql`) by running a temporary container with the `mysql:5.7` image and running the `mysql` client binary.

```shell
kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\
  mysql -h mysql-0.mysql <<EOF
CREATE DATABASE test;
CREATE TABLE test.messages (message VARCHAR(250));
INSERT INTO test.messages VALUES ('hello');
EOF
```

Use the hostname `mysql-read` to send test queries to any server that reports being Ready:

```shell
kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  mysql -h mysql-read -e "SELECT * FROM test.messages"
```

You should get output like this:

```
Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
```

To demonstrate that the `mysql-read` Service distributes connections across servers, you can run `SELECT @@server_id` in a loop:

```shell
kubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\
  bash -ic "while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done"
```

You should see the reported `@@server_id` change randomly, because a different endpoint might be selected upon each connection attempt:

```
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         100 | 2006-01-02 15:04:05 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         102 | 2006-01-02 15:04:06 |
+-------------+---------------------+
+-------------+---------------------+
| @@server_id | NOW()               |
+-------------+---------------------+
|         101 | 2006-01-02 15:04:07 |
+-------------+---------------------+
```

You can press **Ctrl+C** when you want to stop the loop, but it’s useful to keep it running in another window so you can see the effects of the following steps.

### Simulating Pod and Node downtime

To demonstrate the increased availability of reading from the pool of slaves instead of a single server, keep the `SELECT @@server_id` loop from above running while you force a Pod out of the Ready state.

#### Break the Readiness Probe

The [readiness probe](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes) for the `mysql` container runs the command `mysql -h 127.0.0.1 -e 'SELECT 1'` to make sure the server is up and able to execute queries.

One way to force this readiness probe to fail is to break that command:

```shell
kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off
```

This reaches into the actual container’s filesystem for Pod `mysql-2` and renames the `mysql`command so the readiness probe can’t find it. After a few seconds, the Pod should report one of its containers as not Ready, which you can check by running:

```shell
kubectl get pod mysql-2
```

Look for `1/2` in the `READY` column:

```
NAME      READY     STATUS    RESTARTS   AGE
mysql-2   1/2       Running   0          3m
```

At this point, you should see your `SELECT @@server_id` loop continue to run, although it never reports `102` anymore. Recall that the `init-mysql` script defined `server-id` as `100 + $ordinal`, so server ID `102` corresponds to Pod `mysql-2`.

Now repair the Pod and it should reappear in the loop output after a few seconds:

```shell
kubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql
```

#### Delete Pods

The StatefulSet also recreates Pods if they’re deleted, similar to what a ReplicaSet does for stateless Pods.

```shell
kubectl delete pod mysql-2
```

The StatefulSet controller notices that no `mysql-2` Pod exists anymore, and creates a new one with the same name and linked to the same PersistentVolumeClaim. You should see server ID `102` disappear from the loop output for a while and then return on its own.

#### Drain a Node

If your Kubernetes cluster has multiple Nodes, you can simulate Node downtime (such as when Nodes are upgraded) by issuing a [drain](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#drain).

First determine which Node one of the MySQL Pods is on:

```shell
kubectl get pod mysql-2 -o wide
```

The Node name should show up in the last column:

```
NAME      READY     STATUS    RESTARTS   AGE       IP            NODE
mysql-2   2/2       Running   0          15m       10.244.5.27   kubernetes-node-9l2t
```

Then drain the Node by running the following command, which cordons it so no new Pods may schedule there, and then evicts any existing Pods. Replace `<node-name>` with the name of the Node you found in the last step.

This might impact other applications on the Node, so it’s best to **only do this in a test cluster**.

```shell
kubectl drain <node-name> --force --delete-local-data --ignore-daemonsets
```

Now you can watch as the Pod reschedules on a different Node:

```shell
kubectl get pod mysql-2 -o wide --watch
```

It should look something like this:

```
NAME      READY   STATUS          RESTARTS   AGE       IP            NODE
mysql-2   2/2     Terminating     0          15m       10.244.1.56   kubernetes-node-9l2t
[...]
mysql-2   0/2     Pending         0          0s        <none>        kubernetes-node-fjlm
mysql-2   0/2     Init:0/2        0          0s        <none>        kubernetes-node-fjlm
mysql-2   0/2     Init:1/2        0          20s       10.244.5.32   kubernetes-node-fjlm
mysql-2   0/2     PodInitializing 0          21s       10.244.5.32   kubernetes-node-fjlm
mysql-2   1/2     Running         0          22s       10.244.5.32   kubernetes-node-fjlm
mysql-2   2/2     Running         0          30s       10.244.5.32   kubernetes-node-fjlm
```

And again, you should see server ID `102` disappear from the `SELECT @@server_id` loop output for a while and then return.

Now uncordon the Node to return it to a normal state:

```shell
kubectl uncordon <node-name>
```

### Scaling the number of slaves

With MySQL replication, you can scale your read query capacity by adding slaves. With StatefulSet, you can do this with a single command:

```shell
kubectl scale statefulset mysql  --replicas=5
```

Watch the new Pods come up by running:

```shell
kubectl get pods -l app=mysql --watch
```

Once they’re up, you should see server IDs `103` and `104` start appearing in the `SELECT @@server_id` loop output.

You can also verify that these new servers have the data you added before they existed:

```shell
kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  mysql -h mysql-3.mysql -e "SELECT * FROM test.messages"
Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
```

Scaling back down is also seamless:

```shell
kubectl scale statefulset mysql --replicas=3
```

Note, however, that while scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs. This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them.

You can see this by running:

```shell
kubectl get pvc -l app=mysql
```

Which shows that all 5 PVCs still exist, despite having scaled the StatefulSet down to 3:

```
NAME           STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
data-mysql-0   Bound     pvc-8acbf5dc-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-1   Bound     pvc-8ad39820-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-2   Bound     pvc-8ad69a6d-b103-11e6-93fa-42010a800002   10Gi       RWO           20m
data-mysql-3   Bound     pvc-50043c45-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
data-mysql-4   Bound     pvc-500a9957-b1c5-11e6-93fa-42010a800002   10Gi       RWO           2m
```

If you don’t intend to reuse the extra PVCs, you can delete them:

```shell
kubectl delete pvc data-mysql-3
kubectl delete pvc data-mysql-4
```

### Cleaning up

1.  Cancel the `SELECT @@server_id` loop by pressing **Ctrl+C** in its terminal, or running the following from another terminal:

```shell
   kubectl delete pod mysql-client-loop --now
```

1.  Delete the StatefulSet. This also begins terminating the Pods.

```shell
   kubectl delete statefulset mysql
```

1.  Verify that the Pods disappear. They might take some time to finish terminating.

```shell
   kubectl get pods -l app=mysql
```

You’ll know the Pods have terminated when the above returns:

```
   No resources found.
```

1.  Delete the ConfigMap, Services, and PersistentVolumeClaims.

```shell
   kubectl delete configmap,service,pvc -l app=mysql
```

1.  If you manually provisioned PersistentVolumes, you also need to manually delete them, as well as release the underlying resources. If you used a dynamic provisioner, it automatically deletes the PersistentVolumes when it sees that you deleted the PersistentVolumeClaims. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resources upon deleting the PersistentVolumes.

### What's next

-   Look in the [Helm Charts repository](https://github.com/kubernetes/charts) for other stateful application examples.











## 使用kubectl patch更新API对象

这个任务展示了如何使用 `kubectl patch` 就地更新 API 对象。这个任务中的练习演示了一个策略性合并 patch 和一个 JSON 合并 patch。

### 准备开始

你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://v1-14.docs.kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/) –>

To check the version, enter `kubectl version`.

### 使用策略合并 patch 更新 Deployment

下面是具有两个副本的 Deployment 的配置文件。每个副本是一个 Pod，有一个容器：

[`application/deployment-patch.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/application/deployment-patch.yaml)

```yaml
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: patch-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: patch-demo-ctr
        image: nginx
      tolerations:
      - effect: NoSchedule
        key: dedicated
        value: test-team
```

创建 Deployment：

```shell
kubectl create -f https://k8s.io/examples/application/deployment-patch.yaml
```

查看与 Deployment 相关的 Pod：

```shell
kubectl get pods
```

输出显示 Deployment 有两个 Pod。`1/1` 表示每个 Pod 有一个容器:

```
NAME                        READY     STATUS    RESTARTS   AGE
patch-demo-28633765-670qr   1/1       Running   0          23s
patch-demo-28633765-j5qs3   1/1       Running   0          23s
```

把运行的 Pod 的名字记下来。稍后，您将看到这些 Pod 被终止并被新的 Pod 替换。

此时，每个 Pod 都有一个运行 nginx 镜像的容器。现在假设您希望每个 Pod 有两个容器：一个运行 nginx，另一个运行 redis。

创建一个名为 `patch-file-containers.yaml` 的文件。内容如下:

```yaml
spec:
  template:
    spec:
      containers:
      - name: patch-demo-ctr-2
        image: redis
```

修补您的 Deployment：

```shell
kubectl patch deployment patch-demo --patch "$(cat patch-file-containers.yaml)"
```

查看修补后的 Deployment：

```shell
kubectl get deployment patch-demo --output yaml
```

输出显示 Deployment 中的 PodSpec 有两个容器:

```shell
containers:
- image: redis
  imagePullPolicy: Always
  name: patch-demo-ctr-2
  ...
- image: nginx
  imagePullPolicy: Always
  name: patch-demo-ctr
  ...
```

查看与 patch Deployment 相关的 Pod:

```shell
kubectl get pods
```

输出显示正在运行的 Pod 与以前运行的 Pod 有不同的名称。Deployment 终止了旧的 Pod，并创建了两个 符合更新的部署规范的新 Pod。`2/2` 表示每个 Pod 有两个容器:

```
NAME                          READY     STATUS    RESTARTS   AGE
patch-demo-1081991389-2wrn5   2/2       Running   0          1m
patch-demo-1081991389-jmg7b   2/2       Running   0          1m
```

仔细查看其中一个 patch-demo Pod:

```shell
kubectl get pod <your-pod-name> --output yaml
```

输出显示 Pod 有两个容器:一个运行 nginx，一个运行 redis:

```
containers:
- image: redis
  ...
- image: nginx
  ...
```

#### 策略性合并类的 patch

您在前面的练习中所做的 patch 称为`策略性合并 patch`。 请注意，patch 没有替换`容器`列表。相反，它向列表中添加了一个新容器。换句话说， patch 中的列表与现有列表合并。当您在列表中使用策略性合并 patch 时，并不总是这样。 在某些情况下，列表是替换的，而不是合并的。

对于策略性合并 patch，列表可以根据其 patch 策略进行替换或合并。patch 策略由 Kubernetes 源代码中字段标记中的 `patchStrategy` 键的值指定。 例如，`PodSpec` 结构体的 `Containers` 字段有 `merge` 的 `patchStrategy`：

```go
type PodSpec struct {
  ...
  Containers []Container `json:"containers" patchStrategy:"merge" patchMergeKey:"name" ...`
```

您还可以在 [OpenApi spec](https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json) 规范中看到 patch 策略：

```json
"io.k8s.api.core.v1.PodSpec": {
    ...
     "containers": {
      "description": "List of containers belonging to the pod. ...
      },
      "x-kubernetes-patch-merge-key": "name",
      "x-kubernetes-patch-strategy": "merge"
     },
```

您可以在 [Kubernetes API 文档](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/#podspec-v1-core) 中看到 patch 策略

创建一个名为 `patch-file-tolerations.yaml` 的文件。内容如下:

```yaml
spec:
  template:
    spec:
      tolerations:
      - effect: NoSchedule
        key: disktype
        value: ssd
```

patch Deployment：

```shell
kubectl patch deployment patch-demo --patch "$(cat patch-file-tolerations.yaml)"
```

查看 patch Deployment：

```shell
kubectl get deployment patch-demo --output yaml
```

输出结果显示部署中的 PodSpec 只有一个默认：

```shell
tolerations:
      - effect: NoSchedule
        key: disktype
        value: ssd
```

请注意，PodSpec 中的 `tolerations` 列表被替换，而不是合并。这是因为 PodSpec 的 tolerance 字段的字段标签中没有 `patchStrategy` 键。所以策略合并 patch 使用默认的 patch 策略，也就是 `replace`。

```go
type PodSpec struct {
  ...
  Tolerations []Toleration `json:"tolerations,omitempty" protobuf:"bytes,22,opt,name=tolerations"`
```

### 使用 JSON 合并 patch 更新部署

策略性合并 patch 不同于 [JSON 合并 patch](https://tools.ietf.org/html/rfc7386)。 使用 JSON 合并 patch，如果您想更新列表，您必须指定整个新列表。新的列表完全取代了现有的列表。

`kubectl patch` 命令有一个 `type` 参数，您可以将其设置为以下值之一:

| Parameter value | Merge type                                                   |
| :-------------- | :----------------------------------------------------------- |
| json            | [JSON Patch, RFC 6902](https://tools.ietf.org/html/rfc6902)  |
| merge           | [JSON Merge Patch, RFC 7386](https://tools.ietf.org/html/rfc7386) |
| strategic       | Strategic merge patch                                        |

有关 JSON patch 和 JSON 合并 patch 的比较，查看[ JSON patch 和 JSON 合并 patch](http://erosb.github.io/post/json-patch-vs-merge-patch/)。

`type` 参数的默认值是 `strategic`。在前面的练习中，我们做了一个策略性的合并 patch。

下一步，在相同的部署上执行 JSON 合并 patch。创建一个名为 `patch-file-2` 的文件。内容如下:

```yaml
spec:
  template:
    spec:
      containers:
      - name: patch-demo-ctr-3
        image: gcr.io/google-samples/node-hello:1.0
```

在 patch 命令中，将 `type` 设置为 `merge`：

```shell
kubectl patch deployment patch-demo --type merge --patch "$(cat patch-file-2.yaml)"
```

查看 patch 部署：

```shell
kubectl get deployment patch-demo --output yaml
```

patch 中指定的`容器`列表只有一个容器。 输出显示您的一个容器列表替换了现有的`容器`列表。

```shell
spec:
  containers:
  - image: gcr.io/google-samples/node-hello:1.0
    ...
    name: patch-demo-ctr-3
```

列表中运行的 Pod：

```shell
kubectl get pods
```

在输出中，您可以看到已经终止了现有的 Pod，并创建了新的 Pod。`1/1` 表示每个新 Pod只运行一个容器。

```shell
NAME                          READY     STATUS    RESTARTS   AGE
patch-demo-1307768864-69308   1/1       Running   0          1m
patch-demo-1307768864-c86dc   1/1       Running   0          1m
```

### kubectl patch 命令的其他形式

`kubectl patch` 命令使用 YAML 或 JSON。它可以将 patch 作为文件，也可以直接在命令行中使用。

创建一个文件名称是 `patch-file.json` 内容如下：

```json
{
   "spec": {
      "template": {
         "spec": {
            "containers": [
               {
                  "name": "patch-demo-ctr-2",
                  "image": "redis"
               }
            ]
         }
      }
   }
}
```

以下命令是相同的：

```shell
kubectl patch deployment patch-demo --patch "$(cat patch-file.yaml)"
kubectl patch deployment patch-demo --patch 'spec:\n template:\n  spec:\n   containers:\n   - name: patch-demo-ctr-2\n     image: redis'

kubectl patch deployment patch-demo --patch "$(cat patch-file.json)"
kubectl patch deployment patch-demo --patch '{"spec": {"template": {"spec": {"containers": [{"name": "patch-demo-ctr-2","image": "redis"}]}}}}'
```

### 总结

在本练习中，您使用 `kubectl patch` 更改部署对象的实时配置。您没有更改最初用于创建部署对象的配置文件。 用于更新 API 对象的其他命令包括 [kubectl annotate](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#annotate)， [kubectl edit](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#edit)，[kubectl replace](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#replace)， [kubectl scale](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#scale)， 和 [kubectl apply](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#apply)。

### 接下来

-   [Kubernetes 对象管理器](https://v1-14.docs.kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/)
-   [使用命令管理 Kubernetes 对象](https://v1-14.docs.kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-command/)
-   [使用配置文件强制管理 Kubernetes 对象](https://v1-14.docs.kubernetes.io/docs/concepts/overview/object-management-kubectl/imperative-config/)
-   [使用配置文件对 Kubernetes 对象进行声明式管理](https://v1-14.docs.kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/)











## 缩放StatefulSet

本文介绍如何弹缩StatefulSet.

### 准备工作

-   StatefulSets仅适用于Kubernetes1.5及以上版本.
-   **不是所有Stateful应用都适合弹缩.** 在弹缩前您的应用前. 您必须充分了解您的应用, 不适当的弹缩StatefulSet或许会造成应用自身功能的不稳定.
-   仅当您确定该Stateful应用的集群是完全健康才可执行弹缩操作.

### 使用 `kubectl` 弹缩StatefulSets

弹缩请确认 `kubectl` 已经升级到Kubernetes1.5及以上版本. 如果不确定, 执行 `kubectl version` 命令并检查使用的 `Client Version`.

#### `kubectl 弹缩`

首先, 找到您想要弹缩的StatefulSet. 记住, 您需先清楚是否能弹缩该应用.

```shell
kubectl get statefulsets <stateful-set-name>
```

改变StatefulSet副本数量:

```shell
kubectl scale statefulsets <stateful-set-name> --replicas=<new-replicas>
```

#### 可使用其他命令: `kubectl apply` / `kubectl edit` / `kubectl patch`

另外, 您可以 [in-place updates](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources) StatefulSets.

如果您的StatefulSet开始由 `kubectl apply` 或 `kubectl create --save-config` 创建,更新StatefulSet manifests中的 `.spec.replicas`, 然后执行命令 `kubectl apply`:

```shell
kubectl apply -f <stateful-set-file-updated>
```

除此之外, 可以通过命令 `kubectl edit` 编辑该字段:

```shell
kubectl edit statefulsets <stateful-set-name>
```

或使用 `kubectl patch`:

```shell
kubectl patch statefulsets <stateful-set-name> -p '{"spec":{"replicas":<new-replicas>}}'
```

### 排查故障

#### 缩容工作不正常

当Stateful管理下的任何一个Pod不健康时您不能缩容该StatefulSet. 仅当Stateful下的所有Pods都处于运行和ready状态后才可缩容.

当一个StatefulSet的size > 1, 如果有一个Pod不健康, 没有办法让Kubernetes知道是否是由于永久性故障还是瞬态(升级/维护/节点重启)导致. 如果该Pod不健康是由于永久性 故障导致, 则在不纠正该故障的情况下进行缩容可能会导致一种状态， 即StatefulSet下的Pod数量低于应正常运行的副本数. 这也许会导致StatefulSet不可用.

如果由于瞬态故障而导致Pod不健康,并且Pod可能再次可用，那么瞬态错误可能会干扰您对 StatefulSet的扩容/缩容操作. 一些分布式数据库在节点加入和同时离开时存在问题. 在 这些情况下，最好是在应用级别进行弹缩操作, 并且只有在您确保Stateful应用的集群是完全健康时才执行弹缩.

### 接下来

了解更多 [deleting a StatefulSet](https://kubernetes.io/docs/tasks/manage-stateful-set/deleting-a-statefulset/).





## 删除StatefulSet

本文介绍如何删除 StatefulSet。

### 准备工作

本文假设在您的集群上已经运行了由 StatefulSet 创建的应用。

### 删除 StatefulSet

您可以像删除 Kubernetes 中的其他资源一样删除 StatefulSet：使用 `kubectl delete` 命令，并按文件或者名字指定 StatefulSet。

```shell
kubectl delete -f <file.yaml>
kubectl delete statefulsets <statefulset-name>
```

删除 StatefulSet 之后，您可能需要单独删除关联的无头服务。

```shell
kubectl delete service <service-name>
```

通过 kubectl 删除 StatefulSet 会将其缩容为0，因此删除属于它的所有pods。 如果您只想删除 StatefulSet 而不删除 pods，使用 `--cascade=false`。

```shell
kubectl delete -f <file.yaml> --cascade=false
```

通过将 `--cascade=false` 传递给 `kubectl delete`，在删除 StatefulSet 对象之后，StatefulSet 管理的 pods 会被保留下来。如果 pods 有一个标签 `app=myapp`，则可以按照如下方式删除它们：

```shell
kubectl delete pods -l app=myapp
```

#### Persistent Volumes

删除 StatefulSet 管理的 pods 并不会删除关联的卷。这是为了确保您有机会在删除卷之前从卷中复制数据。在pods离开[终止状态](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods)后删除 PVC 可能会触发删除支持的 Persistent Volumes，具体取决于存储类和回收策略。声明删除后，您永远不应该假设能够访问卷。

**注意：删除 PVC 时要谨慎，因为这可能会导致数据丢失。**

#### 完全删除 StatefulSet

要简单地删除 StatefulSet 中的所有内容，包括关联的 pods，您可能需要运行一系列类似于以下内容的命令：

```shell
grace=$(kubectl get pods <stateful-set-pod> --template '{{.spec.terminationGracePeriodSeconds}}')
kubectl delete statefulset -l app=myapp
sleep $grace
kubectl delete pvc -l app=myapp
```

在上面的例子中，pods 的标签为 `app=myapp`；适当地替换您自己的标签。

#### 强制删除 StatefulSet 类型的 pods

如果您发现 StatefulSet 中的某些 pods 长时间处于 ‘Terminating’ 或者 ‘Unknown’ 状态，则可能需要手动干预以强制从 apiserver 中删除 pods。这是一项潜在的危险任务。详细信息请阅读[删除 StatefulSet 类型的 Pods](https://kubernetes.io/docs/tasks/manage-stateful-set/delete-pods/)。

### 接下来

了解更多有关[强制删除 StatefulSet 类型的 Pods](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/)。





## 强制删除状态集（StatefulSet）pod

本文介绍了如何删除 StatefulSet 管理的部分 pods，并且解释了这样操作时需要记住的注意事项。

### 准备开始

-   这是一项相当高级的任务，并且可能会违反 StatefulSet 固有的某些属性。
-   继续任务之前，请熟悉下面列举的注意事项。

### StatefulSet 注意事项

在 StatefulSet 的正常操作中，**永远不**需要强制删除 StatefulSet 管理的 pod。StatefulSet 控制器负责创建，扩容和删除 StatefulSet 管理的 pods。它尝试确保从序号 0 到 N-1 指定数量的 pods 处于活动状态并准备就绪。StatefulSet 确保在任何时候，集群中最多只有一个具有给定标识的 pod。这就是所谓的由 StatefulSet 提供的*最多一个*的语义。

应谨慎进行手动强制删除操作，因为它可能会违反 StatefulSet 固有的至多一个的语义。StatefulSets 可用于运行分布式和集群级的应用，这些应用需要稳定的网络标识和可靠的存储。这些应用通常配置为具有固定标识固定数量的成员集合。具有相同身份的多个成员可能是灾难性的，并且可能导致数据丢失 (e.g. 基于 quorum 系统中的脑裂场景)。

### 删除 Pods

您可以使用下面的命令执行优雅地删除 pod:

```shell
kubectl delete pods <pod>
```

为了使上面的方法能够正常终止，Pod **一定不能**设置`pod.Spec.TerminationGracePeriodSeconds` 为 0。将 `pod.Spec.TerminationGracePeriodSeconds` 设置为 0s 的做法是不安全的，强烈建议 StatefulSet 类型的 pods 不要使用。优雅删除是安全的，并且会在 kubelet 从 apiserver 中删除名称之前确保 [优雅地关闭 pod ](https://v1-14.docs.kubernetes.io/docs/user-guide/pods/#termination-of-pods)。

Kubernetes (1.5 版本或者更新版本)不会因为一个 Node 无法访问而删除 pods。在无法访问节点上运行的 pods 在[超时](https://v1-14.docs.kubernetes.io/docs/admin/node/#node-condition)后会进入’Terminating’ 或者 ‘Unknown’ 状态。当用户尝试优雅删除无法访问节点上的 pod 时，pods 也可能会进入这些状态。从 apiserver 中删除处于这些状态 pod 的唯一方法如下：

-   Node 对象被删除(要么您删除, 或者[Node Controller](https://v1-14.docs.kubernetes.io/docs/admin/node))。
-   无响应节点上的 kubelet 开始响应，杀死 pod 并从 apiserver 中移除该条目。
-   用户强制删除 pod。

推荐使用第一种或者第二种方法。如果确认节点已经不可用了 (比如，永久断开网络，断电等)，则删除 Node 对象。如果节点遇到网裂，请尝试解决该问题或者等待其解决。当网裂愈合时，kubelet 将完成 pod 的删除并从 apiserver 中释放其名字。

通常，pod 一旦不在节点上运行，或者管理员删除了节点，系统就会完成删除。你可以通过强制删除 pod 来覆盖它。

#### 强制删除

强制删除**不要**等待来自 kubelet 的确认 pod 已被终止。无论强制删除是否成功杀死了 pod，它都会立即从 apiserver 中释放该名字。这将让 StatefulSet 控制器创建一个具有相同标识的替换 pod；这可能导致正在运行 pod 的重复，并且如果所述 pod 仍然可以与 StatefulSet 的成员通信，则将违反 StatefulSet 旨在保证的最多一个的语义。

当你强制删除 StatefulSet 类型的 pod 时，你要确保有问题的 pod 不会再和 StatefulSet 管理的其他 pods通信，并且可以安全地释放其名字以便创建替换 pod。

如果要使用 kubectl version >= 1.5 强制删除 pod，请执行下面命令：

```shell
kubectl delete pods <pod> --grace-period=0 --force
```

如果您使用 kubectl <= 1.4 的任何版本，则应省略 `--force` 选项：

```shell
kubectl delete pods <pod> --grace-period=0
```

如果在这些命令后 pod 仍处于`Unknown`状态，请使用以下命令从集群中删除 pod:

```shell
kubectl patch pod <pod> -p '{"metadata":{"finalizers":null}}'
```

请始终谨慎地执行强制删除 StatefulSet 类型的 pods，并完全了解所涉及地风险。

### 接下来

进一步了解[调试 StatefulSet](https://v1-14.docs.kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/)。









## 使用副本控制器(Replication Controller)执行滚动更新

### 概述

**注**: 创建副本应用的首选方法是使用[Deployment](https://kubernetes.io/docs/api-reference/v1.15/#deployment-v1beta1-apps)，Deployment使用[ReplicaSet](https://kubernetes.io/docs/api-reference/v1.15/#replicaset-v1beta1-extensions)来进行副本控制。 更多信息, 查看[使用Deployment运行一个无状态应用](https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/)。

为了在更新服务的同时不中断业务， `kubectl` 支持[‘滚动更新’](https://kubernetes.io/docs/user-guide/kubectl/v1.6/#rolling-update)，它一次更新一个pod，而不是同时停止整个服务。 有关更多信息，请参阅 [滚动更新设计文档](https://git.k8s.io/community/contributors/design-proposals/cli/simple-rolling-update.md) 和 [滚动更新示例](https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/)。

请注意， `kubectl rolling-update` 仅支持Replication Controllers。 但是，如果使用Replication Controllers部署应用，请考虑将其切换到[Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). Deployment是一种被推荐使用的更高级别的控制器，它可以对应用进行声明性的自动滚动更新。 如果您仍然希望保留您的Replication Controllers并使用 `kubectl rolling-update`进行滚动更新， 请继续往下阅读：

滚动更新可以对replication controller所管理的Pod的配置进行变更，变更可以通过一个新的配置文件来进行，或者，如果只更新镜像，则可以直接指定新的容器镜像。

滚动更新的工作流程：

1.  通过新的配置创建一个replication controller
2.  在新的控制器上增加副本数，在旧的上面减少副本数，直到副本数达到期望值
3.  删除之前的replication controller

使用`kubectl rolling-update`命令来进行滚动更新：

```
$ kubectl rolling-update NAME \
    ([NEW_NAME] --image=IMAGE | -f FILE)
```

### 通过配置文件更新

通过配置文件来进行滚动更新，需要在`kubectl rolling-update`命令后面带上新的配置文件：

```
$ kubectl rolling-update NAME -f FILE
```

这个配置文件必须满足以下条件：

-   指定不同的`metadata.name`值
-   至少要修改`spec.selector`中的一个标签值
-   `metadata.namespace`字段必须相同

Replication Controllers的配置文件详细介绍见[创建Replication Controllers](https://kubernetes.io/docs/tutorials/stateless-application/run-stateless-ap-replication-controller/).

#### 示例

```
// 通过新的配置文件frontend-v2.json来更新frontend-v1的pods
$ kubectl rolling-update frontend-v1 -f frontend-v2.json

// 将frontend-v2.json数据传到标准输入来更新frontend-v1的pods
$ cat frontend-v2.json | kubectl rolling-update frontend-v1 -f -
```

### 更新容器镜像

仅更新容器镜像的话，可通过如下命令，该命令可以指定一个新的控制器名称（可选），通过`--image`参数来指定新的镜像名称和标签。

```
$ kubectl rolling-update NAME [NEW_NAME] --image=IMAGE:TAG
```

`--image`参数仅支持单容器pod，多容器pod使用`--image`参数会返回错误。

如果没有指定 `NEW_NAME` ，新的replication controller创建后会使用一个临时名称，当更新完成，旧的controller被删除后，新的controller名称会被更新成旧的controller名称。

如果`IMAGE:TAG` 和当前值相同，更新就会失败。 因此，我们建议使用版本号来作为标签，而不是使用 `:latest`。从一个 `image:latest`镜像升级到一个新的 `image:latest` 镜像将会失败，即使这两个镜像不是相同的。 所以，我们不建议使用 `:latest` 来作为标签，详细信息见[最佳配置实践](https://kubernetes.io/docs/concepts/configuration/overview/#container-images) 。

#### 示例

```
// 更新frontend-v1的pod到frontend-v2
$ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2

// 更新frontend的pods，不更改replication controller的名称
$ kubectl rolling-update frontend --image=image:v2
```

### 必选和可选字段

必选字段：

-   `NAME`: 需要进行滚动更新的replication controller名称

下面两个字段选其一：

-   `-f FILE`: 新的replication controller的配置文件，JSON或者YAML格式均可。配置文件必须指定一个新的顶层`id`值，且至少包含一个现有`spec.selector`中的键值对。 详细信息见[通过Replication Controller运行无状态应用](https://kubernetes.io/docs/tutorials/stateless-application/run-stateless-ap-replication-controller/#replication-controller-configuration-file)。

    或者：

    

-   `--image IMAGE:TAG`: 更新后的镜像的名称和标签。必须和当前的image:tag不同。

可选字段包括：

-   `NEW_NAME`: 只和 `--image` 一起使用，不和 `-f FILE` 一起使用。标识新的replication controller的名称。
-   `--poll-interval DURATION`: 在更新后轮询控制器状态的间隔时间。有效单位有 `ns`（纳秒），`us` 或 `µs`（微秒），`ms`（毫秒），`s`（秒），`m`（分钟）或 `h`（小时）。 单位可以自由组合（例如 `1m30s`）。 默认值为 `3s`。
-   `--timeout DURATION`: 退出更新之前，等待控制器更新一个pod的最大时间。默认是`5m0s`。有效单位如`--poll-interval`所述。
-   `--update-period DURATION`: 更新两个pod之间等待的时间，默认值是`1m0s`。有效单位如`--poll-interval`所述。

有关`kubectl rolling-update`命令的更多信息见[`kubectl`参考](https://kubernetes.io/docs/user-guide/kubectl/v1.6/#rolling-update).

### 实践

现在你运行了一个1.7.9版本的nginx应用：

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: my-nginx
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

要更新到1.9.1版本，你可以使用[`kubectl rolling-update --image`](https://git.k8s.io/community/contributors/design-proposals/cli/simple-rolling-update.md)来指定一个新的镜像：

```shell
$ kubectl rolling-update my-nginx --image=nginx:1.9.1
Created my-nginx-ccba8fbd8cc8160970f63f9a2696fc46
```

在终端上打开另一个窗口 ，你可以看到`kubectl` 给每个pod都增加了一个值为配置文件哈希值的 `deployment` 标签，用来区分新旧pod：

```shell
$ kubectl get pods -l app=nginx -L deployment
NAME                                              READY     STATUS    RESTARTS   AGE       DEPLOYMENT
my-nginx-ccba8fbd8cc8160970f63f9a2696fc46-k156z   1/1       Running   0          1m        ccba8fbd8cc8160970f63f9a2696fc46
my-nginx-ccba8fbd8cc8160970f63f9a2696fc46-v95yh   1/1       Running   0          35s       ccba8fbd8cc8160970f63f9a2696fc46
my-nginx-divi2                                    1/1       Running   0          2h        2d1d7a8f682934a254002b56404b813e
my-nginx-o0ef1                                    1/1       Running   0          2h        2d1d7a8f682934a254002b56404b813e
my-nginx-q6all                                    1/1       Running   0          8m        2d1d7a8f682934a254002b56404b813e
```

使用`kubectl rolling-update`可以实时看到更新的进度：

```
Scaling up my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 from 0 to 3, scaling down my-nginx from 3 to 0 (keep 3 pods available, don't exceed 4 pods)
Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 1
Scaling my-nginx down to 2
Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 2
Scaling my-nginx down to 1
Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 up to 3
Scaling my-nginx down to 0
Update succeeded. Deleting old controller: my-nginx
Renaming my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 to my-nginx
replicationcontroller "my-nginx" rolling updated
```

如果遇到问题，你可以中途停止滚动更新，并且使用 `--rollback` 来回滚到以前的版本:

```shell
$ kubectl rolling-update my-nginx --rollback
Setting "my-nginx" replicas to 1
Continuing update with existing controller my-nginx.
Scaling up nginx from 1 to 1, scaling down my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 from 1 to 0 (keep 1 pods available, don't exceed 2 pods)
Scaling my-nginx-ccba8fbd8cc8160970f63f9a2696fc46 down to 0
Update succeeded. Deleting my-nginx-ccba8fbd8cc8160970f63f9a2696fc46
replicationcontroller "my-nginx" rolling updated
```

这个例子说明容器的不变性是个巨大的优点。

如果你不仅仅是需要更新镜像，(例如，更新命令参数，环境变量等)，你可以创建一个新的replication controller配置文件，包含一个新的名称和不同的标签值，例如：

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: my-nginx-v4
spec:
  replicas: 5
  selector:
    app: nginx
    deployment: v4
  template:
    metadata:
      labels:
        app: nginx
        deployment: v4
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.2
        args: ["nginx", "-T"]
        ports:
        - containerPort: 80
```

然后使用它来进行更新：

```shell
$ kubectl rolling-update my-nginx -f ./nginx-rc.yaml
Created my-nginx-v4
Scaling up my-nginx-v4 from 0 to 5, scaling down my-nginx from 4 to 0 (keep 4 pods available, don't exceed 5 pods)
Scaling my-nginx-v4 up to 1
Scaling my-nginx down to 3
Scaling my-nginx-v4 up to 2
Scaling my-nginx down to 2
Scaling my-nginx-v4 up to 3
Scaling my-nginx down to 1
Scaling my-nginx-v4 up to 4
Scaling my-nginx down to 0
Scaling my-nginx-v4 up to 5
Update succeeded. Deleting old controller: my-nginx
replicationcontroller "my-nginx-v4" rolling updated
```

### 故障分析

如果更新过程中，达到超时时长`timeout`后还没更新完成，则更新会失败。这时，一些pod会属于新的replication controller，一些会属于旧的。

如果更新失败，可以尝试使用同样的命令来继续更新过程。

在尝试更新之前如果需要回滚到之前的状态，可在之前的命令后面添加`--rollback=true`参数，这将回退所有的更改。





## 水平pod自动缩放

水平Pod自动缩放器(Horizontal Pod Autoscaler)根据观察到的CPU利用率(或者，在支持[自定义指标(custom metrics](https://git.k8s.io/community/contributors/design-proposals/instrumentation/custom-metrics-api.md))的情况下，根据其他应用程序提供的指标)自动调整复制控制器、部署或复制集中的Pod数量。请注意，水平Pod自动缩放不适用于无法缩放的对象，例如守护进程集。

Horizontal Pod Autoscaler 作为 kubernetes API resource 和 controller 的实现。Resource 确定 controller 的行为。Controller 会根据监测到用户指定的目标的 CPU 利用率周期性地调整 replication controller 或 deployment 的 replica 数量。

### Horizontal Pod Autoscaler怎样工作？

![Horizontal Pod Autoscaler diagram](https://d33wubrfki0l68.cloudfront.net/4fe1ef7265a93f5f564bd3fbb0269ebd10b73b4e/1775d/images/docs/horizontal-pod-autoscaler.svg)

Horizontal Pod Autoscaler 由一个控制循环实现，循环周期由 controller manager 中的 `--horizontal-pod-autoscaler-sync-period` 标志指定（默认是 30 秒）。

在每个周期内，controller manager 会查询 HorizontalPodAutoscaler 中定义的 metric 的资源利用率。Controller manager 从 resource metric API（每个 pod 的 resource metric）或者自定义 metric API（所有的metric）中获取 metric。

-   每个 Pod 的 resource metric（例如 CPU），controller 通过 resource metric API 获取 HorizontalPodAutoscaler 中定义的每个 Pod 中的 metric。然后，如果设置了目标利用率，controller 计算利用的值与每个 Pod 的容器里的 resource request 值的百分比。如果设置了目标原始值，将直接使用该原始 metric 值。然后 controller 计算所有目标 Pod 的利用率或原始值（取决于所指定的目标类型）的平均值，产生一个用于缩放所需 replica 数量的比率。 

请注意，如果某些 Pod 的容器没有设置相关的 resource request ，则不会定义 Pod 的 CPU 利用率，并且 Autoscaler 也不会对该 metric 采取任何操作。 有关自动缩放算法如何工作的更多细节，请参阅 [自动缩放算法设计文档](https://git.k8s.io/community/contributors/design-proposals/horizontal-pod-autoscaler.md#autoscaling-algorithm)。

-   对于每个 Pod 自定义的 metric，controller 功能类似于每个 Pod 的 resource metric，只是它使用原始值而不是利用率值。
-   对于对象度量和外部度量，将获取单个度量，该度量描述所讨论的对象。将该指标与目标值进行比较，得到如上所示的比率。在 `autoscaling/v2beta2`  API版本中，可以选择在进行比较之前将该值除以pod的数量。

HorizontalPodAutoscaler 通常从一系列聚合的api中获取度量(`metrics.k8s.io`, `custom.metrics.k8s.io`,  `external.metrics.k8s.io`)。 `metrics.k8s.io` API 通常由metrics-server提供，需要单独启动。有关说明，请参见 [metrics-server](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server) 。HorizontalPodAutoscaler还可以直接从Heapster获取指标。

>   **Note:**
>
>   **FEATURE STATE:** `Kubernetes 1.11` [弃用](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#)
>
>   在Kubernetes 1.11中，不建议从Heapster获取度量。

查看 [metrics API说明](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis)获取更多细节。

自动调度器使用scale子资源访问相应的可伸缩控制器(如复制控制器、部署和副本集)。Scale是一个接口，允许您动态设置副本的数量并检查它们的每个当前状态。关于scale子资源的更多细节可以在 [这里](https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md#scale-subresource).找到。

#### 算法细节

从最基本的角度来看，水平Pod自动缩放器控制器是根据期望度量值与当前度量值的比值来工作的:

```
desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
```

例如，如果当前的度量值是 `200m`，而期望的值是 `100m`,，那么副本的数量将增加一倍，因为 `200.0 / 100.0 == 2.0` ，如果当前的值是 `50m`，我们将把副本的数量减半，因为 `50.0 / 100.0 == 0.5` 。如果比率足够接近1.0(在一个全局可配置的公差范围内，由 `--horizontal-pod-autoscaler-tolerance` 指定，默认值为0.1)，我们将跳过缩放。

当指定一个 `targetAverageValue` 或 `targetAverageUtilization` 时，通过取给定度量在HorizontalPodAutoscaler的缩放目标的所有pod上的平均值，计算 `currentMetricValue` 。然而，在检查容差和决定最终值之前，我们要考虑pod准备就绪和缺少的指标。

所有具有删除时间戳集的pod(即在关闭过程中的pod)和所有失败的pod都会被丢弃。

如果某个特定的Pod缺少度量，则将其保留到稍后;缺少度量的pod将用于调整最终的伸缩量。

当在CPU上缩放时，如果任何pod还没有准备好(即它还在初始化)，*或者* pod最近的度量点是在它就绪之前，那么这个pod也会被放在一边。

由于技术限制，HorizontalPodAutoscaler控制器在决定是否设置某些CPU指标时，不能准确地确定pod第一次准备好。相反，如果Pod还没有准备好，并且在启动后的短时间内转换为“not yet ready”，则它认为Pod还没有准备好。该值配置为 `--horizontal-pod-autoscaler-initial-readiness-delay` 的值，默认值为30秒。一旦pod准备好了，它认为如果从启动到准备的任何转换发生在较长的可配置时间内，那么它就是第一个。该值配置为 `--horizontal-pod-autoscaler-cpu-initialization-period` 的值，默认为5分钟。

然后使用未从上面设置或丢弃的剩余pod计算 `currentMetricValue / desiredMetricValue` 基准比例。

如果有任何遗漏的度量，我们会更保守地重新计算平均值，假设这些pod在按比例缩小时消耗了所需值的100%，在按比例增大时消耗了0%。这抑制了任何潜在规模的规模。

此外，如果存在任何尚未就绪的pod，并且我们可以在不考虑遗漏度量或尚未就绪的pods的情况下进行伸缩，我们保守地假设尚未就绪的pods消耗了所需度量的0%，从而进一步降低了伸缩的幅度。

在考虑了尚未准备好的pod和缺少的指标之后，我们重新计算了使用率。如果新的比率与缩放方向相反，或者在公差范围内，则跳过缩放。否则，我们使用新的比例来缩放。

注意，平均利用率的*原始值*是通过HorizontalPodAutoscaler状态报告回来的，没有考虑到未就绪pod或缺少度量的，即使使用了新的使用率。

如果在HorizontalPodAutoscaler中指定了多个度量，则对每个度量执行此计算，然后选择所需副本计数中最大的一个。如果无法将这些度量转换为所需的副本计数(例如，由于从度量api获取度量出错)，并且可以获取的度量建议按比例缩小，则跳过缩放。这意味着，如果一个或多个指标给出的 `desiredReplicas` 大于当前值，HPA仍然能够扩展。

最后，就在HPA对目标进行缩放之前，记录缩放建议。控制器考虑可配置窗口中的所有建议，并从该窗口中选择最高的建议。这个值可以使用 `--horizontal-pod-autoscaler-downscale-stabilization` 来配置，该标志默认值为5分钟。这意味着规模缩减将逐步发生，消除快速波动的度量值的影响。

### API Object

Horizontal Pod Autoscaler 是 kubernetes 的 `autoscaling` API 组中的 API 资源。当前的稳定版本中，只支持 CPU 自动扩缩容，可以在 `autoscaling/v1` API 版本中找到。

在 alpha 版本中支持根据内存和自定义 metric 扩缩容，可以在 `autoscaling/v2beta2` 中找到。 `autoscaling/v2beta2`  中引入的新字段在 `autoscaling/v1` 中是做为 annotation 而保存的。

关于该 API 对象的更多信息，请参阅 [HorizontalPodAutoscaler Object](https://git.k8s.io/community/contributors/design-proposals/horizontal-pod-autoscaler.md#horizontalpodautoscaler-object)。

### 在 kubectl 中支持 Horizontal Pod Autoscaling

Horizontal Pod Autoscaler 和其他的所有 API 资源一样，通过 `kubectl` 以标准的方式支持。

我们可以使用 `kubectl create` 命令创建一个新的 autoscaler。

我们可以使用 `kubectl get hpa` 列出所有的 autoscaler，使用 `kubectl describe hpa` 获取其详细信息。

最后我们可以使用 `kubectl delete hpa` 删除 autoscaler。

另外，可以使用 `kubectl autoscale` 命令，很轻易的就可以创建一个 Horizontal Pod Autoscaler。

例如，执行 `kubectl autoscale rc foo —min=2 —max=5 —cpu-percent=80` 命令将为 replication controller *foo* 创建一个 autoscaler，目标的 CPU 利用率是`80%`，replica 的数量介于 2 和 5 之间。

关于 `kubectl autoscale` 的更多信息请参阅 [这里](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#autoscale)。

### 滚动更新期间的自动缩放

目前在Kubernetes中，可以通过直接管理 replication controller 或使用 deployment 对象来执行 [滚动更新](https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/)，该 deployment 对象为您管理基础 replication controller。

Horizontal Pod Autoscaler 仅支持后一种方法：Horizontal Pod Autoscaler 被绑定到 deployment 对象，它设置 deployment 对象的大小，deployment 负责设置底层 replication controller 的大小。

Horizontal Pod Autoscaler 不能使用直接操作 replication controller 进行滚动更新，即不能将 Horizontal Pod Autoscaler 绑定到 replication controller，并进行滚动更新（例如使用 `kubectl rolling-update`）。

这不行的原因是，当滚动更新创建一个新的 replication controller 时，Horizontal Pod Autoscaler 将不会绑定到新的 replication controller 上。

### 支持冷却/延迟

当使用水平Pod自动伸缩器管理一组副本的比例时，由于所评估的度量的动态特性，副本的数量可能经常波动。这有时被称为*颠簸(thrashing)*。

从v1.6开始，集群操作员可以通过将全局HPA作为 `kube-controller-manager` 组件的以一个标志(flag)来缓解这个问题:

从v1.12开始，一个新的算法更新消除了高档延迟的需要。

-   `--horizontal-pod-autoscaler-downscale-stabilization`: 此选项的值是一个持续时间，指定自动调度器必须等待多长时间，才能在当前操作完成后执行另一个降级操作。默认值是5分钟 (`5m0s`)。

>   **注意:**在调优这些参数值时，集群操作员应该知道可能的结果。如果延迟(冷却)值设置得太长，可能会有人抱怨水平Pod自动调度器对工作负载变化没有响应。但是，如果延迟值设置得太短，副本集的规模可能会像往常一样不断波动。

### 支持多个度量(metric)

Kubernetes 1.6 中增加了支持基于多个 metric 的扩缩容。您可以使用 `autoscaling/v2beta2` API 版本来为 Horizontal Pod Autoscaler 指定多个 metric。然后 Horizontal Pod Autoscaler controller 将权衡每一个 metric，并根据该 metric 提议一个新的 scale。在所有提议里最大的那个 scale 将作为最终的 scale。

### 支持自定义metrics

>   **注意：** Kubernetes 1.2 根据特定于应用程序的 metric ，通过使用特殊注释的方式，增加了对缩放的 alpha 支持。Kubernetes 1.6删除了对这些注释的支持，支持新的自动缩放API。虽然收集自定义度量的旧方法仍然可用，但是水平Pod自动调度器将不能使用这些度量，而用于指定要伸缩的自定义度量的前注释也不再由水平Pod自动调度器控制器执行。

Kubernetes 1.6增加了在 Horizontal Pod Autoscaler 中使用自定义 metric 的支持。您可以为 `autoscaling/v2beta2` API 中使用的 Horizontal Pod Autoscaler 添加自定义 metric 。Kubernetes 然后查询新的自定义 metric API 来获取相应自定义 metric 的值。

在 [Support for metrics APIs](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis) 查看需求。

### 支持metrics API

默认情况下，HorizontalPodAutoscaler控制器从一系列api中检索度量。为了让集群能够访问这些api，集群管理员必须确保:

-   启用了[API聚合层](https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/) 。
-   注册了相应的api：
    -   对于资源度量是 `metrics.k8s.io` API，通常由 [metrics-server](https://github.com/kubernetes-incubator/metrics-server) 提供。它可以作为集群插件启动。
    -   对于自定义度量是 `custom.metrics.k8s.io` API。它由度量解决方案供应商提供的“适配器”API服务器提供。检查您的度量管道，或[已知解决方案列表](https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api).。如果您想编写自己的代码，请查看[样板](https://github.com/kubernetes-incubator/custom-metrics-apiserver) 。
    -   对于外部度量，这是 `external.metrics.k8s.io` API。它可能由上面提供的自定义度量适配器提供。
-   The `--horizontal-pod-autoscaler-use-rest-clients` 为 `true` 或未设置。将此设置为false将切换到基于Headster的自动缩放，这是不推荐的。

有关这些不同度量路径及其差异的更多信息，请参见 [HPA V2](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/hpa-v2.md), [custom.metrics.k8s.io](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/custom-metrics-api.md) 和 [external.metrics.k8s.io](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/external-metrics-api.md) 的相关设计建议。

有关如何使用它们的示例，请参阅使用[定义度量攻略](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics) 和使用[外部度量攻略](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects)。

### 下一步

-   设计文档: [水平pod自动缩放](https://git.k8s.io/community/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler.md).
-   kubectl 自动缩放命令: [kubectl autoscale](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#autoscale).
-    [水平pod自动缩放器](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)的使用示例





## 水平pod自动缩放攻略

Horizontal Pod Autoscaling可以根据CPU利用率自动伸缩一个Replication Controller、Deployment 或者Replica Set中的Pod数量（或者基于一些应用程序提供的度量指标，目前这一功能处于alpha版本）。

本文将引导您了解如何为php-apache服务器配置和使用Horizontal Pod Autoscaling。 更多Horizontal Pod Autoscaling信息请参阅[Horizontal Pod Autoscaling用户指南](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)。

### 准备工作

本文示例需要一个1.2或者更高版本的可运行的Kubernetes集群以及kubectl。 由于Horizontal Pod Autoscaler需要使用[Heapster](https://github.com/kubernetes/heapster)所收集到的 度量数据，请确保Heapster被正确部署到Kubernetes集群中（如果您遵循[在GCE上搭建Kubernetes入门指南](https://k8smeetup.github.io/docs/getting-started-guides/gce) 中的步骤，默认情况下Heapster已经被部署并被启用）。

如果需要为Horizontal Pod Autoscaler指定多种资源度量指标，您的Kubernetes集群以及kubectl至少需要达到1.6版本。此外，如果需要使用自定义度量指标，您的Kubernetes集群版本必须在kubernetes 1.10或以上，并且必须能够与提供这些自定义指标的API服务器通信。更多详细信息，请参阅[Horizontal Pod Autoscaling用户指南](https://k8smeetup.github.io/docs/user-guide/horizontal-pod-autoscaling/#support-for-custom-metrics)。

### Run & expose php-apache server

为了演示Horizontal Pod Autoscaler，我们将使用一个基于php-apache镜像的定制Docker镜像。Dockerfile内容如下：

```
FROM php:5-apache
ADD index.php /var/www/html/index.php
RUN chmod a+rx index.php
```

它定义了一个index.php页面，执行一些CPU密集型计算:

```
<?php
  $x = 0.0001;
  for ($i = 0; $i <= 1000000; $i++) {
    $x += sqrt($x);
  }
  echo "OK!";
?>
```

首先，我们将启动一个运行镜像的部署，并将其作为服务公开:

```shell
kubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --limits=cpu=500m --expose --port=80
service/php-apache created
deployment.apps/php-apache created
```

### 创建水平pod自动缩放器

现在服务器正在运行，我们将使用 [kubectl autoscale](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#autoscale)创建autoscaler。下面的命令将创建一个水平Pod Autoscaler，它维护由我们在这些指令的第一步中创建的php-apache部署控制的Pod的1到10个副本。粗略地说，HPA将增加和减少副本的数量(通过部署)，以保持所有pod的平均CPU利用率为50%(因为每个pod通过 [kubectl run](https://github.com/kubernetes/kubernetes/blob/master/docs/user-guide/kubectl/kubectl_run.md)请求200毫核(milli-cores)，这意味着平均CPU使用量为100毫核)。有关算法的更多细节，请参见[这里](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details) 。

```shell
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
horizontalpodautoscaler.autoscaling/php-apache autoscaled
```

查看自动缩放器的状态：

```shell
kubectl get hpa
NAME         REFERENCE                     TARGET    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%  1         10        1          18s
```

请注意，由于我们没有向服务器发送任何请求，所以当前CPU消耗为0%( `CURRENT` 列显示了由相应部署控制的所有pod的平均值)。

### 增加负载

现在，我们将看到autoscaler如何对增加的负载做出反应。我们将启动一个容器，并向php-apache服务发送一个无限循环的查询(请在不同的终端运行它):

```shell
kubectl run -i --tty load-generator --image=busybox /bin/sh

Hit enter for command prompt

while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done
```

一分钟左右，执行下面命令，我们应该就能看到更高的CPU负载:

```shell
kubectl get hpa
NAME         REFERENCE                     TARGET      CURRENT   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   305% / 50%  305%      1         10        1          3m
```

这里，CPU消耗已经增加到请求的305%。因此，部署被调整到7个副本:

```shell
kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   7         7         7            7           19m
```

>   **注意:**可能需要几分钟来稳定副本的数量。由于没有以任何方式控制负载量，因此最终的副本数量可能与本例不同。

### 停止负载

我们将通过停止负载来结束我们的示例。

在我们创建基于`busybox`镜像的容器的终端中，输入`<Ctrl> + C`来终止负载的产生。

然后我们可以再次查看负载状态（等待几分钟时间）：

```shell
kubectl get hpa
NAME         REFERENCE                     TARGET       MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%     1         10        1          11m
kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   1         1         1            1           27m
```

现在，CPU利用率已经降到0，所以HPA将自动缩减副本数量至1。

>   **注意** 自动伸缩完成副本数量的改变可能需要几分钟的时间。

### 基于多项度量指标和自定义度量指标的自动伸缩

利用 `autoscaling/v2beta2` API版本，您可以在自动伸缩`php-apache`这个部署时引入其他度量指标。

首先，在 `autoscaling/v2beta2` 表单中获取您的HorizontalPodAutoscaler的YAML:

```shell
kubectl get hpa.v2beta2.autoscaling -o yaml > /tmp/hpa-v2.yaml
```

在编辑器中打开 `/tmp/hpa-v2.yaml` 文件，应该能看到这样的YAML：

```yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
status:
  observedGeneration: 1
  lastScaleTime: <some-time>
  currentReplicas: 1
  desiredReplicas: 1
  currentMetrics:
  - type: Resource
    resource:
      name: cpu
      current:
        averageUtilization: 0
        averageValue: 0
```

请注意， `targetCPUUtilizationPercentage` 字段已被一个名为 `metrics` 的数组替换。CPU利用率度量是一个*资源度量*，因为它表示为pod容器上指定的资源的百分比。注意，您可以指定CPU之外的其他资源度量。默认情况下，唯一支持的其他资源度量是内存。这些资源不会在集群之间更改名称，并且应该始终可用，只要 `metrics.k8s.io` API是可用的。

您还可以指定资源度量指标使用绝对数值，而不是请求值的百分比，通过使用 `AverageValue` 的 `target` 而不是 `AverageUtilization`。并设置 相应的`target.averageValue` 字段而不是 `target.averageUtilization`。

还有另外两种类型的度量标准，它们都被认为是*自定义度量标准*:pod度量标准和对象度量标准。这些指标可能具有特定于集群的名称，并且需要更高级的集群监视设置。

第一个可选的度量类型是*pod度量*。这些指标描述了pods，并在pods之间取平均值，并与目标值进行比较，以确定副本数量。它们的工作原理非常类似于资源度量，只是它们***只***支持 `AverageValue` 的 `target` 类型。

Pod指标是使用这样的度量块指定的:

```yaml
type: Pods
pods:
  metric:
    name: packets-per-second
  target:
    type: AverageValue
    averageValue: 1k
```

第二种可选的度量指标类型是*对象度量指标*。相对于描述Pod，这些度量指标用于描述一个在相同名字空间(namespace)中的其他对象。 请注意这些度量指标用于描述这些对象，并非从对象中获取。对象度量支持 `Value` 和 `AverageValue` 的 `target` 类型。使用 `Value` ，可以直接将目标与API返回的度量进行比较。使用 `AverageValue` ，从定制度量API返回的值除以与目标进行比较之前的pod数量。下面的示例是 `requests-per-second` 的YAML表示：

```yaml
type: Object
object:
  metric:
    name: requests-per-second
  describedObject:
    apiVersion: networking.k8s.io/v1beta1
    kind: Ingress
    name: main-route
  target:
    type: Value
    value: 2k
```

如果您指定了多个上述类型的度量指标，HorizontalPodAutoscaler将会依次考量各个指标。 HorizontalPodAutoscaler将会计算每一个指标所提议的副本数量，然后最终选择一个最高值。

比如，如果您的监控系统能够提供网络流量数据，您可以通过`kubectl edit`命令将上述Horizontal Pod Autoscaler的定义更改为：

```yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: AverageUtilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      targetAverageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      target:
        kind: Value
        value: 10k
status:
  observedGeneration: 1
  lastScaleTime: <some-time>
  currentReplicas: 1
  desiredReplicas: 1
  currentMetrics:
  - type: Resource
    resource:
      name: cpu
    current:
      averageUtilization: 0
      averageValue: 0
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        name: main-route
      current:
        value: 10k
```

然后，您的HorizontalPodAutoscaler将会尝试确保每个Pod的CPU利用率在50%以内，每秒能够服务1000个数据包请求，并确保所有 在Ingress后的Pod每秒能够服务的请求总数达到10000个。

#### 在更具体的度量上自动缩放

许多度量管道允许您通过名称或一组称为*标签*的附加描述符来描述度量。对于所有非资源度量类型(pod、object和外部，如下所述)，可以指定传递到度量管道的附加标签选择器。例如，如果您使用 `verb` 标签收集一个度量 `http_requests` ，您可以指定下面的度量块，只对GET请求进行缩放:

```yaml
type: Object
object:
  metric:
    name: `http_requests`
    selector: `verb=GET`
```

这个选择器使用与完整的Kubernetes标签选择器相同的语法。如果名称和选择器匹配多个系列，则监视管道将确定如何将多个系列折叠成单个值。选择器是附加的，不能选择描述**非**目标对象的对象的度量(在 `Pod` 类型中是目标pod，在对象类型中是描述的对象)。
