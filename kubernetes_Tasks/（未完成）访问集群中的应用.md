# 访问集群中的应用

## Web UI(dashboard)(仪表盘)

Dashboard 是基于网页的 Kubernetes 用户界面。您可以使用 Dashboard 将容器应用部署到 Kubernetes 集群中，也可以对容器应用排错，还能管理集群本身及其附属资源。您可以使用 Dashboard 获取运行在集群中的应用的概览信息，也可以创建或者修改 Kubernetes 资源（如 Deployment，Job，DaemonSet 等等）。例如，您可以对 Deployment 实现弹性伸缩、发起滚动升级、重启 Pod 或者使用向导创建新的应用。

Dashboard 同时展示了 Kubernetes 集群中的资源状态信息和所有报错信息。

![Kubernetes Dashboard UI](https://d33wubrfki0l68.cloudfront.net/349824f68836152722dab89465835e604719caea/6e0b7/images/docs/ui-dashboard.png)

### 部署 Dashboard UI

默认情况下不会部署 Dashboard。可以通过以下命令部署：

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml
```

### 访问 Dashboard UI

访问 Dashboard UI 有多种方式；可以使用 kubectl 命令行，或者使用浏览器访问 Kubernetes 主节点的 API 服务器。

#### 命令行代理

您可以使用 kubectl 命令行工具访问 Dashboard，命令如下：

```
kubectl proxy
```

kubectl 会处理与 API 服务器的认证过程，并使得 Dashboard 可以通过

http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

访问。

UI *只能* 通过执行这条命令的机器进行访问。更多选项参见 `kubectl proxy --help`。

Dashboard 支持 Kubeconfig 和 Token 两种认证方式，为了简化配置，我们通过配置文件 `dashboard-admin.yaml` 为 Dashboard 默认用户赋予 admin 权限。

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
```

应用并且获取token。

```shell
kubectl apply -f dashboard-admin.yaml
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```

然后把token填入就行了。

#### 主节点 API 服务器

UI 可以直接通过 Kubernetes 主节点上的 API 服务器访问。打开浏览器，输入 `https://<master-ip>:<apiserver-port>/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`，其中 `<<master-ip>` 是 Kubernetes 主服务器的 IP 地址或域名。

请注意，只有当 API 服务器允许使用用户名密码认证时，这种方式才可以正常工作。但目前对于安装工具（如 `kubeadm`）来说并非如此。关于如何手工设置，参见 [认证管理文档](https://v1-14.docs.kubernetes.io/docs/reference/access-authn-authz/authentication/)。

如果您不知道配置的用户名密码，可以使用 `kubectl config view` 查询。

### 欢迎界面

当访问空集群的 Dashboard 时，您会看到欢迎界面。页面包含一个指向此文档的链接，以及一个用于部署第一个应用程序的按钮。此外，您可以看到在默认情况下有哪些默认系统应用运行在 `kube-system` [命名空间](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/namespaces/) 中，比如 Dashboard 自己。

![Kubernetes Dashboard welcome page](https://d33wubrfki0l68.cloudfront.net/5f56e7ac82f10f46e70403a246c2b93efcf8b5b3/1c09f/images/docs/ui-dashboard-zerostate.png)

### 部署容器化应用

通过一个简单的部署向导，您可以使用 Dashboard 将容器化应用作为一个 Deployment 和可选的 Service 进行创建和部署。可以手工指定应用的详细配置，或者上传一个包含应用配置的 YAML 或 JSON 文件。

想要访问部署向导，可以点击欢迎界面上的各个部署按钮。向导也可以之后通过点击任何页面右上角的 **创建** 按钮进行快速访问。

#### 指定应用的详细配置

部署向导需要您提供以下信息：

-   **应用名称**（必填）：应用的名称。内容为`应用名称`的[标签](https://v1-14.docs.kubernetes.io/docs/concepts/overview/working-with-objects/labels/) 会被添加到任何将被部署的 Deployment 和 Service。

在选定的 Kubernetes [命名空间](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/namespaces/) 中，应用名称必须唯一。必须由小写字母开头，以数字或者小写字母结尾，并且只含有小写字母、数字和中划线（-）。小于等于24个字符。开头和结尾的空格会被忽略。

-   **容器镜像**（必填）：公共镜像仓库上的 Docker [容器镜像](https://v1-14.docs.kubernetes.io/docs/concepts/containers/images/) 或者私有镜像仓库（通常是 Google Container Registery 或者 Docker Hub）的 URL。容器镜像参数说明必须以冒号结尾。

-   **pod 的数量**（必填）：您希望应用程序部署的 Pod 的数量。值必须为正整数。

系统会创建一个 [Deployment](https://v1-14.docs.kubernetes.io/docs/concepts/workloads/controllers/deployment/) 用于保证集群中运行了期望的 Pod 数量。

-   **服务**（可选）：对于部分应用（比如前端），您可能想对外暴露一个 [Service](https://v1-14.docs.kubernetes.io/docs/concepts/services-networking/service/) ，这个 Service（外部 Service）可能用的是集群之外的公网 IP 地址。对于外部 Service 的情况，需要开放一个或者多个端口来满足。更多信息请参考 [这里](/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/。

其它只能对集群内部可见的 Service 称为内部 Service。

不管哪种 Service 类型，如果您选择创建一个 Service，而且容器在一个端口上开启了监听（入向的），那么您需要定义两个端口。创建的 Service 会把（入向的）端口映射到容器可见的目标端口。该 Service 会把流量路由到您部署的 Pod。支持的协议有 TCP 和 UDP。这个 Service 的内部 DNS 解析名就是之前您定义的应用名称的值。

如果需要，您可以打开 **高级选项** 部分，这里您可以定义更多设置：

-   **描述**：这里您输入的文本会作为一个 [注解](https://v1-14.docs.kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) 添加到 Deployment，并显示在应用的详细信息中。

-   **标签**：应用默认使用的 [标签](https://v1-14.docs.kubernetes.io/docs/concepts/overview/working-with-objects/labels/) 是应用名称和版本。您可以为 Deployment、Service（如果有）定义额外的标签，比如 release（版本）、environment（环境）、tier（层级）、partition（分区） 和 release track（版本跟踪）。

例子：

```conf
   release=1.0
   tier=frontend
   environment=pod
   track=stable
```

-   **命名空间**：Kubernetes 支持多个虚拟集群依附于同一个物理集群。这些虚拟集群被称为 [命名空间](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/namespaces/)，可以让您将资源划分为逻辑命名的组。

Dashboard 通过下拉菜单提供所有可用的命名空间，并允许您创建新的命名空间。命名空间的名称最长可以包含 63 个字母或数字和中横线（-），但是不能包含大写字母。 命名空间的名称不能只包含数字。如果名字被设置成一个数字，比如 10，pod 就会被放在默认的命名空间中。

在 namespace 创建成功的情况下，默认会使用新创建的命名空间。如果创建失败，那么第一个命名空间会被选中。

-   **镜像拉取 Secret**：如果要使用私有的 Docker 容器镜像，需要拉取 [secret](https://v1-14.docs.kubernetes.io/docs/concepts/configuration/secret/) 凭证。

Dashboard 通过下拉菜单提供所有可用的 secret，并允许您创建新的 secret。secret 名称必须遵循 DNS 域名语法，比如 `new.image-pull.secret`。secret 的内容必须是 base64 编码的，并且在一个 [`.dockercfg`](https://v1-14.docs.kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod) 文件中声明。secret 名称最大可以包含 253 个字符。

在镜像拉取 secret 创建成功的情况下，默认会使用新创建的 secret。如果创建失败，则不会使用任何 secret。

-   **CPU 需求（核数）**和**内存需求（MiB）**：您可以为容器定义最小的 [资源限制](https://v1-14.docs.kubernetes.io/docs/tasks/configure-pod-container/limit-range/)。默认情况下，Pod 没有 CPU 和内存限制。

-   **运行命令**和**运行命令参数**：默认情况下，您的容器会运行 Docker 镜像的默认 [入口命令](https://v1-14.docs.kubernetes.io/docs/user-guide/containers/#containers-and-commands)。您可以使用 command 选项覆盖默认值。

-   **以特权运行**：这个设置决定了在 [特权容器](https://v1-14.docs.kubernetes.io/docs/user-guide/pods/#privileged-mode-for-pod-containers) 中运行的进程是否像主机中使用 root 运行的进程一样。特权容器可以使用诸如操纵网络堆栈和访问设备的功能。

-   **环境变量**：Kubernetes 通过 [环境变量](https://v1-14.docs.kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/) 暴露 Service。您可以构建环境变量，或者将环境变量的值作为参数传递给您的命令。它们可以被应用用于查找 Service。值可以通过`$(VAR_NAME)` 语法关联其他变量。

#### 上传 YAML 或者 JSON 文件

Kubernetes 支持声明式配置。所有的配置都存储在遵循 Kubernetes [API](https://v1-14.docs.kubernetes.io/docs/concepts/overview/kubernetes-api/) 架构的 YAML 或者 JSON 配置文件中。

作为一种替代在部署向导中指定应用详情的方式，您可以在 YAML 或者 JSON 文件中定义应用，并且使用 Dashboard 上传文件：

### 使用 Dashboard

以下各节描述了 Kubernetes Dashboard UI 视图；包括它们提供的内容，以及怎么使用它们。

#### 导航栏

当在集群中定义 Kubernetes 对象时，Dashboard 会在初始视图中显示它们。默认情况下只会显示 *默认* 命名空间中的对象，可以通过更改导航栏菜单中的命名空间筛选器进行改变。

Dashboard 展示大部分 Kubernetes 对象，并将它们分组放在几个菜单类别中。

##### 管理

集群和命名空间管理的视图。它会列出节点、命名空间和持久卷，并且有它们的详细视图。节点列表视图包含从所有节点聚合的 CPU 和内存使用的度量值。详细信息视图显示了一个节点的度量值，它的规格、状态、分配的资源、事件和这个节点上运行的 Pod。

##### 负载

入口视图显示选中的命名空间中所有运行的应用。视图按照负载类型（如 Deployment、ReplicaSet、StatefulSet 等）罗列应用，并且每种负载都可以单独查看。列表总结了关于负载的可执行信息，比如一个 ReplicaSet 的准备状态的 Pod 数量，或者目前一个 Pod 的内存使用量。

工作负载的详情视图展示了对象的状态、详细信息和相互关系。例如，ReplicaSet 所控制的 Pod，或者 Deployment 关联的 新 ReplicaSet 和 Pod 水平扩展控制器。

##### 服务发现

服务发现视图展示允许暴露给外网服务和允许集群内部发现的 Kubernetes 资源。因此，Service 和 Ingress 视图展示他们关联的 Pod、给集群连接使用的内部端点和给外部用户使用的外部端点。

##### 存储

存储视图展示持久卷申领（PVC）资源，这些资源被应用程序用来存储数据。

##### 配置

配置视图展示的所有 Kubernetes 资源是在集群中运行的应用程序的实时配置。目前来说就是 ConfigMap 和 Secret。通过这个视图可以编辑和管理配置对象，并显示那些默认隐藏的 secret。

##### 日志查看器

Pod 列表和详细信息页面可以链接到 Dashboard 内置的日志查看器。查看器可以钻取属于同一个 Pod 的不同容器的日志。

![日志浏览](https://v1-14.docs.kubernetes.io/images/docs/ui-dashboard-logs-view.png)

### 接下来

更多信息，参见 [Kubernetes Dashboard 项目页面](https://github.com/kubernetes/dashboard).





## 访问集群

本文阐述多种与集群交互的方法。

### 使用 kubectl 完成集群的第一次访问

当您第一次访问 Kubernetes API 的时候，我们建议您使用 Kubernetes CLI，`kubectl`。

访问集群时，您需要知道集群的地址并且拥有访问的凭证。通常，这些在您通过 [Getting started guide](https://kubernetes.io/docs/setup/) 安装集群时都是自动安装好的，或者其他人安装时也应该提供了凭证和集群地址。

通过以下命令检查 kubectl 是否知道集群地址及凭证：

```shell
$ kubectl config view
```

有许多 [例子](https://kubernetes.io/docs/user-guide/kubectl-cheatsheet) 介绍了如何使用 kubectl，可以在 [kubectl手册](https://kubernetes.io/docs/user-guide/kubectl-overview) 中找到更完整的文档。

### 直接访问 REST API

Kubectl 处理 apiserver 的定位和身份验证。 如果要使用 curl 或 wget 等 http 客户端或浏览器直接访问 REST API，可以通过多种方式查找和验证：

-   以代理模式运行 kubectl。
    -   推荐此方式。
    -   使用已存储的 apiserver 地址。
    -   使用自签名的证书来验证 apiserver 的身份。杜绝 MITM 攻击。
    -   对 apiserver 进行身份验证。
    -   未来可能会实现智能化的客户端负载均衡和故障恢复。
-   直接向 http 客户端提供位置和凭据。
    -   可选的方案。
    -   适用于代理可能引起混淆的某些客户端类型。
    -   需要引入根证书到您的浏览器以防止 MITM 攻击。

#### 使用 kubectl 代理

以下命令以反向代理的模式运行kubectl。它处理 apiserver 的定位和验证。 像这样运行：

```shell
$ kubectl proxy --port=8080 &
```

参阅 [kubectl proxy](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#proxy) 获取更多详细信息。

然后，您可以使用 curl、wget 或浏览器访问 API，如果是 IPv6 则用 [::1] 替换 localhost，如下所示：

```shell
$ curl http://localhost:8080/api/
{
  "versions": [
    "v1"
  ]
}
```

#### 不使用 kubectl 代理

在 Kubernetes 1.3 或更高版本中，`kubectl config view` 不再显示 token。使用 `kubectl describe secret ...` 来获取默认服务帐户的 token，如下所示：

`grep/cut` 方法实现：

```shell
$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d ":" | tr -d " ")
$ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\t')
$ curl $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure
{
  "kind": "APIVersions",
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "10.0.1.149:443"
    }
  ]
}
```

`jsonpath` 方法实现：

```shell
$ APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
$ TOKEN=$(kubectl get secret $(kubectl get serviceaccount default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 --decode )
$ curl $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure
{
  "kind": "APIVersions",
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "10.0.1.149:443"
    }
  ]
}
```

上面的例子使用了 `--insecure` 参数，这使得它很容易受到 MITM 攻击。当 kubectl 访问集群时，它使用存储的根证书和客户端证书来访问服务器（这些安装在 `~/.kube` 目录中）。由于集群证书通常是自签名的，因此可能需要特殊配置才能让您的 http 客户端使用根证书。

在一些集群中，apiserver 不需要身份验证；它可能只服务于 localhost，或者被防火墙保护，这个没有一定的标准。 [配置对 API 的访问](https://kubernetes.io/docs/admin/accessing-the-api) 描述了集群管理员如何进行配置。此类方法可能与未来的高可用性支持相冲突。

### 以编程方式访问 API

Kubernetes 官方提供对 [Go](https://kubernetes.io/zh/docs/tasks/access-application-cluster/access-cluster/#go-client) 和 [Python](https://kubernetes.io/zh/docs/tasks/access-application-cluster/access-cluster/#python-client) 的客户端库支持。

#### Go 客户端

-   想要获得这个库，请运行命令：`go get k8s.io/client-go/<version number>/kubernetes`。参阅 <https://github.com/kubernetes/client-go> 来查看目前支持哪些版本。
-   基于这个 client-go 客户端库编写应用程序。 请注意，client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是从主存储库导入 API 定义，例如，`import "k8s.io/client-go/1.4/pkg/api/v1"` 才是对的。

Go 客户端可以像 kubectl CLI 一样使用相同的 [kubeconfig 文件](https://kubernetes.io/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/) 来定位和验证 apiserver。可参阅 [示例](https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go)。

如果应用程序以 Pod 的形式部署在集群中，那么请参阅 [下一章](https://kubernetes.io/zh/docs/tasks/access-application-cluster/access-cluster/#accessing-the-api-from-a-pod)。

#### Python 客户端

如果想要使用 [Python 客户端](https://github.com/kubernetes-client/python)，请运行命令：`pip install kubernetes`。参阅 [Python Client Library page](https://github.com/kubernetes-client/python) 以获得更详细的安装参数。

Python 客户端可以像 kubectl CLI 一样使用相同的 [kubeconfig 文件](https://kubernetes.io/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/) 来定位和验证 apiserver，可参阅 [示例](https://github.com/kubernetes-client/python/tree/master/examples/example1.py)。

#### 其它语言

目前有多个 [客户端库](https://kubernetes.io/docs/reference/using-api/client-libraries/) 为其它语言提供访问 API 的方法。 参阅其它库的相关文档以获取他们是如何验证的。

#### 从 Pod 中访问 API

当你从 Pod 中访问 API 时，定位和验证 apiserver 会有些许不同。

在 Pod 中定位 apiserver 的推荐方式是通过 `kubernetes.default.svc` 这个 DNS 名称，该名称将会解析为服务 IP，然后服务 IP 将会路由到 apiserver。

向 apiserver 进行身份验证的推荐方法是使用 [服务帐户](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) 凭据。 通过 kube-system，pod 与服务帐户相关联，并且该服务帐户的凭证（token）被放置在该 pod 中每个容器的文件系统中，位于 `/var/run/secrets/kubernetes.io/serviceaccount/token`。

如果可用，则将证书放入每个容器的文件系统中的 `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt`，并且应该用于验证 apiserver 的服务证书。

最后，命名空间化的 API 操作所使用的默认命名空间将被放置在每个容器的 `/var/run/secrets/kubernetes.io/serviceaccount/namespace` 文件中。

在 pod 中，建议连接 API 的方法是：

-   在 pod 的 sidecar 容器中运行 `kubectl proxy`，或者以后台进程的形式运行。 这将把 Kubernetes API 代理到当前 pod 的 localhost interface，所以 pod 中的所有容器中的进程都能访问它。
-   使用 Go 客户端库，并使用 `rest.InClusterConfig()` 和 `kubernetes.NewForConfig()`函数创建一个客户端。 他们处理 apiserver 的定位和身份验证。[示例](https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go)

在每种情况下，pod 的凭证都是为了与 apiserver 安全地通信。

### 访问集群中正在运行的服务

上一节介绍了如何连接 Kubernetes API 服务。本节介绍如何连接到 Kubernetes 集群上运行的其他服务。 在 Kubernetes 中，[节点](https://kubernetes.io/docs/admin/node)，[pods](https://kubernetes.io/docs/user-guide/pods) 和 [服务](https://kubernetes.io/docs/user-guide/services) 都有自己的 IP。 在许多情况下，集群上的节点 IP，pod IP 和某些服务 IP 将无法路由，因此无法从集群外部的计算机（例如桌面计算机）访问它们。

#### 连接的方法

有多种方式可以从集群外部连接节点、pod 和服务：

-   通过公共 IP 访问服务。
    -   类型为 `NodePort` 或 `LoadBalancer` 的服务，集群外部可以访问。 请参阅 [服务](https://kubernetes.io/docs/user-guide/services) 和 [kubectl expose](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#expose) 文档。
    -   取决于您的集群环境，该服务可能仅暴露给您的公司网络，或者也可能暴露给整个互联网。 请考虑公开该服务是否安全。它是否进行自己的身份验证？
    -   在服务后端放置 pod。要从一组副本中访问一个特定的 pod，例如进行调试，请在 pod 上放置一个唯一的标签，然后创建一个选择此标签的新服务。
    -   在大多数情况下，应用程序开发人员不应该通过其 nodeIP 直接访问节点。
-   使用 Proxy Verb 访问服务、node 或者 pod。
    -   在访问远程服务之前进行 apiserver 身份验证和授权。 如果服务不能够安全地暴露到互联网，或者服务不能获得节点 IP 端口的访问权限，或者是为了 debug，那么请使用此选项。
    -   代理可能会给一些 web 应用带来问题。
    -   只适用于 HTTP/HTTPS。
    -   更多详细信息在 [这里]。
-   从集群中的 node 或者 pod 中访问。
    -   运行一个 pod，然后使用 [kubectl exec](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#exec) 来连接 pod 里的 shell。 然后从 shell 中连接其它的节点、pod 和服务。
    -   有些集群可能允许您通过 ssh 连接到 node，从那您可能可以访问集群的服务。 这是一个非正式的方式，可能可以运行在个别的集群上。 浏览器和其它一些工具可能没有被安装。集群的 DNS 可能无法使用。

#### 发现内建服务

通常来说，集群中会有 kube-system 创建的一些运行的服务。

通过 `kubectl cluster-info` 命令获得这些服务列表：

```shell
$ kubectl cluster-info

  Kubernetes master is running at https://104.197.5.247
  elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
  kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy
  kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy
  grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
  heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
```

这展示了访问每个服务的 proxy-verb URL。 例如，如果集群启动了集群级别的日志（使用 Elasticsearch），并且传递合适的凭证，那么可以通过 `https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/`进行访问。日志也能通过 kubectl 代理获取，例如： `http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/`。 （参阅 [上面的内容](https://kubernetes.io/zh/docs/tasks/access-application-cluster/access-cluster/#accessing-the-cluster-api) 来获取如何使用 kubectl 代理来传递凭证）

##### 手动构建 apiserver 代理 URL

如上所述，您可以使用 `kubectl cluster-info` 命令来获得服务的代理 URL。要创建包含服务端点、后缀和参数的代理 URL，只需添加到服务的代理 URL： `http://`*kubernetes_master_address*`/api/v1/namespaces/`*namespace_name*`/services/`*service_name[:port_name]*`/proxy`

如果尚未为端口指定名称，则不必在 URL 中指定 *port_name*。

默认情况下，API server 使用 http 代理您的服务。要使用 https，请在服务名称前加上 `https:`： `http://`*kubernetes_master_address*`/api/v1/namespaces/`*namespace_name*`/services/`*https:service_name:[port_name]*`/proxy`

URL 名称段支持的格式为：

-   `<service_name>` - 使用 http 代理到默认或未命名的端口
-   `<service_name>:<port_name>` - 使用 http 代理到指定的端口
-   `https:<service_name>:` - 使用 https 代理到默认或未命名的端口（注意后面的冒号）
-   `https:<service_name>:<port_name>` - 使用 https 代理到指定的端口

##### 示例

-   要访问 Elasticsearch 服务端点 `_search?q=user:kimchy`，您需要使用：`http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy`
-   要访问 Elasticsearch 集群健康信息 `_cluster/health?pretty=true`，您需要使用：`https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true`

```json
  {
    "cluster_name" : "kubernetes_logging",
    "status" : "yellow",
    "timed_out" : false,
    "number_of_nodes" : 1,
    "number_of_data_nodes" : 1,
    "active_primary_shards" : 5,
    "active_shards" : 5,
    "relocating_shards" : 0,
    "initializing_shards" : 0,
    "unassigned_shards" : 5
  }
```

#### 使用 web 浏览器访问运行在集群上的服务

您可以在浏览器地址栏中输入 apiserver 代理 URL。但是：

-   Web 浏览器通常不能传递 token，因此您可能需要使用基本（密码）身份验证。Apiserver 可以配置为接受基本身份验证，但您的集群可能未进行配置。
-   某些 Web 应用程序可能无法运行，尤其是那些使用客户端 javascript 以不知道代理路径前缀的方式构建 URL 的应用程序。

### 请求重定向

已弃用并删除了重定向功能。请改用代理（见下文）。

### 多种代理

使用 Kubernetes 时可能会遇到几种不同的代理：

1.  [kubectl 代理](https://kubernetes.io/zh/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api)：
    -   在用户的桌面或 pod 中运行
    -   代理从本地主机地址到 Kubernetes apiserver
    -   客户端到代理将使用 HTTP
    -   代理到 apiserver 使用 HTTPS
    -   定位 apiserver
    -   添加身份验证 header

1.  [apiserver 代理](https://kubernetes.io/zh/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services)：
    -   内置于 apiserver 中
    -   将集群外部的用户连接到集群 IP，否则这些 IP 可能无法访问
    -   运行在 apiserver 进程中
    -   客户端代理使用 HTTPS（也可配置为 http）
    -   代理将根据可用的信息决定使用 HTTP 或者 HTTPS 代理到目标
    -   可用于访问节点、Pod 或服务
    -   在访问服务时进行负载平衡

1.  [kube proxy](https://kubernetes.io/docs/concepts/services-networking/service/#ips-and-vips)：
    -   运行在每个节点上
    -   代理 UDP 和 TCP
    -   不能代理 HTTP
    -   提供负载均衡
    -   只能用来访问服务

1.  位于 apiserver 之前的 Proxy/Load-balancer：
    -   存在和实现因集群而异（例如 nginx）
    -   位于所有客户和一个或多个 apiserver 之间
    -   如果有多个 apiserver，则充当负载均衡器

1.  外部服务上的云负载均衡器：
    -   由一些云提供商提供（例如 AWS ELB，Google Cloud Load Balancer）
    -   当 Kubernetes 服务类型为 `LoadBalancer` 时自动创建
    -   只使用 UDP/TCP
    -   具体实现因云提供商而异。

除了前两种类型之外，Kubernetes 用户通常不需要担心任何其他问题。集群管理员通常会确保后者的正确配置。





## 配置对多个集群的访问

本文展示如何使用配置文件来配置对多个集群的访问。 在将集群、用户和上下文定义在一个或多个配置文件中之后，用户可以使用 `kubectl config use-context` 命令快速地在集群之间进行切换。

>   **Note:** **注意：** 用于配置集群访问的文件有时被称为 *kubeconfig 文件*。 这是一种引用配置文件的通用方式，并不意味着存在一个名为 `kubeconfig` 的文件。



### 准备工作

需要安装 [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 命令行工具。

### 定义集群、用户和上下文

假设用户有两个集群，一个用于正式开发工作，一个用于其它临时用途（scratch）。 在 `development` 集群中，前端开发者在名为 `frontend` 的名字空间下工作， 存储开发者在名为 `storage` 的名字空间下工作。 在 `scratch` 集群中， 开发人员可能在默认名字空间下工作，也可能视情况创建附加的名字空间。 访问开发集群需要通过证书进行认证。 访问其它临时用途的集群需要通过用户名和密码进行认证。

创建名为 `config-exercise` 的目录。 在 `config-exercise` 目录中，创建名为 `config-demo` 的文件，其内容为：

```yaml
apiVersion: v1
kind: Config
preferences: {}

clusters:
- cluster:
  name: development
- cluster:
  name: scratch

users:
- name: developer
- name: experimenter

contexts:
- context:
  name: dev-frontend
- context:
  name: dev-storage
- context:
  name: exp-scratch
```

配置文件描述了集群、用户名和上下文。 `config-demo` 文件中含有描述两个集群、两个用户和三个上下文的框架。

进入 `config-exercise` 目录。 输入以下命令，将群集详细信息添加到配置文件中：

```shell
kubectl config --kubeconfig=config-demo set-cluster development --server=https://1.2.3.4 --certificate-authority=fake-ca-file
kubectl config --kubeconfig=config-demo set-cluster scratch --server=https://5.6.7.8 --insecure-skip-tls-verify
```

将用户详细信息添加到配置文件中：

```shell
kubectl config --kubeconfig=config-demo set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile
kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password
```

将上下文详细信息添加到配置文件中：

```shell
kubectl config --kubeconfig=config-demo set-context dev-frontend --cluster=development --namespace=frontend --user=developer
kubectl config --kubeconfig=config-demo set-context dev-storage --cluster=development --namespace=storage --user=developer
kubectl config --kubeconfig=config-demo set-context exp-scratch --cluster=scratch --namespace=default --user=experimenter
```

打开 `config-demo` 文件查看添加的详细信息。 也可以使用 `config view` 命令进行查看：

```shell
kubectl config --kubeconfig=config-demo view
```

输出展示了两个集群、两个用户和三个上下文：

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
- cluster:
    insecure-skip-tls-verify: true
    server: https://5.6.7.8
  name: scratch
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
- context:
    cluster: development
    namespace: storage
    user: developer
  name: dev-storage
- context:
    cluster: scratch
    namespace: default
    user: experimenter
  name: exp-scratch
current-context: ""
kind: Config
preferences: {}
users:
- name: developer
  user:
    client-certificate: fake-cert-file
    client-key: fake-key-file
- name: experimenter
  user:
    password: some-password
    username: exp
```

每个上下文包含三部分（集群、用户和名字空间），例如， `dev-frontend` 上下文表明：使用 `developer` 用户的凭证来访问 `development` 集群的 `frontend` 名字空间。

设置当前上下文：

```shell
kubectl config --kubeconfig=config-demo use-context dev-frontend
```

现在当输入 `kubectl` 命令时，相应动作会应用于 `dev-frontend` 上下文中所列的集群和名字空间，同时，命令会使用 `dev-frontend` 上下文中所列用户的凭证。

使用 `--minify` 参数，来查看与当前上下文相关联的配置信息。

```shell
kubectl config --kubeconfig=config-demo view --minify
```

输出结果展示了 `dev-frontend` 上下文相关的配置信息：

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
current-context: dev-frontend
kind: Config
preferences: {}
users:
- name: developer
  user:
    client-certificate: fake-cert-file
    client-key: fake-key-file
```

现在假设用户希望在其它临时用途集群中工作一段时间。

将当前上下文更改为 `exp-scratch`：

```shell
kubectl config --kubeconfig=config-demo use-context exp-scratch
```

现在用户 `kubectl` 下达的任何命令都将应用于 `scratch` 集群的默认名字空间。 同时，命令会使用 `exp-scratch` 上下文中所列用户的凭证。

查看更新后的当前上下文 `exp-scratch` 相关的配置：

```shell
kubectl config --kubeconfig=config-demo view --minify
```

最后，假设用户希望在 `development` 集群中的 `storage` 名字空间下工作一段时间。

将当前上下文更改为 `dev-storage`：

```shell
kubectl config --kubeconfig=config-demo use-context dev-storage
```

查看更新后的当前上下文 `dev-storage` 相关的配置：

```shell
kubectl config --kubeconfig=config-demo view --minify
```

### 创建第二个配置文件

在 `config-exercise` 目录中，创建名为 `config-demo-2` 的文件，其中包含以下内容：

```yaml
apiVersion: v1
kind: Config
preferences: {}

contexts:
- context:
    cluster: development
    namespace: ramp
    user: developer
  name: dev-ramp-up
```

上述配置文件定义了一个新的上下文，名为 `dev-ramp-up`。

### 设置 kubeconfig 环境变量

查看是否有名为 `KUBECONFIG` 的环境变量。 如有，保存 `KUBECONFIG` 环境变量当前的值，以便稍后恢复。 例如，在 Linux 中：

```shell
export  KUBECONFIG_SAVED=$KUBECONFIG
```

`KUBECONFIG` 环境变量是配置文件路径的列表，该列表在 Linux 和 Mac 中以冒号分隔，在 Windows 中以分号分隔。 如果有 `KUBECONFIG` 环境变量，请熟悉列表中的配置文件。

临时添加两条路径到 `KUBECONFIG` 环境变量中。 例如，在 Linux 中：

```shell
export  KUBECONFIG=$KUBECONFIG:config-demo:config-demo-2
```

在 `config-exercise` 目录中输入以下命令：

```shell
kubectl config view
```

输出展示了 `KUBECONFIG` 环境变量中所列举的所有文件合并后的信息。 特别地， 注意合并信息中包含来自 `config-demo-2` 文件的 `dev-ramp-up` 上下文和来自 `config-demo` 文件的三个上下文：

```yaml
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
- context:
    cluster: development
    namespace: ramp
    user: developer
  name: dev-ramp-up
- context:
    cluster: development
    namespace: storage
    user: developer
  name: dev-storage
- context:
    cluster: scratch
    namespace: default
    user: experimenter
  name: exp-scratch
```

更多关于 kubeconfig 文件如何合并的信息，请参考 [使用 kubeconfig 文件组织集群访问](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)

### 探索 $HOME/.kube 目录

如果用户已经拥有一个集群，可以使用 `kubectl` 与集群进行交互。 那么很可能在 `$HOME/.kube` 目录下有一个名为 `config` 的文件。

进入 `$HOME/.kube` 目录， 看看那里有什么文件。 通常会有一个名为 `config` 的文件，目录中可能还有其他配置文件。 请简单地熟悉这些文件的内容。

### 将 $HOME/.kube/config 追加到 KUBECONFIG 环境变量中

如果有 `$HOME/.kube/config` 文件，并且还未列在 `KUBECONFIG` 环境变量中， 那么现在将它追加到 `KUBECONFIG` 环境变量中。 例如，在 Linux 中：

```shell
export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config
```

在配置练习目录中输入以下命令，来查看当前 `KUBECONFIG` 环境变量中列举的所有文件合并后的配置信息：

```shell
kubectl config view
```

### 清理

将 `KUBECONFIG` 环境变量还原为原始值。 例如，在 Linux 中：

```shell
export KUBECONFIG=$KUBECONFIG_SAVED
```

### 接下来

-   [使用 kubeconfig 文件组织集群访问](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
-   [kubectl 配置](https://kubernetes.io/docs/user-guide/kubectl/v1.15/)





## 使用端口转发访问集群中的应用程序

本文展示如何使用 `kubectl port-forward` 连接到在 Kubernetes 集群中运行的 Redis 服务。这种类型的连接对数据库调试很有用。

### 准备工作

-   你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

-   安装 [redis-cli](http://redis.io/topics/rediscli)。

### 创建 Redis deployment 和服务

1.  创建一个 Redis deployment：
```
    kubectl create -f <https://k8s.io/docs/tutorials/stateless-application/guestbook/redis-master-deployment.yaml>
```

查看输出是否成功，以验证是否成功创建 deployment：
```
    deployment "redis-master" created
```
当 pod 是 ready 时，您将得到：
```
   kubectl get pods
    NAME                            READY     STATUS    RESTARTS   AGE
    redis-master-765d459796-258hz   1/1       Running   0          50s

   kubectl get deployment

    NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
    redis-master 1         1         1            1           55s

   kubectl get rs

    NAME                      DESIRED   CURRENT   READY     AGE
    redis-master-765d459796   1         1         1         1m
```

1.  创建一个 Redis 服务：
```
    kubectl create -f <https://k8s.io/docs/tutorials/stateless-application/guestbook/redis-master-service.yaml>

```
查看输出是否成功，以验证是否成功创建服务：
```
    service "redis-master" created
```

检查服务是否创建：
```
   kubectl get svc | grep redis

    NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
    redis-master   ClusterIP   10.0.0.213   <none>        6379/TCP   27s
```

1.  验证 Redis 服务是否运行在 pod 中并且监听 6379 端口：
```
    kubectl get pods redis-master-765d459796-258hz –template=‘{{(index (index .spec.containers 0).ports 0).containerPort}}{{”\n”}}’

```
输出应该显示端口：
```
    6379
```

### 转发一个本地端口到 pod 端口

1.  从 Kubernetes v1.10 开始，`kubectl port-forward` 允许使用资源名称（例如服务名称）来选择匹配的 pod 来进行端口转发。

    ```
    kubectl port-forward redis-master-765d459796-258hz 6379:6379 
    ```

```
这相当于

    kubectl port-forward pods/redis-master-765d459796-258hz 6379:6379
或者

    kubectl port-forward deployment/redis-master 6379:6379 
或者

    kubectl port-forward rs/redis-master 6379:6379 
或者

    kubectl port-forward svc/redis-master 6379:6379
以上所有命令都应该有效。输出应该类似于：

    I0710 14:43:38.274550    3655 portforward.go:225] Forwarding from 127.0.0.1:6379 -> 6379
    I0710 14:43:38.274797    3655 portforward.go:225] Forwarding from [::1]:6379 -> 6379
```

1.  启动 Redis 命令行接口：

    ```
    redis-cli
    ```

1.  在 Redis 命令行提示符下，输入 `ping` 命令：

    ```
    127.0.0.1:6379>ping
    ```

```
成功的 ping 请求应该返回 PONG。
```

### 讨论

与本地 6379 端口建立的连接将转发到运行 Redis 服务器的 pod 的 6379 端口。通过此连接，您可以使用本地工作站来调试在 pod 中运行的数据库。

>   Warning:
>
>   **警告：** 由于已知的限制，目前的端口转发仅适用于 TCP 协议。 在 [issue 47862](https://github.com/kubernetes/kubernetes/issues/47862) 中正在跟踪对 UDP 协议的支持。

------

### 接下来

学习更多关于 [kubectl port-forward](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#port-forward)。







## 使用服务访问集群中的应用程序

本文展示如何创建一个 Kubernetes 服务对象，能让外部客户端访问在集群中运行的应用。该服务为一个应用的两个运行实例提供负载均衡。



### 教程目标

-   运行 Hello World 应用的两个实例。
-   创建一个服务对象来暴露一个 node port。
-   使用服务对象来访问正在运行的应用。

### 准备工作

一个集群。



### 为运行在两个 pod 中的应用创建一个服务：

1.  在您的集群中运行一个 Hello World 应用： 

    ```shell
    kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml
    ```

    上面的命令创建一个 [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) 对象和一个关联的 [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/) 对象。这个 ReplicaSet 有两个 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/)，每个 Pod 都运行着 Hello World 应用。

2.  展示 Deployment 的信息：

    ```shell
    kubectl get deployments hello-world
    kubectl describe deployments hello-world
    ```

3.  展示您的 ReplicaSet 对象信息： 
    ```shell
    kubectl get replicasets kubectl describe replicasets
    ```

4.  创建一个服务对象来暴露 deployment： 
    ```shell
    kubectl expose deployment hello-world --type=NodePort --name=example-service
    ```

5.  展示服务信息： 
    ```shell
    kubectl describe services example-service
    ```
    输出类似于： 

    ```shell
    Name:                   example-service
    Namespace:              default
    Labels:                 run=load-balancer-example
    Annotations:            <none>
    Selector:               run=load-balancer-example
    Type:                   NodePort
    IP:                     10.32.0.16
    Port:                   <unset> 8080/TCP
    TargetPort:             8080/TCP
    NodePort:               <unset> 31496/TCP
    Endpoints:              10.200.1.4:8080,10.200.2.5:8080
    Session Affinity:       None
    Events:                 <none>
    ```

    （译者注：

    -   NodePort是在节点上暴露的端口，外部浏览访问集群的服务的一种方式（另一种是LoadBalancer）
    
    -   targetPort是在容器内暴露的端口
    
    -   port是集群内部服务之间访问服务的入口，集群内其他容器可以通过port来访问服务，但外部如果没有设置nodeport则不能访问。服务把port映射到targetPort来访问容器，port默认与targetPort相同
    
        参考https://blog.csdn.net/yjk13703623757/article/details/79819415）
    
    注意服务中的 NodePort 值。例如在上面的输出中，NodePort 是 31496。
    
6.  列出运行 Hello World 应用的 pod： 

    ```shell
    kubectl get pods --selector="run=load-balancer-example" --output=wide
    ```

    输出大致是：

    ```shell
    NAME                           READY   STATUS    ...  IP           NODE
    hello-world-2895499144-bsbk5   1/1     Running   ...  10.200.1.4   worker1
    hello-world-2895499144-m1pwt   1/1     Running   ...  10.200.2.5   worker2
    ```

7.  获取运行 Hello World 的 pod 的其中一个节点的公共 IP 地址。如何获得此地址取决于您设置集群的方式。 例如，如果您使用的是 Minikube，则可以通过运行 `kubectl cluster-info` 来查看节点地址。 如果您使用的是 Google Compute Engine 实例，则可以使用 `gcloud compute instances list` 命令查看节点的公共地址。

8.  在您选择的节点上，创建一个防火墙规则以开放 node port 上的 TCP 流量。 例如，如果您的服务的 NodePort 值为 31568，请创建一个防火墙规则以允许 31568 端口上的 TCP 流量。 不同的云提供商提供了不同方法来配置防火墙规则。

9.  使用节点地址和 node port 来访问 Hello World 应用：

    ```shell
    curl http://<public-node-ip>:<node-port>
    ```

    这里的 `<public-node-ip>` 是您节点的公共 IP 地址，`<node-port>` 是您服务的 NodePort 值。 对于请求成功的响应是一个 hello 消息：

    ```shell
    Hello Kubernetes!
    ```

### 使用服务配置文件

作为 `kubectl expose` 的替代方法，您可以使用 [服务配置文件](https://kubernetes.io/docs/concepts/services-networking/service/) 来创建服务。

### 清理现场

想要删除服务，输入以下命令：

```
kubectl delete services example-service
```

想要删除运行 Hello World 应用的 Deployment、ReplicaSet 和 Pod，输入以下命令：

```
kubectl delete deployment hello-world
```

### 接下来

学习更多关于如何 [通过服务连接应用](https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/)。





## 使用服务将前端连接到后端

本任务会描述如何创建前端微服务和后端微服务。后端微服务是一个 hello 欢迎程序。 前端和后端的连接是通过 Kubernetes 服务对象（Service object）完成的。

### 教程目标

-   使用部署对象（Deployment object）创建并运行一个微服务
-   从后端将流量路由到前端
-   使用服务对象把前端应用连接到后端应用

### 准备工作

-   一个集群

-   本任务使用 [外部负载均衡服务](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/)， 所以需要对应的可支持此功能的环境。如果你的环境不能支持，你可以使用 [NodePort](https://kubernetes.io/docs/user-guide/services/#type-nodeport) 类型的服务代替。

### 使用部署对象（Deployment）创建后端

后端是一个简单的 hello 欢迎微服务应用。这是后端应用的 Deployment 配置文件：

[`hello.yaml docs/tasks/access-application-cluster`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/access-application-cluster/hello.yaml) 

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: hello
spec:
  replicas: 7
  template:
    metadata:
      labels:
        app: hello
        tier: backend
        track: stable
    spec:
      containers:
        - name: hello
          image: "gcr.io/google-samples/hello-go-gke:1.0"
          ports:
            - name: http
              containerPort: 80
```

创建后端 Deployment：

```
kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/hello.yaml
```

查看后端的 Deployment 信息：

```
kubectl describe deployment hello
```

输出类似于：

```
Name:                           hello
Namespace:                      default
CreationTimestamp:              Mon, 24 Oct 2016 14:21:02 -0700
Labels:                         app=hello
                                tier=backend
                                track=stable
Selector:                       app=hello,tier=backend,track=stable
Replicas:                       7 updated | 7 total | 7 available | 0 unavailable
StrategyType:                   RollingUpdate
MinReadySeconds:                0
RollingUpdateStrategy:          1 max unavailable, 1 max surge
OldReplicaSets:                 <none>
NewReplicaSet:                  hello-3621623197 (7/7 replicas created)
Events:
...
```

### 创建后端服务对象（Service object）

前端连接到后端的关键是 Service。Service 创建一个固定 IP 和 DNS 解析名入口， 使得后端微服务可达。Service 使用 selector 标签来寻找目标 Pod。

首先，浏览 Service 的配置文件：

[`hello-service.yaml docs/tasks/access-application-cluster`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/access-application-cluster/hello-service.yaml) 

```yaml
kind: Service
apiVersion: v1
metadata:
  name: hello
spec:
  selector:
    app: hello
    tier: backend
  ports:
  - protocol: TCP
    port: 80
    targetPort: http
```

配置文件中，你可以看到 Service 将流量路由到包含 `app: hello` 和 `tier: backend` 标签的 Pod。

创建 `hello` Service：

```
kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/hello-service.yaml
```

此时，你已经有了一个在运行的后端 Deployment，你也有了一个 Service 用于路由网络流量。

### 创建前端应用

既然你已经有了后端应用，你可以创建一个前端应用连接到后端。前端应用通过 DNS 名连接到后端的工作 Pods。 DNS 名是 “hello”，也就是 Service 配置文件中 `name` 字段的值。

前端 Deployment 中的 Pods 运行一个 nginx 镜像，这个已经配置好镜像去寻找后端的 hello Service。 只是 nginx 的配置文件：

[`frontend/frontend.conf docs/tasks/access-application-cluster/frontend`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/access-application-cluster/frontend/frontend.conf) 

```conf
upstream hello {
    server hello;
}

server {
    listen 80;

    location / {
        proxy_pass http://hello;
    }
}
```

与后端类似，前端用包含一个 Deployment 和一个 Service。Service 的配置文件包含了 `type: LoadBalancer`， 也就是说，Service 会使用你的云服务商的默认负载均衡设备。

[`frontend.yaml docs/tasks/access-application-cluster`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/access-application-cluster/frontend.yaml) 

```yaml
kind: Service
apiVersion: v1
metadata:
  name: frontend
spec:
  selector:
    app: hello
    tier: frontend
  ports:
  - protocol: "TCP"
    port: 80
    targetPort: 80
  type: LoadBalancer
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hello
        tier: frontend
        track: stable
    spec:
      containers:
      - name: nginx
        image: "gcr.io/google-samples/hello-frontend:1.0"
        lifecycle:
          preStop:
            exec:
              command: ["/usr/sbin/nginx","-s","quit"]
```

创建前端 Deployment 和 Service：

```
kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/frontend.yaml
```

通过输出确认两个资源都已经被创建：

```
deployment "frontend" created
service "frontend" created
```

**注意**：这个 nginx 配置文件是被打包在 [容器镜像](https://kubernetes.io/docs/tasks/access-application-cluster/frontend/Dockerfile) 里的。 更好的方法是使用 [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/)，这样的话你可以更轻易地更改配置。

### 与前端 Service 交互

一旦你创建了 LoadBalancer 类型的 Service，你可以使用这条命令查看外部 IP：

```
kubectl get service frontend
```

外部 IP 字段的生成可能需要一些时间。如果是这种情况，外部 IP 会显示为 `<pending>`。

```
NAME       CLUSTER-IP      EXTERNAL-IP   PORT(S)  AGE
frontend   10.51.252.116   <pending>     80/TCP   10s
```

使用相同的命令直到它显示外部 IP 地址：

```
NAME       CLUSTER-IP      EXTERNAL-IP        PORT(S)  AGE
frontend   10.51.252.116   XXX.XXX.XXX.XXX    80/TCP   1m
```

### 通过前端发送流量

前端和后端已经完成连接了。你可以使用 curl 命令通过你的前端 Service 的外部 IP 访问服务端点。

```
curl http://<EXTERNAL-IP>
```

后端生成的消息输出如下：

```
{"message":"Hello"}
```

### 接下来

-   了解更多 [Services](https://kubernetes.io/docs/concepts/services-networking/service/)
-   了解更多 [ConfigMaps](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/)





## 创建一个外部负载平衡器

本文展示如何创建一个外部负载均衡器。

创建服务时，您可以选择自动创建云网络负载均衡器。这提供了一个外部可访问的 IP 地址，可将流量分配到集群节点上的正确端口上 _假设集群在支持的环境中运行，并配置了正确的云负载平衡器提供商包_。

有关如何配置和使用 Ingress 资源以为服务提供外部可访问的 URL、负载均衡流量、终止 SSL 等功能，请查看 [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) 文档。

### 准备工作

一个集群。

### 配置文件

要创建外部负载均衡器，请将以下内容添加到 [服务配置文件](https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer)：

```json
    "type": "LoadBalancer"
```

您的配置文件可能会如下所示：

```json
    {
      "kind": "Service",
      "apiVersion": "v1",
      "metadata": {
        "name": "example-service"
      },
      "spec": {
        "ports": [{
          "port": 8765,
          "targetPort": 9376
        }],
        "selector": {
          "app": "example"
        },
        "type": "LoadBalancer"
      }
    }
```

### 使用 kubectl

您也可以使用 `kubectl expose` 命令及其 `--type=LoadBalancer` 参数创建服务：

```bash
kubectl expose rc example --port=8765 --target-port=9376 \
        --name=example-service --type=LoadBalancer
```

此命令通过使用与引用资源（在上面的示例的情况下，名为 `example` 的 replication controller）相同的选择器来创建一个新的服务。

更多信息（包括更多的可选参数），请参阅 [`kubectl expose` reference](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#expose)。

### 找到您的 IP 地址

您可以通过 `kubectl` 获取服务信息，找到为您的服务创建的 IP 地址：

```bash
kubectl describe services example-service
```

这将获得如下输出：

```bash
    Name:                   example-service
    Namespace:              default
    Labels:                 <none>
    Annotations:            <none>
    Selector:               app=example
    Type:                   LoadBalancer
    IP:                     10.67.252.103
    LoadBalancer Ingress:   192.0.2.89
    Port:                   <unnamed> 80/TCP
    NodePort:               <unnamed> 32445/TCP
    Endpoints:              10.64.0.4:80,10.64.1.5:80,10.64.2.4:80
    Session Affinity:       None
    Events:                 <none>
```

IP 地址列在 `LoadBalancer Ingress` 旁边。



>   Note:
>
>   
>
>   
>
>   **注意：** 如果您在 Minikube 上运行服务，您可以通过以下命令找到分配的 IP 地址和端口：



```bash
minikube service example-service --url
```

### 保留客户端源 IP

由于此功能的实现，目标容器中看到的源 IP 将 *不是客户端的原始源 IP*。要启用保留客户端 IP，可以在服务的 spec 中配置以下字段（支持 GCE/Google Kubernetes Engine 环境）：

-   `service.spec.externalTrafficPolicy` - 表示此服务是否希望将外部流量路由到节点本地或集群范围的端点。有两个可用选项：”Cluster”（默认）和 “Local”。”Cluster” 隐藏了客户端源 IP，可能导致第二跳到另一个节点，但具有良好的整体负载分布。 “Local” 保留客户端源 IP 并避免 LoadBalancer 和 NodePort 类型服务的第二跳，但存在潜在的不均衡流量传播风险。
-   `service.spec.healthCheckNodePort` - 指定服务的 healthcheck nodePort（数字端口号）。如果未指定，则 serviceCheckNodePort 由服务 API 后端使用已分配的 nodePort 创建。如果客户端指定，它将使用客户端指定的 nodePort 值。仅当 type 设置为 “LoadBalancer” 并且 externalTrafficPolicy 设置为 “Local” 时才生效。

可以通过在服务的配置文件中将 `externalTrafficPolicy` 设置为 “Local” 来激活此功能。

```json
    {
      "kind": "Service",
      "apiVersion": "v1",
      "metadata": {
        "name": "example-service"
      },
      "spec": {
        "ports": [{
          "port": 8765,
          "targetPort": 9376
        }],
        "selector": {
          "app": "example"
        },
        "type": "LoadBalancer",
        "externalTrafficPolicy": "Local"
      }
    }
```

#### 特性可用性

| k8s 版本  | 特性支持             |
| :-------- | :------------------- |
| 1.7+      | 支持完整的 API 字段  |
| 1.5 - 1.6 | 支持 Beta Annotation |
| <1.5      | 不支持               |

您可以在下面找到已弃用的 Beta annotation，在稳定版本前使用它来使用该功能。较新的 Kubernetes 版本可能会在 v1.7 之后停止支持这些功能。 请更新现有应用程序以直接使用这些字段。

-   `service.beta.kubernetes.io/external-traffic` annotation <-> `service.spec.externalTrafficPolicy` 字段
-   `service.beta.kubernetes.io/healthcheck-nodeport` annotation <-> `service.spec.healthCheckNodePort` 字段

`service.beta.kubernetes.io/external-traffic` annotation 与 `service.spec.externalTrafficPolicy` 字段相比拥有一组不同的值。值匹配如下：

-   “OnlyLocal” annotation <-> “Local” 字段
-   “Global” annotation <-> “Cluster” 字段

**请注意，此功能目前尚未实现在所有云提供商/环境中。**

已知的问题：

-   AWS: [kubernetes/kubernetes#35758](https://github.com/kubernetes/kubernetes/issues/35758)
-   Weave-Net: [weaveworks/weave/#2924](https://github.com/weaveworks/weave/issues/2924)

### 外部负载均衡器提供商

请务必注意，此功能的数据路径由 Kubernetes 集群外部的负载均衡器提供。

当服务类型设置为 `LoadBalancer` 时，Kubernetes 向集群中的 pod 提供与 `type=<ClusterIP>` 等效的功能，并通过使用 Kubernetes pod 的条目对负载均衡器（从外部到 Kubernetes）进行编程来扩展它。 Kubernetes 服务控制器自动创建外部负载均衡器，健康检查（如果需要），防火墙规则（如果需要），并获取云提供商分配的外部 IP 并将其填充到服务对象中。

### 保留源 IP 时的注意事项和限制

GCE/AWS 负载均衡器不为其目标池提供权重。对于旧的 LB kube-proxy 规则来说，这不是一个问题，它可以在所有端点之间正确平衡。

使用新功能，外部流量不会在 pod 之间平均负载，而是在节点级别平均负载（因为 GCE/AWS 和其他外部 LB 实现无法指定每个节点的权重，因此它们的平衡跨所有目标节点，并忽略每个节点上的 pod 数量）。

但是，我们可以声明，对于 NumServicePods << NumNodes 或 NumServicePods >> NumNodes 时，即使没有权重，也会看到接近相等的分布。

一旦外部负载平衡器提供权重，就可以将此功能添加到 LB 编程路径中。 *未来工作：1.4 版本不提供权重支持，但可能会在将来版本中添加*

内部 pod 到 pod 的流量应该与 ClusterIP 服务类似，所有 pod 的概率相同。







## 配置云提供商的防火墙

许多云服务提供商（比如 谷歌计算引擎）定义防火墙以防止服务无意间暴露到互联网上。 当暴露服务给外网时，你可能需要在防火墙上开启一个或者更多的端口来支持服务。 本文描述了这个过程，以及其他云服务商的具体信息。

### 准备工作

一个集群。



### 负载均衡（LoadBalancer）服务的访问限制

当以 `spec.type: LoadBalancer` 方式配置服务时，你可以使用 `spec.loadBalancerSourceRanges` 来指定允许访问负载均衡器的 ip 段。 这个字段采用 CIDR 的 IP 段， kubernetes 使用该段配置防火墙。目前只有 谷歌计算引擎，谷歌云原生引擎，亚马逊弹性原生云服务 和 微软云原生平台支持此功能。 如果云服务提供商不支持这个功能，这个字段将被忽略。

假设内部子网为假设10.0.0.0/8，在下面这个例子中，将创建一个仅能由群集内部IP访问的负载均衡器。此负载均衡器不允许来自 kubernetes 集群外部客户端的访问。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  ports:
  - port: 8765
    targetPort: 9376
  selector:
    app: example
  type: LoadBalancer
  loadBalancerSourceRanges:
  - 10.0.0.0/8
```

在下面这个例子中，将创建一个只能被 IP 为 130.211.204.1 和 130.211.204.2 的客户端访问的负载据衡器。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  ports:
  - port: 8765
    targetPort: 9376
  selector:
    app: example
  type: LoadBalancer
  loadBalancerSourceRanges:
  - 130.211.204.1/32
  - 130.211.204.2/32
```

### 谷歌计算引擎 （Google Compute Engine）

当以 `spec.type: LoadBalancer` 方式配置服务时，该服务的防火墙将自动打开。 当以 `spec.type: NodePort` 方式配置服务时，该服务的防火墙在默认情况下不会打开。

谷歌计算引擎的防火墙会进行记录 [他处](https://cloud.google.com/compute/docs/networking#firewalls_1)。

你也可以使用 gcloud 命令行工具自行添加防火墙：

```shell
gcloud compute firewall-rules create my-rule --allow=tcp:<port>
```

>   Note:
>
>   GCE 防火墙是按照虚拟机来定义的，而不是通过ip地址来定义的。 这就意味着当你在防火墙上打开一个服务端口时，任何在那台虚拟机 IP 上的同一端口的服务 都有被外部访问的潜在可能。需要注意的是，对于其他的 kubernetes 服务而言，这不是问题。 因为他们监听的ip 地址与主机节点外部的 ip 地址并不相同。
>
>   试想一下：
>
>   -   你建立一个（ ip 地址为1.2.3.4）端口为80的外部负载均衡器
>
>   -   因为在防火墙上为集群的所有节点都打开了 80 端口，所以外部的服务可以向你的 服务发送数据包。
>
>   -   最后你又虚拟机上的80端口启动 nginx 服务器（ip地址2.3.4.5）。 这个 nginx 在虚拟机的外部 IP 地址上也被暴露到了互联网上。
>
>   因此请务必小心，在谷歌计算引擎或者谷歌云原生引擎中打开防火墙时，可能无意间把其他服务也暴露给了互联网。

### 反馈



## 列出集群中运行的所有容器镜像

本文展示如何使用 kubectl 来列出集群中所有运行 pod 的容器的镜像

### 准备工作

一个集群

在本练习中，您将使用 kubectl 来获取集群中运行的所有 Pod，并格式化输出来提取每个 pod 中的容器列表。

### 列出所有命名空间下的所有容器

-   使用 `kubectl get pods --all-namespaces` 获取所有命名空间下的所有 Pod
-   使用 `-o jsonpath={..image}` 来格式化输出，以仅包含容器镜像名称。 这将以递归方式从返回的 json 中解析出 `image` 字段。
    -   参阅 [jsonpath reference](https://kubernetes.io/docs/user-guide/jsonpath/) 来获取更多关于如何使用 jsonpath 的信息。
-   使用标准化工具来格式化输出：`tr`, `sort`, `uniq`
    -   使用 `tr` 以用换行符替换空格
    -   使用 `sort` 来对结果进行排序
    -   使用 `uniq` 来聚合镜像计数

```sh
kubectl get pods --all-namespaces -o jsonpath="{..image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c
```

上面的命令将递归获取所有返回项目的名为 `image` 的字段。

作为替代方案，可以使用 Pod 的镜像字段的绝对路径。这确保即使字段名称重复的情况下也能检索到正确的字段，例如，特定项目中的许多字段都称为 `name`：

```sh
kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}"
```

jsonpath 解释如下：

-   `.items[*]`: 对于每个返回的值
-   `.spec`: 获取 spec
-   `.containers[*]`: 对于每个容器
-   `.image`: 获取镜像

>   Note:
>
>   **注意：** 按名字获取单个 Pod 时，例如 `kubectl get pod nginx`，路径的 `.items[*]`部分应该省略，因为返回的是一个 Pod 而不是一个项目列表。

### 列出 Pod 中的容器

可以使用 `range` 操作进一步控制格式化，以单独操作每个元素。

```sh
kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}' |\
sort
```

### 列出以 label 过滤后的 Pod 的所有容器

要获取匹配特定标签的 Pod，请使用 -l 参数。以下匹配仅与标签 `app=nginx` 相符的 Pod。

```sh
kubectl get pods --all-namespaces -o=jsonpath="{..image}" -l app=nginx
```

### 列出以命名空间过滤后的 Pod 的所有容器

要获取匹配特定命名空间的 Pod，请使用 namespace 参数。以下仅匹配 `kube-system` 命名空间下的 Pod。

```sh
kubectl get pods --namespace kube-system -o jsonpath="{..image}"
```

### 使用 go-template 代替 jsonpath 来获取容器

作为 jsonpath 的替代，Kubectl 支持使用 [go-templates](https://golang.org/pkg/text/template/) 来格式化输出：

```sh
kubectl get pods --all-namespaces -o go-template --template="{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}"
```

### 接下来

#### 参考

-   [Jsonpath](https://kubernetes.io/docs/user-guide/jsonpath/) 参考指南
-   [Go template](https://golang.org/pkg/text/template/) 参考指南







## （未翻译）使用NGINX Ingress控制器在Minikube上设置Ingress

An [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) is an API object that defines rules which allow external access to services in a cluster. An [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) fulfills the rules set in the Ingress.

>   **Caution:** For the Ingress resource to work, the cluster **must** also have an Ingress controller running.

This page shows you how to set up a simple Ingress which routes requests to Service web or web2 depending on the HTTP URI.

### Before you begin

You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using [Minikube](https://kubernetes.io/docs/setup/minikube), or you can use one of these Kubernetes playgrounds:

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

### Create a Minikube cluster

1.  Click **Launch Terminal**

    [Launch Terminal](https://www.katacoda.com/courses/kubernetes/playground)

    

2.  (Optional) If you installed Minikube locally, run the following command:

    ```shell
    minikube start
    ```

### Enable the Ingress controller

1.  To enable the NGINX Ingress controller, run the following command:

    ```shell
    minikube addons enable ingress
    ```

2.  Verify that the NGINX Ingress controller is running

    ```shell
    kubectl get pods -n kube-system
    ```

    >   **Note:** This can take up to a minute.

    Output:

    ```shell
    NAME                                        READY     STATUS    RESTARTS   AGE
    default-http-backend-59868b7dd6-xb8tq       1/1       Running   0          1m
    kube-addon-manager-minikube                 1/1       Running   0          3m
    kube-dns-6dcb57bcc8-n4xd4                   3/3       Running   0          2m
    kubernetes-dashboard-5498ccf677-b8p5h       1/1       Running   0          2m
    nginx-ingress-controller-5984b97644-rnkrg   1/1       Running   0          1m
    storage-provisioner                         1/1       Running   0          2m
    ```

### Deploy a hello, world app

1.  Create a Deployment using the following command:

    ```shell
    kubectl run web --image=gcr.io/google-samples/hello-app:1.0 --port=8080
    ```

    Output:

    ```shell
    deployment.apps/web created
    ```

2.  Expose the Deployment:

    ```shell
    kubectl expose deployment web --target-port=8080 --type=NodePort
    ```

    Output:

    ```shell
    service/web exposed
    ```

3.  Verify the Service is created and is available on a node port:

    ```shell
    kubectl get service web
    ```

    Output:

    ```shell
    NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    web       NodePort   10.104.133.249   <none>        8080:31637/TCP   12m
    ```

4.  Visit the service via NodePort:

    ```shell
    minikube service web --url
    ```

    Output:

    ```shell
    http://172.17.0.15:31637
    ```

    >   **Note:** Katacoda environment only: at the top of the terminal panel, click the plus sign, and then click **Select port to view on Host 1**. Enter the NodePort, in this case `31637`, and then click **Display Port**.

    Output:

    ```shell
    Hello, world!
    Version: 1.0.0
    Hostname: web-55b8c6998d-8k564
    ```

    You can now access the sample app via the Minikube IP address and NodePort. The next step lets you access the app using the Ingress resource.

### Create an Ingress resource

The following file is an Ingress resource that sends traffic to your Service via hello-world.info.

1.  Create `example-ingress.yaml` from the following file:

    ```
    apiVersion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1
    kind: Ingress
    metadata:
      name: example-ingress
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    spec:
     rules:
     - host: hello-world.info
       http:
         paths:
         - path: /*
           backend:
             serviceName: web
             servicePort: 8080
    ```

2.  Create the Ingress resource by running the following command:

    ```shell
    kubectl apply -f example-ingress.yaml
    ```

    Output:

    ```shell
    ingress.networking.k8s.io/example-ingress created
    ```

3.  Verify the IP address is set:

    ```shell
    kubectl get ingress
    ```

    >   **Note:** This can take a couple of minutes.

    ```shell
    NAME              HOSTS              ADDRESS       PORTS     AGE
    example-ingress   hello-world.info   172.17.0.15   80        38s
    ```

4.  Add the following line to the bottom of the `/etc/hosts` file.

    >   **Note:** If you are running Minikube locally, use `minikube ip` to get the external IP. The IP address displayed within the ingress list will be the internal IP.

    ```
    172.17.0.15 hello-world.info
    ```

    This sends requests from hello-world.info to Minikube.

5.  Verify that the Ingress controller is directing traffic:

    ```shell
    curl hello-world.info
    ```

    Output:

    ```shell
    Hello, world!
    Version: 1.0.0
    Hostname: web-55b8c6998d-8k564
    ```

    >   **Note:** If you are running Minikube locally, you can visit hello-world.info from your browser.

### Create Second Deployment

1.  Create a v2 Deployment using the following command:

    ```shell
    kubectl run web2 --image=gcr.io/google-samples/hello-app:2.0 --port=8080
    ```

    Output:

    ```shell
    deployment.apps/web2 created
    ```

2.  Expose the Deployment:

    ```shell
    kubectl expose deployment web2 --target-port=8080 --type=NodePort
    ```

    Output:

    ```shell
    service/web2 exposed
    ```

### Edit Ingress

1.  Edit the existing `example-ingress.yaml` and add the following lines:

    ```yaml
         - path: /v2/*
           backend:
             serviceName: web2
             servicePort: 8080
    ```

2.  Apply the changes:

    ```shell
    kubectl apply -f example-ingress.yaml
    ```

    Output:

    ```shell
    ingress.extensions/example-ingress configured
    ```

### Test Your Ingress

1.  Access the 1st version of the Hello World app.

    ```shell
    curl hello-world.info
    ```

    Output:

    ```shell
    Hello, world!
    Version: 1.0.0
    Hostname: web-55b8c6998d-8k564
    ```

2.  Access the 2nd version of the Hello World app.

    ```shell
    curl hello-world.info/v2
    ```

    Output:

    ```shell
    Hello, world!
    Version: 2.0.0
    Hostname: web2-75cd47646f-t8cjk
    ```

    >   **Note:** If you are running Minikube locally, you can visit hello-world.info and hello-world.info/v2 from your browser.

### What's next

-   Read more about [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)
-   Read more about [Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)
-   Read more about [Services](https://kubernetes.io/docs/concepts/services-networking/service/)







## 同一Pod中的容器之间使用共享卷通信

本文旨在说明如何让一个 Pod 内的两个容器使用一个卷（Volume）进行通信。

### 准备工作

你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

### 创建一个包含两个容器的 Pod

在这个练习中，你会创建一个包含两个容器的 Pod。两个容器共享一个卷用于他们之间的通信。 Pod 的配置文件如下：

[`two-container-pod.yaml docs/tasks/access-application-cluster`](https://github.com/kubernetes/website/blob/master/content/zh/docs/tasks/access-application-cluster/two-container-pod.yaml)

```yaml
two-container-pod.yaml docs/tasks/access-application-cluster 

apiVersion: v1
kind: Pod
metadata:
  name: two-containers
spec:

  restartPolicy: Never

  volumes:
  - name: shared-data
    emptyDir: {}

  containers:

  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html

  - name: debian-container
    image: debian
    volumeMounts:
    - name: shared-data
      mountPath: /pod-data
    command: ["/bin/sh"]
    args: ["-c", "echo Hello from the debian container > /pod-data/index.html"]
```

在配置文件中，你可以看到 Pod 有一个共享卷，名为 `shared-data`。

配置文件中的第一个容器运行了一个 nginx 服务器。共享卷的挂载路径是 `/usr/share/nginx/html`。 第二个容器是基于 debian 镜像的，有一个 `/pod-data` 的挂载路径。第二个容器运行了下面的命令然后终止。

```
echo Hello from the debian container > /pod-data/index.html
```

注意，第二个容器在 nginx 服务器的根目录下写了 `index.html` 文件。

创建一个包含两个容器的 Pod：

```
kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/two-container-pod.yaml
```

查看 Pod 和容器的信息：

```
kubectl get pod two-containers --output=yaml
```

这是输出的一部分：

```
apiVersion: v1
kind: Pod
metadata:
  ...
  name: two-containers
  namespace: default
  ...
spec:
  ...
  containerStatuses:

  - containerID: docker://c1d8abd1 ...
    image: debian
    ...
    lastState:
      terminated:
        ...
    name: debian-container
    ...

  - containerID: docker://96c1ff2c5bb ...
    image: nginx
    ...
    name: nginx-container
    ...
    state:
      running:
    ...
```

你可以看到 debian 容器已经被终止了，而 nginx 服务器依然在运行。

进入 nginx 容器的 shell：

```
kubectl exec -it two-containers -c nginx-container -- /bin/bash
```

在 shell 中，确认 nginx 还在运行。

```
root@two-containers:/# ps aux
```

输出类似于这样：

```
USER       PID  ...  STAT START   TIME COMMAND
root         1  ...  Ss   21:12   0:00 nginx: master process nginx -g daemon off;
```

回忆一下，debian 容器在 nginx 的根目录下创建了 `index.html` 文件。 使用 `curl` 向 nginx 服务器发送一个 GET 请求：

```
root@two-containers:/# apt-get update
root@two-containers:/# apt-get install curl
root@two-containers:/# curl localhost
```

输出表示 nginx 提供了 debian 容器写的页面：

```
Hello from the debian container
```

### 讨论

Pod 能有多个容器的主要原因是为了支持辅助应用（helper applications），以协助主应用（primary application）。 辅助应用的典型例子是数据抽取，数据推送和代理。辅助应用和主应用经常需要相互通信。 就如这个练习所示，通信通常是通过共享文件系统完成的，或者，也通过回环网络接口 localhost 完成。 举个网络接口的例子，web 服务器带有一个协助程序用于拉取 Git 仓库的更新。

在本练习中的卷为 Pod 生命周期中的容器相互通信提供了一种方法。如果 Pod 被删除或者重建了， 任何共享卷中的数据都会丢失。

### 接下来

-   更多学习内容 [混合容器的方式](http://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns.html)。
-   学习 [模块化架构的混合容器](http://www.slideshare.net/Docker/slideshare-burns)。
-   参见 [配置一个使用存储卷的 Pod](https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/)。
-   参见 [卷](https://kubernetes.io/docs/api-reference/v1.15/#volume-v1-core)。
-   参见 [Pod](https://kubernetes.io/docs/api-reference/v1.15/#pod-v1-core).





## 为集群配置DNS

Kubernetes 提供 DNS 集群插件，大多数支持的环境默认情况下都会启用。

有关如何为 Kubernetes 集群配置 DNS 的详细信息，请参阅 [Kubernetes DNS 插件示例.](https://github.com/kubernetes/kubernetes/tree/release-1.5/examples/cluster-dns)