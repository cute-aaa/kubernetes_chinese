# 管理集群

## 使用kubeadm来管理

### 使用kubeadm管理证书

**FEATURE STATE:** `Kubernetes v1.15` [stable](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#)

 [kubeadm](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/) 生成的客户端证书将在1年后过期。本页面解释如何使用kubeadm管理证书续订。

#### 准备工作

熟悉[Kubernetes的PKI证书和要求](https://kubernetes.io/docs/setup/certificates/).。

#### 检查证书到期时间

`check-expiration` 可以用来检查证书到期时间。

```
kubeadm alpha certs check-expiration
```

输出大致是：

```
CERTIFICATE                EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
admin.conf                 May 15, 2020 13:03 UTC   364d            false
apiserver                  May 15, 2020 13:00 UTC   364d            false
apiserver-etcd-client      May 15, 2020 13:00 UTC   364d            false
apiserver-kubelet-client   May 15, 2020 13:00 UTC   364d            false
controller-manager.conf    May 15, 2020 13:03 UTC   364d            false
etcd-healthcheck-client    May 15, 2020 13:00 UTC   364d            false
etcd-peer                  May 15, 2020 13:00 UTC   364d            false
etcd-server                May 15, 2020 13:00 UTC   364d            false
front-proxy-client         May 15, 2020 13:00 UTC   364d            false
scheduler.conf             May 15, 2020 13:03 UTC   364d            false
```

该命令显示`/etc/kubernetes/pki` 文件夹下的客户端证书和kubeadm使用的KUBECONFIG文件(`admin.conf`, `controller-manager.conf` 和 `scheduler.conf`)中嵌入的客户端证书的过期/剩余时间。

此外，如果证书是外部管理的，kubeadm会通知用户;在这种情况下，用户应该手动/使用其他工具管理证书更新。

>   **警告：** `kubeadm` 不能管理由外部CA签名的证书

>   **Note:** `kubelet.conf` 没有包含在上面的列表中，因为kubeadm 将kubelet配置为自动证书更新

#### 自动更新证书

`kubeadm` 在控制面板 [升级](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-15/) 时更新所有证书。

这个特性是为解决最简单的用例而设计的;如果您对证书更新没有特定的要求，并且定期执行Kubernetes版本升级(每次升级之间间隔不到1年)，kubeadm将负责使您的集群保持最新且相当安全。

>   **注意：** 为了保持安全，最好经常升级集群。

如果你对更新证书有更复杂的需求，可以通过将 `--certificate-renewal=false` 传递给 `kubeadm upgrade apply` 或 `kubeadm upgrade node` 节点来选择退出默认行为。

#### 手动更新证书

可以使用 `kubeadm alpha certs renew` 命令随时手动更新证书。

命令使用CA（或者front-proxy-CA）和存储 `/etc/kubernetes/pki` 中的键执行更新。

>   **警告：** 如果你在运行一个高可用（HA）集群，这个命令需要在所有控制面板节点执行

>   **Note:** `alpha certs renew` 使用现有证书作为属性(公共名称（Common Name）、组织（Organization）、SAN等)的权威源，而不是kubeadm-config ConfigMap。强烈建议两者保持同步。

`kubeadm alpha certs renew` 提供了以下选项：

kubernetes证书通常在一年后过期。

-   `--csr-only` 可以通过生成证书签名请求(无需实际更新证书)来使用外部CA更新证书；更多信息见下一段。
-   也可以更新单个证书而不是所有证书

#### 使用Kubernetes certificates(证书) API来更新证书

这部分提供关于如何使用Kubernetes certificates API执行手动证书更新的更多细节。

>   **小心：** 这些是高级主题，为那些需要将组织的证书基础设施集成到kubead构建的集群中(integrate their organization’s certificate infrastructure into a kubeadm-built cluster)的用户设计。如果默认的kubeadm配置能满足你的需求，那就应该让kubeadm管理证书。

##### 设置一个签名器

kubernetes的证书颁发权在box之外不管用。你可以设置一个外部签名器，比如 [cert-manager](https://cert-manager.readthedocs.io/en/latest/tutorials/ca/creating-ca-issuer.html)， 或者使用内置签名器。

内置签名器是 [`kube-controller-manager`](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/) 的一部分。

要激活内置签名器的话，传入 `--cluster-signing-cert-file` 和 `--cluster-signing-key-file` 参数。

如果你在创建一个新集群，可以使用 kubeadm [配置文件](https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2)：

```yaml
  apiVersion: kubeadm.k8s.io/v1beta2
  kind: ClusterConfiguration
  controllerManager:
    extraArgs:
      cluster-signing-cert-file: /etc/kubernetes/pki/ca.crt
      cluster-signing-key-file: /etc/kubernetes/pki/ca.key
```

##### 创建证书签名请求（certificate signing requests (CSR)）

可以使用 `kubeadm alpha certs renew --use-api` 为kubernetes证书API创建证书签名请求。

如果设置了外部签名器，比如 [cert-manager][cert-manager]，证书签名请求(CSR)会自动被批准。否则，您必须使用 [`kubectl certificate`](https://kubernetes.io/docs/setup/best-practices/certificates/) 命令手动批准证书。下面的kubeadm命令输出要批准的证书的名称，然后阻塞并等待批准：

```shell
sudo kubeadm alpha certs renew apiserver --use-api &
```

输出大致是：

```console
[1] 2890
[certs] certificate request "kubeadm-cert-kube-apiserver-ld526" created
```

##### 批准证书签名请求(CSR)

如果设置了外部签名器，证书签名请求(CSR)会自动被批准。否则，您必须使用 [`kubectl certificate`](https://kubernetes.io/docs/setup/best-practices/certificates/) 命令手动批准证书。像这样：

```shell
kubectl certificate approve kubeadm-cert-kube-apiserver-ld526
```

输出大致是：

```shell
certificatesigningrequest.certificates.k8s.io/kubeadm-cert-kube-apiserver-ld526 approved
```

可以使用 `kubectl get csr` 查看挂起的证书的列表

##### 使用外部CA更新证书

本节提供关于如何使用外部CA执行手动证书更新的更多细节。

为了更好地与外部CAs集成，kubeadm也可以生成证书签名请求(CSR)。CSR表示向CA发送请求给客户端一个已签名的证书。在kubeadm术语中，通常由磁盘上的(on-desk)CA签名的任何证书都可以作为CSR。但CA不能作为CSR。

##### 创建证书签名请求(CSR)

可以使用 `--csr-dir` 传入一个目录来输出指定位置下的CSR。如果没有指定 `--csr-dir` ，就会使用默认证书目录(`/etc/kubernetes/pki`) 。会在输出中给出CSR和对应的私钥。当证书被签名之后，证书和私钥必须复制到PKI目录(默认为 `/etc/kubernetes/pki`)。

CSR表示向CA发送请求给客户端一个已签名的证书。

可以使用 `kubeadm alpha certs renew --csr-only` 来创建证书签名请求。

会在输出中给出CSR和对应的私钥；可以使用 `--csr-dir` 传入一个目录来输出指定位置下的CSR。

可以使用 `kubeadm alpha certs renew --csr-only` 来更新证书。与 `kubeadm init` 一样，可以使用 `--csr-dir` 指定输出目录。要使用新证书，就把证书和私钥复制到PKI目录(默认为 `/etc/kubernetes/pki`)。

CSR包含了一个证书名，域名(们)，和IP们，但没有指定用法。

在发行证书时指定[正确的证书用法](https://kubernetes.io/docs/setup/best-practices/certificates/#all-certificates) 是CA的职责。

-   在 `openssl` 中使用 [`openssl ca` 命令](https://superuser.com/questions/738612/openssl-ca-keyusage-extension) 来完成。
-   在 `cfssl` 中要 [在配置文件中指定用法](https://github.com/cloudflare/cfssl/blob/master/doc/cmd/cfssl.txt#L170)

使用你喜欢的方法签署证书之后，证书和私钥必须复制到PKI目录(默认是 `/etc/kubernetes/pki`)。





### 将kubeadm 高可用(HA)集群从v1.12升级到v1.13



### 将kubeadm集群从v1.12升级到v1.13



### 将kubeadm集群从v1.13升级到v1.14



### 将kubeadm集群从v1.14升级到v1.15





## 管理内存、CPU、API资源

### 为命名空间配置默认内存请求和限制

本文介绍怎样给命名空间配置默认的内存请求和限制。如果在一个有默认内存限制的命名空间创建容器，该容器没有声明自己的内存限制时，将会被指定默认内存限制。Kubernetes 还为某些情况指定了默认的内存请求，本章后面会进行介绍。

#### 准备工作

一个集群。

集群中的每个节点必须至少有2 GiB的内存。

#### 创建命名空间

创建一个命名空间，以便本练习中所建的资源与集群的其余资源相隔离。

```shell
kubectl create namespace default-mem-example
```

#### 创建 LimitRange 和 Pod

这里给出了一个限制范围对象的配置文件。该配置声明了一个默认的内存请求和一个默认的内存限制。

[`admin/resource/memory-defaults.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-defaults.yaml)

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
```

在 default-mem-example 命名空间创建限制范围：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace=default-mem-example
```

现在，如果在 default-mem-example 命名空间创建容器，并且该容器没有声明自己的内存请求和限制值，它将被指定一个默认的内存请求256 MiB和一个默认的内存限制512 Mib。

[`admin/resource/memory-defaults-pod.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-defaults-pod.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo
spec:
  containers:
  - name: default-mem-demo-ctr
    image: nginx
```

创建 Pod

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace=default-mem-example
```

查看 Pod 的详情：

```shell
kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example
```

输出内容显示该Pod的容器有一个256 MiB的内存请求和一个512 MiB的内存限制。这些都是限制范围声明的默认值。

```shell
containers:
- image: nginx
  imagePullPolicy: Always
  name: default-mem-demo-ctr
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 256Mi
```

删除你的 Pod：

```shell
kubectl delete pod default-mem-demo --namespace=default-mem-example
```

#### 声明容器的限制而不声明它的请求会怎么样？

这里给出了包含一个容器的 Pod 的配置文件。该容器声明了内存限制，而没有声明内存请求：

[`admin/resource/memory-defaults-pod-2.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-defaults-pod-2.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo-2
spec:
  containers:
  - name: default-mem-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "1Gi"
```

创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace=default-mem-example
```

查看 Pod 的详情：

```shell
kubectl get pod default-mem-demo-2 --output=yaml --namespace=default-mem-example
```

输出结果显示容器的内存请求被设置为它的内存限制相同的值。注意该容器没有被指定默认的内存请求值256Mi。

```
resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
```

#### 声明容器的内存请求而不声明内存限制会怎么样？

这里给出了一个包含一个容器的 Pod 的配置文件。该容器声明了内存请求，但没有内存限制：

[`admin/resource/memory-defaults-pod-3.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-defaults-pod-3.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo-3
spec:
  containers:
  - name: default-mem-demo-3-ctr
    image: nginx
    resources:
      requests:
        memory: "128Mi"
```

创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace=default-mem-example
```

查看 Pod 声明：

```shell
kubectl get pod default-mem-demo-3 --output=yaml --namespace=default-mem-example
```

输出结果显示该容器的内存请求被设置为了容器配置文件中声明的数值。容器的内存限制被设置为512Mi，即命名空间的默认内存限制。

```yaml
resources:
  limits:
    memory: 512Mi
  requests:
    memory: 128Mi
```

#### 设置默认内存限制和请求的动机

如果你的命名空间有资源配额，那么默认内存限制是很有帮助的。下面是一个例子，通过资源配额为命名空间设置两项约束：

-   运行在命名空间中的每个容器必须有自己的内存限制。
-   命名空间中所有容器的内存使用量之和不能超过声明的限制值。

如果一个容器没有声明自己的内存限制，会被指定默认限制，然后它才会被允许在限定了配额的命名空间中运行。

#### 接下来

##### 集群管理员参考

-   [为命名空间配置默认的 CPU 请求和限制](https://kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/)
-   [为命名空间配置最小和最大内存限制](https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)
-   [为命名空间配置最小和最大 CPU 限制](https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-   [为命名空间配置内存和 CPU 配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
-   [为命名空间配置 Pod 配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/)
-   [为 API 对象配置配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/)

##### 应用开发者参考

-   [为容器和 Pod 分配内存资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
-   [为容器和 Pod 分配 CPU 资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
-   [为 Pod 配置服务数量](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)



### 为命名空间配置默认CPU请求和限制

本章介绍怎样为命名空间配置默认的 CPU 请求和限制。 一个 Kubernetes 集群可被划分为多个命名空间。如果在配置了 CPU 限制的命名空间创建容器，并且该容器没有声明自己的 CPU 限制，那么这个容器会被指定默认的 CPU 限制。Kubernetes 在一些特定情况还会指定 CPU 请求，本文后续章节将会对其进行解释。

#### 准备工作

你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

#### 创建命名空间

创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。

```shell
kubectl create namespace default-cpu-example
```

创建 LimitRange 和 Pod

这里给出了 LimitRange 对象的配置文件。该配置声明了一个默认的 CPU 请求和一个默认的 CPU 限制。

[`admin/resource/cpu-defaults.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-defaults.yaml)

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
```

在命名空间 default-cpu-example 中创建 LimitRange 对象：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace=default-cpu-example
```

现在如果在 default-cpu-example 命名空间创建一个容器，该容器没有声明自己的 CPU 请求和限制时，将会给它指定默认的 CPU 请求0.5和默认的 CPU 限制值1.

这里给出了包含一个容器的 Pod 的配置文件。该容器没有声明 CPU 请求和限制。

[`admin/resource/cpu-defaults-pod.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-defaults-pod.yaml) 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx
```

创建 Pod。

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace=default-cpu-example
```

查看该 Pod 的声明：

```shell
kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example
```

输出显示该 Pod 的容器有一个500 millicpus的 CPU 请求和一个1 cpu的 CPU 限制。这些是 LimitRange 声明的默认值。

```shell
containers:
- image: nginx
  imagePullPolicy: Always
  name: default-cpu-demo-ctr
  resources:
    limits:
      cpu: "1"
    requests:
      cpu: 500m
```

#### 只声明容器的限制，而不声明请求会怎么样？

这是包含一个容器的 Pod 的配置文件。该容器声明了 CPU 限制，而没有声明 CPU 请求。

[`admin/resource/cpu-defaults-pod-2.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-defaults-pod-2.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-2
spec:
  containers:
  - name: default-cpu-demo-2-ctr
    image: nginx
    resources:
      limits:
        cpu: "1"
```

创建 Pod

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace=default-cpu-example
```

查看 Pod 的声明：

```
kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example
```

输出显示该容器的 CPU 请求和 CPU 限制设置相同。注意该容器没有被指定默认的 CPU 请求值0.5 cpu。

```
resources:
  limits:
    cpu: "1"
  requests:
    cpu: "1"
```

#### 只声明容器的请求，而不声明它的限制会怎么样？

这里给出了包含一个容器的 Pod 的配置文件。该容器声明了 CPU 请求，而没有声明 CPU 限制。

[`admin/resource/cpu-defaults-pod-3.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-defaults-pod-3.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-3
spec:
  containers:
  - name: default-cpu-demo-3-ctr
    image: nginx
    resources:
      requests:
        cpu: "0.75"
```

创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace=default-cpu-example
```

查看 Pod 的声明：

```
kubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example
```

结果显示该容器的 CPU 请求被设置为容器配置文件中声明的数值。容器的CPU限制被设置为1 cpu，即该命名空间的默认 CPU 限制值。

```
resources:
  limits:
    cpu: "1"
  requests:
    cpu: 750m
```

#### 默认 CPU 限制和请求的动机

如果你的命名空间有一个[资源配额](https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/),那么有一个默认的 CPU 限制是有帮助的。这里有两条资源配额强加给命名空间的限制：

-   命名空间中运行的每个容器必须有自己的 CPU 限制。
-   命名空间中所有容器使用的 CPU 总和不能超过一个声明值。

如果容器没有声明自己的 CPU 限制，将会给它一个默认限制，这样它就能被允许运行在一个有配额限制的命名空间中。

#### 接下来

##### 集群管理员参考

-   [为命名空间配置默认内存请求和限制](https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)
-   [为命名空间配置内存限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)
-   [为命名空间配置CPU限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-   [为命名空间配置内存和CPU配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
-   [为命名空间配置Pod配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/)
-   [为API对象配置配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/)

##### 应用开发者参考

-   [为容器和Pod分配内存资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
-   [为容器和Pod分配CPU资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
-   [为Pod配置Service数量](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)





### 为命名空间配置最小和最大内存约束

此页面介绍如何设置在命名空间中运行的容器使用的内存的最小值和最大值。 您可以在 [LimitRange](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#limitrange-v1-core)对象中指定最小和最大内存值。 如果 Pod 不满足 LimitRange 施加的约束，则无法在命名空间中创建它。

#### 准备工作

一个集群。

集群中每个节点必须至少要有1 GiB 的内存。

#### 创建命名空间

创建一个命名空间，以便在此练习中创建的资源与群集的其余资源隔离。

```shell
kubectl create namespace constraints-mem-example
```

#### 创建 LimitRange 和 Pod

下面是 LimitRange 的配置文件：

[`admin/resource/memory-constraints.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-constraints.yaml)

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-min-max-demo-lr
spec:
  limits:
  - max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
```

创建 LimitRange:

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace=constraints-mem-example
```

查看 LimitRange 的详情：

```shell
kubectl get limitrange mem-min-max-demo-lr --namespace=constraints-mem-example --output=yaml
```

输出显示预期的最小和最大内存约束。 但请注意，即使您没有在 LimitRange 的配置文件中指定默认值，也会自动创建它们。

```
  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
```

现在，只要在 constraints-mem-example 命名空间中创建容器，Kubernetes 就会执行下面的步骤：

-   如果 Container 未指定自己的内存请求和限制，将为它指定默认的内存请求和限制。
-   验证 Container 的内存请求是否大于或等于500 MiB。
-   验证 Container 的内存限制是否小于或等于1 GiB。

这里给出了包含一个 Container 的 Pod 配置文件。Container 声明了600 MiB 的内存请求和800 MiB 的内存限制， 这些满足了 LimitRange 施加的最小和最大内存约束。

[`admin/resource/memory-constraints-pod.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-constraints-pod.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo
spec:
  containers:
  - name: constraints-mem-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "800Mi"
      requests:
        memory: "600Mi"
```

创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace=constraints-mem-example
```

确认下 Pod 中的容器在运行：

```shell
kubectl get pod constraints-mem-demo --namespace=constraints-mem-example
```

查看 Pod 详情：

```shell
kubectl get pod constraints-mem-demo --output=yaml --namespace=constraints-mem-example
```

输出结果显示容器的内存请求为600 MiB，内存限制为800 MiB。这些满足了 LimitRange 设定的限制范围。

```yaml
resources:
  limits:
     memory: 800Mi
  requests:
    memory: 600Mi
```

删除你创建的 Pod：

```shell
kubectl delete pod constraints-mem-demo --namespace=constraints-mem-example
```

#### 尝试创建一个超过最大内存限制的 Pod

这里给出了包含一个容器的 Pod 的配置文件。容器声明了800 MiB 的内存请求和1.5 GiB 的内存限制。

[`admin/resource/memory-constraints-pod-2.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-constraints-pod-2.yaml) 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo-2
spec:
  containers:
  - name: constraints-mem-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "1.5Gi"
      requests:
        memory: "800Mi"
```

尝试创建 Pod:

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace=constraints-mem-example
```

输出结果显示 Pod 没有创建成功，因为容器声明的内存限制太大了：

```
Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-2.yaml":
pods "constraints-mem-demo-2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi.
```

#### 尝试创建一个不满足最小内存请求的 Pod

这里给出了包含一个容器的 Pod 的配置文件。容器声明了100 MiB 的内存请求和800 MiB 的内存限制。

[`admin/resource/memory-constraints-pod-3.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-constraints-pod-3.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo-3
spec:
  containers:
  - name: constraints-mem-demo-3-ctr
    image: nginx
    resources:
      limits:
        memory: "800Mi"
      requests:
        memory: "100Mi"
```

尝试创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace=constraints-mem-example
```

输出结果显示 Pod 没有创建成功，因为容器声明的内存请求太小了：

```
Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-3.yaml":
pods "constraints-mem-demo-3" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi.
```

#### 创建一个没有声明内存请求和限制的 Pod

这里给出了包含一个容器的 Pod 的配置文件。容器没有声明内存请求，也没有声明内存限制。

[`admin/resource/memory-constraints-pod-4.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/memory-constraints-pod-4.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-mem-demo-4
spec:
  containers:
  - name: constraints-mem-demo-4-ctr
    image: nginx
```

创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace=constraints-mem-example
```

查看 Pod 详情：

```
kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml
```

输出结果显示 Pod 的内存请求为1 GiB，内存限制为1 GiB。容器怎样获得哪些数值呢？

```
resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
```

因为你的容器没有声明自己的内存请求和限制，它从 LimitRange 那里获得了[默认的内存请求和限制](https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)。

此时，你的容器可能运行起来也可能没有运行起来。回想一下我们本次任务的先决条件是你的每个节点都至少有1 GiB 的内存。如果你的每个节点都只有1 GiB 的内存，那将没有一个节点拥有足够的可分配内存来满足1 GiB 的内存请求。

删除你的 Pod：

```
kubectl delete pod constraints-mem-demo-4 --namespace=constraints-mem-example
```

#### 强制执行内存最小和最大限制

LimitRange 为命名空间设定的最小和最大内存限制只有在 Pod 创建和更新时才会强制执行。如果你更新 LimitRange，它不会影响此前创建的 Pod。

#### 设置内存最小和最大限制的动因

做为集群管理员，你可能想规定 Pod 可以使用的内存总量限制。例如：

-   集群的每个节点有2 GB 内存。你不想接受任何请求超过2 GB 的 Pod，因为集群中没有节点可以满足。

#### 数据清理

删除你的命名空间：

```shell
kubectl delete namespace constraints-mem-example
```

#### 接下来

##### 集群管理员参考

-   [为命名空间配置默认内存请求和限制](https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)
-   [为命名空间配置内存限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)
-   [为命名空间配置 CPU 限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-   [为命名空间配置内存和 CPU 配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
-   [为命名空间配置 Pod 配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/)
-   [为 API 对象配置配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/)

##### 应用开发者参考

-   [为容器和 Pod 分配内存资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
-   [为容器和 Pod 分配 CPU 资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
-   [为 Pod 配置 Service 数量](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)









### 为命名空间配置最小和最大CPU约束

本章介绍命名空间中可以被容器和Pod使用的CPU资源的最小和最大值。你可以通过 [LimitRange](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#limitrange-v1-core) 对象声明 CPU 的最小和最大值. 如果 Pod 不能满足 LimitRange 的限制，它就不能在命名空间中创建。

#### 准备工作

一个集群

你的集群中每个节点至少要有1个CPU。

#### 创建命名空间

创建一个命名空间，以便本练习中创建的资源和集群的其余资源相隔离。

```shell
kubectl create namespace constraints-cpu-example
```

#### 创建 LimitRange 和 Pod

这里给出了 LimitRange 的配置文件：

[`admin/resource/cpu-constraints.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-constraints.yaml)

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
spec:
  limits:
  - max:
      cpu: "800m"
    min:
      cpu: "200m"
    type: Container
```

创建 LimitRange:

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace=constraints-cpu-example
```

查看 LimitRange 详情：

```shell
kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example
```

输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。

```yaml
limits:
- default:
    cpu: 800m
  defaultRequest:
    cpu: 800m
  max:
    cpu: 800m
  min:
    cpu: 200m
  type: Container
```

现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：

-   如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。
-   核查容器声明的 CPU 请求确保其大于或者等于200 millicpu。
-   核查容器声明的 CPU 限制确保其小于或者等于800 millicpu。



>   **Note:** 当创建 LimitRange 对象时，你也可以声明 huge-page 和 GPU 的限制。当这些资源同时声明了 ‘default’ 和 ‘defaultRequest’ 参数时，两个参数值必须相同。



这里给出了包含一个容器的 Pod 的配置文件。该容器声明了500 millicpu的 CPU 请求和800 millicpu的 CPU 限制。这些参数满足了 LimitRange 对象规定的 CPU 最小和最大限制。

[`admin/resource/cpu-constraints-pod.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-constraints-pod.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo
spec:
  containers:
  - name: constraints-cpu-demo-ctr
    image: nginx
    resources:
      limits:
        cpu: "800m"
      requests:
        cpu: "500m"
```

创建Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace=constraints-cpu-example
```

确认一下 Pod 中的容器在运行：

```shell
kubectl get pod constraints-cpu-demo --namespace=constraints-cpu-example
```

查看 Pod 的详情：

```shell
kubectl get pod constraints-cpu-demo --output=yaml --namespace=constraints-cpu-example
```

输出结果表明容器的 CPU 请求为500 millicpu，CPU限制为800 millicpu。这些参数满足 LimitRange 规定的限制范围。

```yaml
resources:
  limits:
    cpu: 800m
  requests:
    cpu: 500m
```

#### 删除 Pod

```shell
kubectl delete pod constraints-cpu-demo --namespace=constraints-cpu-example
```

#### 尝试创建一个超过最大 CPU 限制的 Pod

这里给出了包含一个容器的 Pod 的配置文件。容器声明了500 millicpu的CPU请求和1.5 cpu的 CPU 限制。

[`admin/resource/cpu-constraints-pod-2.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-constraints-pod-2.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo-2
spec:
  containers:
  - name: constraints-cpu-demo-2-ctr
    image: nginx
    resources:
      limits:
        cpu: "1.5"
      requests:
        cpu: "500m"
```

尝试创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace=constraints-cpu-example
```

输出结果表明 Pod 没有创建成功，因为容器声明的 CPU 限制太大了：

```
Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-2.yaml":
pods "constraints-cpu-demo-2" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m.
```

#### 尝试创建一个不满足最小 CPU 请求的 Pod

这里给出了包含一个容器的 Pod 的配置文件。该容器声明了100 millicpu的 CPU 请求和800 millicpu的 CPU 限制。

[`admin/resource/cpu-constraints-pod-3.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-constraints-pod-3.yaml) 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo-4
spec:
  containers:
  - name: constraints-cpu-demo-4-ctr
    image: nginx
    resources:
      limits:
        cpu: "800m"
      requests:
        cpu: "100m"
```

尝试创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace=constraints-cpu-example
```

输出结果显示 Pod 没有创建成功，因为容器声明的 CPU 请求太小了：

```
Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-3.yaml":
pods "constraints-cpu-demo-4" is forbidden: minimum cpu usage per Container is 200m, but request is 100m.
```

#### 创建一个没有声明CPU请求和CPU限制的Pod

这里给出了包含一个容器的Pod的配置文件。该容器没有声明CPU请求和CPU限制。

[`admin/resource/cpu-constraints-pod-4.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/cpu-constraints-pod-4.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo-4
spec:
  containers:
  - name: constraints-cpu-demo-4-ctr
    image: vish/stress
```

创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace=constraints-cpu-example
```

查看 Pod 的详情：

```
kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml
```

输出结果显示 Pod 的容器有个800 millicpu的 CPU 请求和800 millicpu的 CPU 限制。容器时怎样得到那些值的呢？

```yaml
resources:
  limits:
    cpu: 800m
  requests:
    cpu: 800m
```

因为你的容器没有声明自己的 CPU 请求和限制，LimitRange 给它指定了[默认的CPU请求和限制](https://kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/)

此时，你的容器可能运行也可能没有运行。回想一下，本任务的先决条件是你的节点要有1 个 CPU。如果你的每个节点仅有1个 CPU，那么可能没有任何一个节点可以满足800 millicpu的 CPU 请求。如果你在用的节点恰好有两个 CPU，那么你才可能有足够的 CPU来满足800 millicpu的请求。

```
kubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example
```

#### CPU 最小和最大限制的强制执行

只有当Pod创建或者更新时，LimitRange为命名空间规定的CPU最小和最大限制才会被强制执行。如果你对LimitRange进行修改，那不会影响此前创建的Pod。

#### 最小和最大 CPU 限制范围的动机

作为集群管理员，你可能想设定 Pod 可以使用的 CPU 资源限制。例如：

-   集群中的每个节点有两个 CPU。你不想接受任何请求超过2个 CPU 的 Pod，因为集群中没有节点可以支持这种请求。
-   你的生产和开发部门共享一个集群。你想允许生产工作负载消耗3个 CPU，而开发工作负载的消耗限制为1个 CPU。你为生产和开发创建不同的命名空间，并且你为每个命名空间都应用了 CPU 限制。

#### 环境清理

删除你的命名空间：

```shell
kubectl delete namespace constraints-cpu-example
```

#### 接下来

##### 集群管理员参考：

-   [为命名空间配置默认内存请求和限制](https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)
-   [为命名空间配置内存限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)
-   [为命名空间配置CPU限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-   [为命名空间配置内存和CPU配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
-   [为命名空间配置Pod配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/)
-   [为API对象配置配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/)

##### 应用开发者参考：

-   [为容器和Pod分配内存资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
-   [为容器和Pod分配CPU资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
-   [为Pod配置Service数量](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)





### 为命名空间配置内存和CPU配额

本文介绍怎样为命名空间设置容器可用的内存和 CPU 总量。你可以通过 [ResourceQuota](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#resourcequota-v1-core) 对象设置配额.

#### 准备工作

一个集群。

集群中每个节点至少有1 GiB的内存。

#### 创建命名空间

创建一个命名空间，以便本练习中创建的资源和集群的其余部分相隔离。

```shell
kubectl create namespace quota-mem-cpu-example
```

#### 创建资源配额(ResourceQuota)

这里给出一个 ResourceQuota 对象的配置文件：

[`admin/resource/quota-mem-cpu.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-mem-cpu.yaml)

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
```

创建 ResourceQuota

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace=quota-mem-cpu-example
```

查看 ResourceQuota 详情：

```shell
kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml
```

ResourceQuota 在 quota-mem-cpu-example 命名空间中设置了如下要求：

-   每个容器必须有内存请求和限制，以及 CPU 请求和限制。
-   所有容器的内存请求总和不能超过1 GiB。
-   所有容器的内存限制总和不能超过2 GiB。
-   所有容器的 CPU 请求总和不能超过1 cpu。
-   所有容器的 CPU 限制总和不能超过2 cpu。

#### 创建 Pod

这里给出 Pod 的配置文件：

[`admin/resource/quota-mem-cpu-pod.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-mem-cpu-pod.yaml) 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo
spec:
  containers:
  - name: quota-mem-cpu-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "800Mi"
        cpu: "800m" 
      requests:
        memory: "600Mi"
        cpu: "400m"
```

创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace=quota-mem-cpu-example
```

检查下 Pod 中的容器在运行：

```
kubectl get pod quota-mem-cpu-demo --namespace=quota-mem-cpu-example
```

再查看 ResourceQuota 的详情：

```
kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml
```

输出结果显示了配额以及有多少配额已经被使用。你可以看到 Pod 的内存和 CPU 请求值及限制值没有超过配额。

```
status:
  hard:
    limits.cpu: "2"
    limits.memory: 2Gi
    requests.cpu: "1"
    requests.memory: 1Gi
  used:
    limits.cpu: 800m
    limits.memory: 800Mi
    requests.cpu: 400m
    requests.memory: 600Mi
```

#### 尝试创建第二个 Pod

这里给出了第二个 Pod 的配置文件：

[`admin/resource/quota-mem-cpu-pod-2.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-mem-cpu-pod-2.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: quota-mem-cpu-demo-2
spec:
  containers:
  - name: quota-mem-cpu-demo-2-ctr
    image: redis
    resources:
      limits:
        memory: "1Gi"
        cpu: "800m"      
      requests:
        memory: "700Mi"
        cpu: "400m"
```

配置文件中，你可以看到 Pod 的内存请求为700 MiB。请注意新的内存请求与已经使用的内存请求只和超过了内存请求的配额。600 MiB + 700 MiB > 1 GiB。

尝试创建 Pod：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace=quota-mem-cpu-example
```

第二个 Pod 不能被创建成功。输出结果显示创建第二个 Pod 会导致内存请求总量超过内存请求配额。

```
Error from server (Forbidden): error when creating "examples/admin/resource/quota-mem-cpu-pod-2.yaml":
pods "quota-mem-cpu-demo-2" is forbidden: exceeded quota: mem-cpu-demo,
requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi
```

#### 讨论

如你在本练习中所见，你可以用 ResourceQuota 限制命名空间中所有容器的内存请求总量。同样你也可以限制内存限制总量、CPU 请求总量、CPU 限制总量。

如果你想对单个容器而不是所有容器进行限制，就请使用 [LimitRange](https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)。

#### 数据清理

删除你的命名空间：

```shell
kubectl delete namespace quota-mem-cpu-example
```

#### 接下来

##### 集群管理员参考

-   [为命名空间配置默认内存请求和限制](https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)
-   [为命名空间配置内存限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)
-   [为命名空间配置 CPU 限制的最小值和最大值](https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-   [为命名空间配置内存和 CPU 配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
-   [为命名空间配置 Pod 配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/)
-   [为 API 对象配置配额](https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/)

应用开发者参考

-   [为容器和 Pod 分配内存资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
-   [为容器和 Pod 分配CPU资源](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
-   [为 Pod 配置 Service 数量](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)





### 为命名空间配置Pod配额

本文介绍怎样给命名空间配置可以运行的 Pod 总数配额。你在 [ResourceQuota](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/#resourcequota-v1-core)对象中可以进行声明。

#### 准备工作

一个集群。

#### 创建命名空间

创建一个命名空间，以便本练习所创建的资源和集群的其余资源相隔离。

```shell
kubectl create namespace quota-pod-example
```

#### 创建一个 ResourceQuota

这里给出了一个 ResourceQuota 对象的配置文件：

[`admin/resource/quota-pod.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-pod.yaml)

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-demo
spec:
  hard:
    pods: "2"
```

创建 ResourceQuota

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace=quota-pod-example
```

查看 ResourceQuota 详情：

```shell
kubectl get resourcequota pod-demo --namespace=quota-pod-example --output=yaml
```

输出结果显示该命名空间有两个 Pod 的配额，并且当前没有 Pod；也就是配额没有被使用。

```yaml
spec:
  hard:
    pods: "2"
status:
  hard:
    pods: "2"
  used:
    pods: "0"
```

这里给出了一个 Deployment 的配置文件：

[`admin/resource/quota-pod-deployment.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-pod-deployment.yaml)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-quota-demo
spec:
  selector:
    matchLabels:
      purpose: quota-demo
  replicas: 3
  template:
    metadata:
      labels:
        purpose: quota-demo
    spec:
      containers:
      - name: pod-quota-demo
        image: nginx
```

配置文件中，`replicas: 3` 使 Kubernetes 尝试创建3个 Pod，都运行相同的应用。

创建 Deployment：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace=quota-pod-example
```

查看 Deployment 详情：

```shell
kubectl get deployment pod-quota-demo --namespace=quota-pod-example --output=yaml
```

输出结果显示尽管 Deployment 声明了三个副本，但由于配额的限制只创建了两个 Pod。

```yaml
spec:
  ...
  replicas: 3
...
status:
  availableReplicas: 2
...
lastUpdateTime: 2017-07-07T20:57:05Z
    message: 'unable to create pods: pods "pod-quota-demo-1650323038-" is forbidden:
      exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited: pods=2'
```

#### 清理环境

删除你的命名空间：

```shell
kubectl delete namespace quota-pod-example
```

#### 接下来

##### 集群管理员参考

-   [为命名空间配置默认内存请求和限制](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)
-   [为命名空间配置内存限制的最小值和最大值](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)
-   [为命名空间配置 CPU 限制的最小值和最大值](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-   [为命名空间配置内存和 CPU 配额](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
-   [为命名空间配置 Pod 配额](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/)
-   [为 API 对象配置配额](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/quota-api-object/)

##### 应用开发者参考

-   [为容器和 Pod 分配内存资源](https://v1-14.docs.kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
-   [为容器和 Pod 分配 CPU 资源](https://v1-14.docs.kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
-   [为 Pod 配置 Service 数量](https://v1-14.docs.kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)





## 安装网络策略提供者(Network Policy Provider)

### 使用Calico提供的网络策略

本页展示了两种在 Kubernetes 上快速创建 Calico 集群的方法。

#### 准备工作

决定您想部署一个[云](https://kubernetes.io/zh/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/#%E5%9C%A8-Google-Kubernetes-Engine-GKE-%E4%B8%8A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA-Calico-%E9%9B%86%E7%BE%A4) 还是 [本地](https://kubernetes.io/zh/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/#%E4%BD%BF%E7%94%A8-kubeadm-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%9C%AC%E5%9C%B0-Calico-%E9%9B%86%E7%BE%A4) 集群。

#### 在 Google Kubernetes Engine (GKE) 上创建一个 Calico 集群

**先决条件**: [gcloud](https://cloud.google.com/sdk/docs/quickstarts)

1.  启动一个带有 Calico 的 GKE 集群，只需加上flag `--enable-network-policy`。

    **语法**

    ```shell
    gcloud container clusters create [CLUSTER_NAME] --enable-network-policy
    ```

    **示例**

    ```shell
    gcloud container clusters create my-calico-cluster --enable-network-policy
    ```

2.  使用如下命令验证部署是否正确。

    ```shell
    kubectl get pods --namespace=kube-system
    ```

    Calico 的 pods 名以 `calico` 打头，检查确认每个 pods 状态为 `Running`。


#### 使用 kubeadm 创建一个本地 Calico 集群

在15分钟内使用 kubeadm 得到一个本地单主机 Calico 集群，请参考 [Calico 快速入门](https://docs.projectcalico.org/latest/getting-started/kubernetes/)。

#### 接下来

集群运行后，您可以按照 [声明 Network Policy](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/) 去尝试使用 Kubernetes NetworkPolicy。





### 使用Cilium提供的网络策略

本页展示了如何使用 Cilium 作为 NetworkPolicy。

关于 Cilium 的背景知识，请阅读 [Cilium 介绍](https://cilium.readthedocs.io/en/latest/intro)。

#### 准备工作

一个集群。

#### 在 Minikube 上部署 Cilium 用于基本测试

为了轻松熟悉 Cilium 您可以根据[Cilium Kubernetes 入门指南](https://docs.cilium.io/en/latest/gettingstarted/minikube/)在 minikube 中执行一个 cilium 的基本的 DaemonSet 安装。

在 minikube 中的安装配置使用一个简单的“一体化” YAML 文件，包括了 Cilium 的 DaemonSet 配置，连接 minikube 的 etcd 实例，以及适当的 RBAC 设置。

```shell
$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/master/examples/kubernetes/cilium.yaml
configmap "cilium-config" created
secret "cilium-etcd-secrets" created
serviceaccount "cilium" created
clusterrolebinding "cilium" created
daemonset "cilium" created
clusterrole "cilium" created
```

入门指南其余的部分用一个示例应用说明了如何强制执行L3/L4（即 IP 地址+端口）的安全策略以及L7 （如 HTTP）的安全策略。

#### 部署 Cilium 用于生产用途

关于部署 Cilium 用于生产的详细说明，请见[Cilium Kubernetes 安装指南](https://cilium.readthedocs.io/en/latest/kubernetes/install/) ，此文档包括详细的需求、说明和生产用途 DaemonSet 文件示例。

#### 了解 Cilium 组件

部署使用 Cilium 的集群会添加 Pods 到`kube-system`命名空间。 要查看此Pod列表，运行：

```shell
kubectl get pods --namespace=kube-system
```

您将看到像这样的 Pods 列表：

```console
NAME            DESIRED   CURRENT   READY     NODE-SELECTOR   AGE
cilium          1         1         1         <none>          2m
...
```

有两个主要组件需要注意：

-   在集群中的每个节点上都会运行一个 `cilium` Pod，并利用Linux BPF执行网络策略管理该节点上进出 Pod 的流量。
-   对于生产部署，Cilium 应该复用 Kubernetes 所使用的键值存储集群（如 etcd），其通常在Kubernetes 的 master 节点上运行。 [Cilium Kubernetes安装指南](https://cilium.readthedocs.io/en/latest/kubernetes/install/) 包括了一个示例 DaemonSet，可以自定义指定此键值存储集群。 简单的 minikube 的“一体化” DaemonSet 不需要这样的配置，因为它会自动连接到 minikube 的 etcd 实例。

#### 接下来

群集运行后，您可以按照[声明网络策略](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/) 用 Cilium 试用 Kubernetes NetworkPolicy。 玩得开心，如果您有任何疑问，请联系 [Cilium Slack Channel](https://cilium.herokuapp.com/)。







### 使用kube-router提供的网络策略

本页展示了如何使用 [Kube-router](https://github.com/cloudnativelabs/kube-router) 作为 网络策略。

#### 准备工作

您需要拥有一个正在运行的 Kubernetes 集群。如果您还没有集群，可以使用任意的集群安装器如 Kops，Bootkube，Kubeadm 等创建一个。

#### 安装 Kube-router 插件

Kube-router 插件自带一个Network Policy 控制器，监视来自于Kubernetes API server 的 NetworkPolicy 和 pods 的变化，根据策略指示配置 iptables 规则和 ipsets 来允许或阻止流量。请根据 [尝试通过集群安装器使用 Kube-router](https://www.kube-router.io/docs/user-guide/#try-kube-router-with-cluster-installers) 指南安装 Kube-router 插件。

#### 接下来

在您安装 Kube-router 插件后，可以根据 [声明 Network Policy](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/) 去尝试使用 Kubernetes NetworkPolicy。



### 使用Romana提供的网络策略

本页展示如何使用 Romana 作为 NetworkPolicy。

#### 准备工作

完成[kubeadm 入门指南](https://kubernetes.io/docs/getting-started-guides/kubeadm/)中的1、2、3步。

#### 使用 kubeadm 安装 Romana

按照[容器化安装指南](https://github.com/romana/romana/tree/master/containerize)获取 kubeadm。

运用网络策略

使用以下的一种方式去运用网络策略：

-   Romana 网络策略
    -   [Romana 网络策略例子](https://github.com/romana/core/blob/master/doc/policy.md)
-   网络策略API

#### 接下来

Romana 安装完成后，您可以按照[声明 Network Policy](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/)去尝试使用 Kubernetes NetworkPolicy。





### 使用Weave Net提供的网络策略

本页展示了如何使用使用 Weave Net 作为 NetworkPolicy。

#### 准备工作

您需要拥有一个 Kubernetes 集群。按照[kubeadm 入门指南](https://kubernetes.io/docs/getting-started-guides/kubeadm/)来引导一个。

#### 安装 Weave Net 插件

按照[通过插件集成Kubernetes](https://www.weave.works/docs/net/latest/kube-addon/)指南。

Kubernetes 的 Weave Net 插件带有[网络策略控制器](https://www.weave.works/docs/net/latest/kube-addon/#npc)，可自动监控 Kubernetes 所有命名空间中的任何 NetworkPolicy 注释。 配置`iptables`规则以允许或阻止策略指示的流量。

#### 测试安装

验证 weave 是否有效。

输入以下命令：

```shell
kubectl get po -n kube-system -o wide
```

输出类似这样：

```
NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
weave-net-1t1qg                         2/2       Running   0          9d        192.168.2.10    worknode3
weave-net-231d7                         2/2       Running   1          7d        10.2.0.17       worknodegpu
weave-net-7nmwt                         2/2       Running   3          9d        192.168.2.131   masternode
weave-net-pmw8w                         2/2       Running   0          9d        192.168.2.216   worknode2
```

每个 Node 都有一个 weave Pod，所有 Pod 都是`Running`和`2/2 READY`。（`2/2`表示每个Pod都有`weave`和`weave-npc`。）

#### 接下来

安装Weave Net插件后，您可以按照[声明网络策略](https://kubernetes.io/docs/tasks/administration-cluster/declare-network-policy/)来试用 Kubernetes NetworkPolicy。 如果您有任何疑问，请联系我们[#weave-community on Slack 或 Weave User Group](https://github.com/weaveworks/weave#getting-help)。







## 使用kubernetes API来访问集群

本页展示了如何使用 Kubernetes API 访问集群

### 准备工作

一个集群。

### 访问集群 API

#### 使用 kubectl 进行首次访问

首次访问 Kubernetes API 时，请使用 Kubernetes 命令行工具 `kubectl` 。

要访问集群，您需要知道集群位置并拥有访问它的凭证。通常，当您完成[入门指南](https://v1-14.docs.kubernetes.io/docs/setup/)时，这会自动设置完成，或者由其他人设置好集群并将凭证和位置提供给您。

使用此命令检查 kubectl 已知的位置和凭证：

```shell
kubectl config view
```

许多[样例](https://github.com/kubernetes/examples/tree/v1.14.3/)提供了使用 kubectl 的介绍。完整文档请见 [kubectl 手册](https://v1-14.docs.kubernetes.io/docs/reference/kubectl/overview/)。

#### 直接访问 REST API

kubectl 处理对 API 服务器的定位和身份验证。如果您想通过 http 客户端（如 `curl` 或 `wget`，或浏览器）直接访问 REST API，您可以通过多种方式对 API 服务器进行定位和身份验证：



1.  以代理模式运行 kubectl（推荐）。 推荐使用此方法，因为它用存储的 apiserver 位置并使用自签名证书验证 API 服务器的标识。使用这种方法无法进行中间人（MITM）攻击。
2.  另外，您可以直接为 http 客户端提供位置和身份认证。这适用于被代理混淆的客户端代码。为防止中间人攻击，您需要将根证书导入浏览器。



使用 Go 或 Python 客户端库可以在代理模式下访问 kubectl。

##### 使用 kubectl 代理

下列命令使 kubectl 运行在反向代理模式下。它处理 API 服务器的定位和身份认证。

像这样运行它：

```shell
kubectl proxy --port=8080 &
```

参见 [kubectl 代理](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#proxy) 获取更多细节。

然后您可以通过 curl，wget，或浏览器浏览 API，像这样：

```shell
curl http://localhost:8080/api/
```

输出类似如下：

```json
{
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "10.0.1.149:443"
    }
  ]
}
```

##### 不使用 kubectl 代理

通过将身份认证令牌直接传给 API 服务器，可以避免使用 kubectl 代理，像这样：

使用 `grep/cut` 方式：

```shell
# Check all possible clusters, as you .KUBECONFIG may have multiple contexts:
kubectl config view -o jsonpath='{"Cluster name\tServer\n"}{range .clusters[*]}{.name}{"\t"}{.cluster.server}{"\n"}{end}'

# Select name of cluster you want to interact with from above output:
export CLUSTER_NAME="some_server_name"

# Point to the API server refering the cluster name
APISERVER=$(kubectl config view -o jsonpath="{.clusters[?(@.name==\"$CLUSTER_NAME\")].cluster.server}")

# Gets the token value
TOKEN=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 -d)

# Explore the API with TOKEN
curl -X GET $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure
```

输出类似如下：

```json
{
  "kind": "APIVersions",
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "10.0.1.149:443"
    }
  ]
}
```

使用 `jsonpath` 方式：

```shell
$ APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
$ TOKEN=$(kubectl get secret $(kubectl get serviceaccount default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 --decode )
$ curl $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure
{
  "kind": "APIVersions",
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "10.0.1.149:443"
    }
  ]
}
```

上面例子使用了 `--insecure` 标志位。这使它易受到 MITM 攻击。当 kubectl 访问集群时，它使用存储的根证书和客户端证书访问服务器。（已安装在 `~/.kube` 目录下）。由于集群认证通常是自签名的，因此可能需要特殊设置才能让你的 http 客户端使用根证书。

在一些集群中，API 服务器不需要身份认证；它运行在本地，或由防火墙保护着。对此并没有一个标准。[配置对 API 的访问](https://v1-14.docs.kubernetes.io/docs/reference/access-authn-authz/controlling-access/) 阐述了一个集群管理员如何对此进行配置。这种方法可能与未来的高可用性支持发生冲突。

#### 编程方式访问 API

Kubernetes 官方支持 [Go](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#go-%E5%AE%A2%E6%88%B7%E7%AB%AF) 和 [Python](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#python-%E5%AE%A2%E6%88%B7%E7%AB%AF) 的客户端库.

##### Go 客户端

-   要获取库，运行下列命令：`go get k8s.io/client-go/<version number>/kubernetes`参见 <https://github.com/kubernetes/client-go> 查看受支持的版本。
-   基于 client-go 客户端编写应用程序。注意 client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是主仓库导入 API 定义，例如 `import "k8s.io/client-go/1.4/pkg/api/v1"` 是正确做法。

Go 客户端可以使用与 kubectl 命令行工具相同的 [kubeconfig 文件](https://v1-14.docs.kubernetes.io/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/) 定位和验证 API 服务器。参见这个 [例子](https://git.k8s.io/client-go/examples/out-of-cluster-client-configuration/main.go)：

```go
import (
   "fmt"
   "k8s.io/client-go/1.4/kubernetes"
   "k8s.io/client-go/1.4/pkg/api/v1"
   "k8s.io/client-go/1.4/tools/clientcmd"
)
...
   // uses the current context in kubeconfig
   config, _ := clientcmd.BuildConfigFromFlags("", "path to kubeconfig")
   // creates the clientset
   clientset, _:= kubernetes.NewForConfig(config)
   // access the API to list pods
   pods, _:= clientset.CoreV1().Pods("").List(v1.ListOptions{})
   fmt.Printf("There are %d pods in the cluster\n", len(pods.Items))
...
```

如果该应用程序部署为集群中的一个 Pod，请参阅 [下一节](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-api/#%E4%BB%8E-pod-%E4%B8%AD%E8%AE%BF%E9%97%AE-api)。

##### Python 客户端

要使用 [Python 客户端](https://github.com/kubernetes-client/python)，运行下列命令：`pip install kubernetes` 参见 [Python 客户端库主页](https://github.com/kubernetes-client/python) 查看更多安装选项。

Python 客户端可以使用与 kubectl 命令行工具相同的 [kubeconfig 文件](https://v1-14.docs.kubernetes.io/docs/concepts/cluster-administration/authenticate-across-clusters-kubeconfig/) 定位和验证 API 服务器。参见这个 [例子](https://github.com/kubernetes-client/python/tree/master/examples/example1.py)：

```python
from kubernetes import client, config

config.load_kube_config()

v1=client.CoreV1Api()
print("Listing pods with their IPs:")
ret = v1.list_pod_for_all_namespaces(watch=False)
for i in ret.items:
    print("%s\t%s\t%s" % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))
```

##### 其他语言

有许多 [客户端库](https://v1-14.docs.kubernetes.io/docs/reference/using-api/client-libraries/) 可以用于从其他语言访问 API。请参阅其他库的文档了解它们的身份验证方式。

#### 从 Pod 中访问 API

从 Pod 访问 API 时，对 API 服务器的定位和身份验证会有所不同。

从 Pod 使用 Kubernetes API 的最简单的方法就是使用一个官方的 [客户端库](https://v1-14.docs.kubernetes.io/docs/reference/using-api/client-libraries/)。这些库可以自动发现 API 服务器并进行身份验证。

在运行在 Pod 中时，可以通过 `default` 命名空间中的名为 `kubernetes` 的服务访问 Kubernetes apiserver。也就是说，Pods 可以使用 `kubernetes.default.svc` 主机名来查询 API 服务器。官方客户端库自动完成这个工作。

从一个 Pod 内，向 API 服务器进行身份认证的推荐的做法是使用 [服务账号](https://v1-14.docs.kubernetes.io/docs/user-guide/service-accounts) 凭证。默认的，一个 Pod 与一个服务账号关联，该服务账户的凭证（令牌）放置在此 Pod 中每个容器的文件系统树中的 `/var/run/secrets/kubernetes.io/serviceaccount/token` 处。

如果可用，凭证包被放入每个容器的文件系统树中的 `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt` 处，并且将被用于验证 API 服务器的服务证书。

最后，用于命名空间 API 操作的默认的命名空间放置在每个容器中的 `/var/run/secrets/kubernetes.io/serviceaccount/namespace` 文件中。

从一个 Pod 内，连接 Kubernetes API 的推荐方法是：

-   使用官方的 [客户端库](https://v1-14.docs.kubernetes.io/docs/reference/using-api/client-libraries/) 因为他们会自动地完成 API 主机发现和身份认证。以 Go 客户端来说，`rest.InClusterConfig()` 可以帮助解决这个问题。参见 [这里的一个例子](https://git.k8s.io/client-go/examples/in-cluster-client-configuration/main.go)。

-   如果您想要在没有官方客户端库的情况下查询 API，可以在 Pod 里以一个新的边车容器的 [命令](https://v1-14.docs.kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/)的方式运行 `kubectl proxy` 。此方式下，`kubectl proxy` 将对 API 进行身份验证并将其公开在 Pod 的 `localhost` 接口上，以便 Pod 中的其他容器可以直接使用它。

在每种情况下，Pod 的服务账号凭证被用于与 API 服务器的安全通信。





## 访问集群中运行的服务

本文展示了如何连接 Kubernetes 集群上运行的服务。

### 准备工作

一个集群。

### 访问集群上运行的服务

在 Kubernetes 里， [nodes](https://kubernetes.io/docs/admin/node)、[pods](https://kubernetes.io/docs/user-guide/pods) 和 [services](https://kubernetes.io/docs/user-guide/services) 都有它们自己的 IP。许多情况下，集群上的 node IP、pod IP 和某些 service IP 路由不可达，所以不能从一个集群之外的节点访问它们，例如从你自己的台式机。

#### 连接方式

你有多种从集群外连接 nodes、pods 和 services 的选项：

-   通过公共 IP 访问 services。
    -   使用具有 `NodePort` 或 `LoadBalancer` 类型的 service，可以从外部访问它们。请查阅 [services](https://kubernetes.io/docs/user-guide/services) 和 [kubectl expose](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#expose) 文档。
    -   取决于你的集群环境，你可以仅把 service 暴露在你的企业网络环境中，也可以将其暴露在因特网上。需要考虑暴露的 service 是否安全，它是否有自己的用户认证？
    -   将 pods 放置于 services 背后。如果要访问一个副本集合中特定的 pod，例如用于调试目的时，请给 pod 指定一个独特的标签并创建一个新 service 选择这个标签。
    -   大部分情况下，都不需要应用开发者通过节点 IP 直接访问 nodes。
-   通过 Proxy Verb 访问 services、nodes 或者 pods。
    -   在访问 Apiserver 远程服务之前是否经过认证和授权？如果你的服务暴露到因特网中不够安全，或者需要获取 node IP 之上的端口，又或者处于调试目的时，请使用这个特性。
    -   Proxies 可能给某些应用带来麻烦。
    -   仅适用于 HTTP/HTTPS。
    -   在[这里](https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-services/#manually-constructing-apiserver-proxy-urls)描述
-   从集群中的 node 或者 pod 访问。
    -   运行一个 pod，然后使用 [kubectl exec](https://kubernetes.io/docs/user-guide/kubectl/v1.6/#exec) 连接到它的一个shell。从那个 shell 连接其他的 nodes、pods 和 services。
    -   某些集群可能允许你 ssh 到集群中的节点。你可能可以从那儿访问集群服务。这是一个非标准的方式，可能在一些集群上能工作，但在另一些上却不能。浏览器和其他工具可能安装或可能不会安装。集群 DNS 可能不会正常工作。

#### 发现内置服务

典型情况下，kube-system 会启动集群中的几个服务。使用 `kubectl cluster-info` 命令获取它们的列表：

```shell
$ kubectl cluster-info

  Kubernetes master is running at https://104.197.5.247
  elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
  kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy
  kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy
  grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
  heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy
```

这显示了用于访问每个服务的 proxy-verb URL。例如，这个集群启用了（使用 Elasticsearch）集群层面的日志，如果提供合适的凭据可以通过 `https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/`访问，或通过一个 kubectl 代理地址访问，如：`http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/`。（请查看 [上文](https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-services/#accessing-the-cluster-api) 关于如何传递凭据或者使用 kubectl 代理的说明。）

##### 手动构建 apiserver 代理 URLs

如同上面所提到的，你可以使用 `kubectl cluster-info` 命令取得 service 的代理 URL。为了创建包含 service endpoints、suffixes 和 parameters 的代理 URLs，你可以简单的在 service 的代理 URL中 添加： `http://`*kubernetes_master_address*`/api/v1/namespaces/`*namespace_name*`/services/`*service_name[:port_name]*`/proxy`

如果还没有为你的端口指定名称，你可以不用在 URL 中指定 *port_name*。

##### 示例

-   你可以通过 `http://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy`访问 Elasticsearch service endpoint `_search?q=user:kimchy`。
-   你可以通过 `https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true`访问 Elasticsearch 集群健康信息 endpoint `_cluster/health?pretty=true`。

```json
  {
    "cluster_name" : "kubernetes_logging",
    "status" : "yellow",
    "timed_out" : false,
    "number_of_nodes" : 1,
    "number_of_data_nodes" : 1,
    "active_primary_shards" : 5,
    "active_shards" : 5,
    "relocating_shards" : 0,
    "initializing_shards" : 0,
    "unassigned_shards" : 5
  }
```

##### 通过 web 浏览器访问集群中运行的服务

你或许能够将 apiserver 代理的 url 放入浏览器的地址栏，然而：

-   Web 服务器不总是能够传递令牌，所以你可能需要使用基本（密码）认证。 Apiserver 可以配置为接受基本认证，但你的集群可能并没有这样配置。
-   某些 web 应用可能不能工作，特别是那些使用客户端侧 javascript 的应用，它们构造 url 的方式可能不能理解代理路径前缀。





## 为节点发布资源扩展

### 准备工作

有一个集群



### 获取节点名

```shell
kubectl get nodes
```

选择其中的一个节点来做这次练习。



### 在某个节点上附件一个新的扩展资源

要在节点上附件一个新的扩展资源，需要给kubernetes API server发一个 HTTP PATCH请求。比如你想在某个节点上附加4个dongle，那么下面就是一个附加四个dongle资源的PATCH请求的例子：

```shell
PATCH /api/v1/nodes/<your-node-name>/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "add",
    "path": "/status/capacity/example.com~1dongle",
    "value": "4"
  }
]
```

注意，kubernetes并不知道dongle是什么有什么用，前面的PATCH请求只是告诉你的节点有四个叫作dongle的东西。

启动代理，可以更简便地给kubernetes API server：

```
kubectl proxy
```

在另一个命令行窗口发送HTTP PATCH请求。把 `<your-node-name>` 换成你的节点名：

```shell
curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/example.com~1dongle", "value": "4"}]' \
http://localhost:8001/api/v1/nodes/<your-node-name>/status
```

>   **Note:** 在前面的请求中，patch路径中的 `~1` 代表 `/` ，JSON-Patch中的operation路径被解释为json指针（JSON-Pointer），详见 [IETF RFC 6901](https://tools.ietf.org/html/rfc6901)的第3部分。

输出显示这个节点有4个dongle的容量

```
"capacity": {
  "cpu": "2",
  "memory": "2049008Ki",
  "example.com/dongle": "4",
```

查看节点的详细信息：

```
kubectl describe node <your-node-name>
```

再次输出了dongle资源：

```yaml
Capacity:
 cpu:  2
 memory:  2049008Ki
 example.com/dongle:  4
```

现在，app开发人员可以创建一些包含请求一定数量的dongle的pod了。查看[Assign Extended Resources to a Container](https://kubernetes.io/docs/tasks/configure-pod-container/extended-resource/)。（配置pod和容器.md -> 分配扩展资源给容器）。



### 讨论

扩展资源跟内存和CPU很像，比如，节点有确定数量的内存和CPU可以被所有运行在节点上的组件共享，也可以有确定数量的dongle可以被所有运行在节点上的组件共享。并且app开发者创建容器时可以请求一定数量的内存和CPU，也可以请求一定数量的dongle。

扩展资源对kubernetes是非透明的，kubernetes对他们一无所知。只知道一个季度电商有一定数量的这些东西。扩展资源只能以整数附加，比如节点可以附加4个dongle，但不能附加4.5个dongle。

#### 存储的例子

假如一个节点有800G的特殊磁盘空间，你就可以为这篇特殊空间起个名字，比如叫 example.com/special-storage，然后你就可以以一定大小的块来附加了，比如100G。这样，你的节点就会附加8个 example.com/special-storage 类型的资源。

```yaml
Capacity:
 ...
 example.com/special-storage: 8
```

如果你想允许对特殊存储的任意请求，可以使用大小为1byte的块来附加特殊存储。这样，你将会附加800G的类型为 example.com/special-storage 的资源。

```yaml
Capacity:
 ...
 example.com/special-storage:  800Gi
```

然后容器就可以申请任意字节的特殊存储了，最高800G。



### 清理

下面是在节点上移除附加dongle的PATCH请求：

```shell
PATCH /api/v1/nodes/<your-node-name>/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "remove",
    "path": "/status/capacity/example.com~1dongle",
  }
]
```

启动代理，可以更简单地给kubernetes API server 发送请求：

```sh
kubectl proxy
```

在另一个命令行窗口发送HTTP PATCH请求。把 `<your-node-name>` 换成你的节点名：

```shell
curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "remove", "path": "/status/capacity/example.com~1dongle"}]' \
http://localhost:8001/api/v1/nodes/<your-node-name>/status
```

验证dongle是否被移除：

```
kubectl describe node <your-node-name> | grep dongle
```









## 在集群中自动缩放DNS服务

这一节展示在kubernetes集群中如何启用和配置DNS服务自动缩放。

### 准备工作

-   一个集群

-   本指南假设你的节点使用AMD64或Intel 64 CPU架构
-   确保 [DNS 功能(feature)](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) 本身处于开启状态
-   建议使用kubernetes1.4.0及之后的版本

### 确认是否已经启用DNS水平自动缩放

列出集群中kube-system命名空间下的 [部署(Deployments)](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) ：

```shell
kubectl get deployment --namespace=kube-system
```

输出大致是：

```
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
...
dns-autoscaler        1         1         1            1           ...
...
```

如果在输出中看到了“dns-autoscaler” ，那么DNS 水平自动缩放已经启用了，可以跳到 [优化自动缩放参数](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/#tuning-autoscaling-parameters).

### 获取DNS部署或副本控制器的名称

列出集群中kube-system命名空间下的部署：

```shell
kubectl get deployment --namespace=kube-system
```

输出大致是：

```
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
...
coredns      2         2         2            2           ...
...
```

在kubernetes 1.12之前的版本中，DNS部署叫做“kube-dns”。

在Kubernetes 1.5之前的版本中，DNS是使用副本控制器(ReplicationController)而不是部署来实现的。因此，如果在前面的输出中没有看到kube-dns或类似的名称，列出集群中kube-system命名空间下的ReplicationController：

```shell
kubectl get rc --namespace=kube-system
```

输出大致是：

```
NAME            DESIRED   CURRENT   READY     AGE
...
kube-dns-v20    1         1         1         ...
...
```

### 确定缩放目标

如果你有DNS部署，那么你的缩放目标是：

```
Deployment/<your-deployment-name>
```

 `<your-deployment-name>` 是你的DNS部署的名称。比如，如果你的DNS部署名是coredns，那么你的缩放目标就是Deployment/coredns。

如果你有DNS 副本控制器，那么你的缩放目标是：

```
ReplicationController/<your-rc-name>
```

 `<your-rc-name>` 是你的DNS副本控制器的名称。比如，如果你的DNS副本控制器名是kube-dns-v20，那么你的缩放目标就是ReplicationController/kube-dns-v20。

### 开启DNS水平自动缩放

这部分，你会创建一个部署，部署中的pod运行基于 `cluster-proportional-autoscaler-amd64` 的镜像。

创建一个名为 `dns-horizontal-autoscaler.yaml`  的文件，内容如下：



```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dns-autoscaler
  namespace: kube-system
  labels:
    k8s-app: dns-autoscaler
spec:
  selector:
    matchLabels:
      k8s-app: dns-autoscaler
  template:
    metadata:
      labels:
        k8s-app: dns-autoscaler
    spec:
      containers:
      - name: autoscaler
        image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.6.0
        resources:
          requests:
            cpu: 20m
            memory: 10Mi
        command:
        - /cluster-proportional-autoscaler
        - --namespace=kube-system
        - --configmap=dns-autoscaler
        - --target=<SCALE_TARGET>
        # When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.
        # If using small nodes, "nodesPerReplica" should dominate.
        - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"min":1}}
        - --logtostderr=true
        - --v=2
```

在文件中，将 `<SCALE_TARGET>` 替换为你的缩放目标。

进入包含你的配置文件的目录，输入这个命令来创建部署：

```shell
kubectl apply -f dns-horizontal-autoscaler.yaml
```

执行成功的输出结果：

```
deployment.apps/kube-dns-autoscaler created
```

现在DNS 水平缩放已经启用了。

### 优化自动缩放参数

验证是否存在 dns-autoscaler [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) ：

```shell
kubectl get configmap --namespace=kube-system
```

输出大致是：

```
NAME                  DATA      AGE
...
dns-autoscaler        1         ...
...
```

修改ConfigMap中的数据：

```shell
kubectl edit configmap dns-autoscaler --namespace=kube-system
```

看这一行：

```yaml
linear: '{"coresPerReplica":256,"min":1,"nodesPerReplica":16}'
```

根据您的需要修改字段。“min”字段表示DNS后端数量的最小值。后端数量的实际值计算公式如下：

```
replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )
```

注意 `coresPerReplica` 和 `nodesPerReplica` 的值都是整数。

其思想是，当集群使用具有多个核的节点时，`coresPerReplica` 占主导地位。当一个集群使用的节点的核比较少时，`nodesPerReplica` 占主导地位。

还支持其他的缩放模式。详见[集群比例自动伸缩器](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler)。

### 禁用DNS水平自动缩放

有几个选项可以优化DNS水平自动缩放。使用哪个选项取决于不同的条件。

#### 选项1：将dns-autoscaler部署缩减到0个副本

这个选项适用于所有情况。输入这个命令：

```shell
kubectl scale deployment --replicas=0 dns-autoscaler --namespace=kube-system
```

输出：

```
deployment.extensions/dns-autoscaler scaled
```

验证副本数是否为0：

```shell
kubectl get deployment --namespace=kube-system
```

输出显示列DESIRED 和CURRENT 为0：

```
NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
...
dns-autoscaler        0         0         0            0           ...
...
```

#### 选项2：删除dns-autoscaler部署

dns-autoscaler处于你的控制下时有效。这意味着没有人会重新创建它：

```shell
kubectl delete deployment dns-autoscaler --namespace=kube-system
```

输出：

```
deployment.extensions "dns-autoscaler" deleted
```

#### 选项3：从master节点删除dns-autoscaler清单文件

dns-autoscaler处于(已废弃) [插件管理器(Addon Manager)](https://git.k8s.io/kubernetes/cluster/addons/README.md) 的控制下并且你有对master的写入权限时有效。 

登录到主节点并删除对应的清单（manifest）文件。这个dns-autoscaler的常见路径是：

```
/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
```

清单文件删除之后，插件管理器会删除dns-autoscaler 部署。

### 理解DNS水平自动扩展如何工作

-   集群比例自动缩放（cluster-proportional-autoscaler）应用程序与DNS服务分开部署。 
-   自动缩放器Pod运行一个客户端，该客户端轮询Kubernetes API服务器，以确定集群中的节点和核心的数量。
-   根据当前可调度节点和核心以及给定的缩放参数，计算所需的副本数并将其应用于DNS后端。
-   缩放参数和数据点通过ConfigMap提供给autoscaler，它会在每个轮询间隔刷新它的参数表，以获得最新的缩放参数需求。
-   不需要重新构建或重新启动自动缩放器Pod就可以更改缩放参数。
-   自动调度器提供了一个控制接口来支持两种控制模式：*线性*和*梯形*。

### 未来开发

正在考虑未来开发除了线性和梯形之外的自定义度量的控制模式。

基于DNS指定的缩放度量的DNS后端缩放是未来的发展方向。当前使用集群中的节点和内核的数量来实现是有限制的。

正在考虑未来支持自定义度量，类似于[水平Pod自动缩放(Horizontal Pod Autoscaling)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) 提供的指标)。

### 下一步

-   学习关于 [集群比例自动缩放的实现](https://github.com/kubernetes-incubator/cluster-proportional-autoscaler) 的更多知识





## 更改持久卷的回收策略

本文展示了如何更改 Kubernetes PersistentVolume 的回收策略。

### 准备工作

一个集群。

### 为什么要更改 PersistentVolume 的回收策略

`PersistentVolumes` 可以有多种回收策略，包括 “Retain”、”Recycle” 和 “Delete”。对于动态配置的 `PersistentVolumes` 来说，默认回收策略为 “Delete”。这表示当用户删除对应的 `PersistentVolumeClaim` 时，动态配置的 volume 将被自动删除。如果 volume 包含重要数据时，这种自动行为可能是不合适的。那种情况下，更适合使用 “Retain” 策略。使用 “Retain” 时，如果用户删除 `PersistentVolumeClaim`，对应的 `PersistentVolume` 不会被删除。相反，它将变为 `Released` 状态，表示所有的数据可以被手动恢复。

### 更改 PersistentVolume 的回收策略

1.  列出你集群中的 PersistentVolumes

    ```shell
    kubectl get pv
    ```

    输出类似于这样：

    ```
    NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                  REASON    AGE
    pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1                   10s
    pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2                   6s
    pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim3                   3s
    ```

这个列表同样包含了绑定到每个 volume 的 claims 名称，以便更容易的识别动态配置的 volumes。

1.  选择你的 PersistentVolumes 中的一个并更改它的回收策略：

    ```shell
    kubectl patch pv <your-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
    ```

    这里的 `<your-pv-name>` 是你选择的 PersistentVolume 的名字。

2.  验证你选择的 PersistentVolume 拥有正确的策略：

    ```shell
    kubectl get pv
    ```

    输出类似于这样：

    ```
    NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                  REASON    AGE
    pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim1                   40s
    pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Delete          Bound     default/claim2                   36s
    pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94   4Gi        RWO           Retain          Bound     default/claim3                   33s
    ```

    在前面的输出中，你可以看到绑定到 claim `default/claim3` 的 volume 拥有的回收策略为 `Retain`。当用户删除 claim `default/claim3` 时，它不会被自动删除。

### 接下来

-   了解更多关于 [PersistentVolumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)的信息。
-   了解更多关于 [PersistentVolumeClaims](https://kubernetes.io/docs/user-guide/persistent-volumes/#persistentvolumeclaims) 的信息。

### 参考

-   [PersistentVolume](https://kubernetes.io/docs/api-reference/v1.15/#persistentvolume-v1-core)
-   [PersistentVolumeClaim](https://kubernetes.io/docs/api-reference/v1.15/#persistentvolumeclaim-v1-core)
-   查阅 [PersistentVolumeSpec](https://kubernetes.io/docs/api-reference/v1.15/#persistentvolumeclaim-v1-core) 的 `persistentVolumeReclaimPolicy` 字段。





## 更改默认的存储类(StorageClass)

本文展示了如何改变默认的 Storage Class，它用于为没有特殊需求的 PersistentVolumeClaims 配置 volumes。

### 准备工作

你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

### 为什么要改变默认 storage class？

取决于安装模式，您的 Kubernetes 集群可能和一个被标记为默认的已有 StorageClass 一起部署。这个默认的 StorageClass 以后将被用于动态的为没有特定 storage class 需求的 PersistentVolumeClaims 配置存储。更多细节请查看 [PersistentVolumeClaim 文档](https://kubernetes.io/docs/user-guide/persistent-volumes/#class-1)。

预先安装的默认 StorageClass 可能不能很好的适应您期望的工作负载；例如，它配置的存储可能太过昂贵。如果是这样的话，您可以改变默认 StorageClass，或者完全禁用它以防止动态配置存储。

简单的删除默认 StorageClass 可能行不通，因为它可能会被您集群中的插件管理器自动重建。请查阅您的安装文档中关于插件管理器的细节，以及如何禁用单个插件。

### 改变默认 StorageClass

1.  列出您集群中的 StorageClasses：

    ```
    kubectl get storageclass
    ```

    输出类似这样：

    ```
    NAME                 TYPE
    standard (default)   kubernetes.io/gce-pd
    gold                 kubernetes.io/gce-pd
    ```

    默认 StorageClass 标记是 `(default)` 。

2.  标记默认 StorageClass 非默认：

    默认 StorageClass 的注解 `storageclass.kubernetes.io/is-default-class` 设置为 `true`。注解的其它任意值或者缺省值将被解释为 `false`。

    要标记一个 StorageClass 为非默认的，您需要改变它的值为 `false`：

    ```
    kubectl patch storageclass <your-class-name> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
    ```

    这里的 `<your-class-name>` 是您选择的 StorageClass 的名字。

3.  标记一个 StorageClass 为默认的：

    和前面的步骤类似，您需要添加/设置注解 `storageclass.kubernetes.io/is-default-class=true`。

    ```
    kubectl patch storageclass <your-class-name> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
    ```

    请注意，最多只能有一个 StorageClass 能够被标记为默认。如果它们中有两个或多个被标记为默认，Kubernetes 将忽略这个注解，也就是它将表现为没有默认 StorageClass。

1.  验证您选用的 StorageClass 为默认的：

    ```
    kubectl get storageclass
    ```

    输出类似这样：

    ```
    NAME             TYPE
    standard         kubernetes.io/gce-pd
    gold (default)   kubernetes.io/gce-pd
    ```





## 集群管理

本文描述了和集群生命周期相关的几个主题：创建新集群、更新集群的 master 和 worker 节点、执行节点维护（例如升级内核）以及升级运行中集群的 Kubernetes API 版本。

### 创建和配置集群

要在一组机器上安装 Kubernetes， 请根据您的环境，查阅现有的 [入门指南](https://kubernetes.io/docs/getting-started-guides/)

### 升级集群

集群升级当前是配套提供的，某些发布版本在升级时可能需要特殊处理。推荐管理员在升级他们的集群前，同时查阅 [发行说明](https://git.k8s.io/kubernetes/CHANGELOG.md) 和版本具体升级说明。

-   [升级到 1.6](https://kubernetes.io/docs/admin/upgrade-1-6)

#### 升级 Google Compute Engine 集群

Google Compute Engine Open Source (GCE-OSS) 通过删除和重建 master 来支持 master 升级。通过维持相同的 Persistent Disk (PD) 以保证在升级过程中保留数据。

GCE 的 Node 升级采用 [管理实例组](https://cloud.google.com/compute/docs/instance-groups/)，每个节点将被顺序的删除，然后使用新软件重建。任何运行在那个节点上的 Pod 需要用 Replication Controller 控制，或者在扩容之后手动重建。

开源 Google Compute Engine (GCE) 集群上的升级过程由 `cluster/gce/upgrade.sh` 脚本控制。

运行 `cluster/gce/upgrade.sh -h` 获取使用说明。

例如，只将 master 升级到一个指定的版本 (v1.0.2):

```shell
cluster/gce/upgrade.sh -M v1.0.2
```

或者，将整个集群升级到最新的稳定版本：

```shell
cluster/gce/upgrade.sh release/stable
```

#### 升级 Google Kubernetes Engine 集群

Google Kubernetes Engine 自动升级 master 组件（例如 `kube-apiserver`、`kube-scheduler`）至最新版本。它还负责 master 运行的操作系统和其它组件。

节点升级过程由用户初始化，[Google Kubernetes Engine 文档](https://cloud.google.com/kubernetes-engine/docs/clusters/upgrade) 里有相关描述。

#### 在其他平台上升级集群

不同的供应商和工具管理升级的过程各不相同。建议您查阅它们有关升级的主要文档。

-   [kops](https://github.com/kubernetes/kops)
-   [kubespray](https://github.com/kubernetes-incubator/kubespray)
-   [CoreOS Tectonic](https://coreos.com/tectonic/docs/latest/admin/upgrade.html)
-   …

### 调整集群大小

如果集群资源短缺，您可以轻松的添加更多的机器，如果集群正运行在[节点自注册模式](https://kubernetes.io/docs/admin/node/#self-registration-of-nodes)下的话。如果正在使用的是 GCE 或者 Google Kubernetes Engine，这将通过调整管理节点的实例组的大小完成。在 [Google Cloud Console page](https://console.developers.google.com/) 的 `Compute > Compute Engine > Instance groups > your group > Edit group` 下修改实例数量或使用 gcloud CLI 都可以完成这个任务。

```shell
gcloud compute instance-groups managed resize kubernetes-minion-group --size 42 --zone $ZONE
```

实例组将负责在新机器上放置恰当的镜像并启动它们。Kubelet 将向 API server 注册它的节点以使其可以用于调度。如果您对 instance group 进行缩容，系统将会随机选取节点来终止。

在其他环境上，您可能需要手动配置机器并告诉 Kubelet API server 在哪台机器上运行。

#### 集群自动伸缩

如果正在使用 GCE 或者 Google Kubernetes Engine，您可以配置您的集群，使其能够基于 pod 需求自动重新调整大小。

如 [Compute Resource](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) 所述，用户可以控制预留多少 CPU 和内存来分配给 pod。这个信息被 Kubernetes scheduler 用来寻找一个运行 pod 的地方。如果没有一个节点有足够的空闲容量（或者不能满足其他 pod 的需求），这个 pod 就需要等待某些 pod 结束，或者一个新的节点被添加。

集群 autoscaler 查找不能被调度的 pod 并检查添加一个新节点（和集群中其它节点类似的）是否有帮助。如果是的话，它将调整集群的大小以容纳等待调度的 pod。

如果发现在一段延时时间内（默认10分钟，将来有可能改变）某些节点不再需要，集群 autoscaler 也会缩小集群。

集群 autoscaler 在每一个实例组（GCE）或节点池（Google Kubernetes Engine）上配置。

如果您使用 GCE，那么您可以在使用 kube-up.sh 脚本创建集群的时候启用它。要想配置集群 autoscaler，您需要设置三个环境变量：

-   `KUBE_ENABLE_CLUSTER_AUTOSCALER` - 如果设置为 true 将启用集群 autoscaler。
-   `KUBE_AUTOSCALER_MIN_NODES` - 集群的最小节点数量。
-   `KUBE_AUTOSCALER_MAX_NODES` - 集群的最大节点数量。

示例：

```shell
KUBE_ENABLE_CLUSTER_AUTOSCALER=true KUBE_AUTOSCALER_MIN_NODES=3 KUBE_AUTOSCALER_MAX_NODES=10 NUM_NODES=5 ./cluster/kube-up.sh
```

在 Google Kubernetes Engine 上，您可以在创建、更新集群或创建一个特别的节点池（您希望自动伸缩的）时，通过给对应的 `gcloud` 命令传递 `--enable-autoscaling` `--min-nodes`和 `--max-nodes` 来配置集群 autoscaler。

示例：

```shell
gcloud container clusters create mytestcluster --zone=us-central1-b --enable-autoscaling --min-nodes=3 --max-nodes=10 --num-nodes=5
gcloud container clusters update mytestcluster --enable-autoscaling --min-nodes=1 --max-nodes=15
```

**集群 autoscaler 期望节点未被手动修改过（例如通过 kubectl 添加标签），因为那些属性可能不能被传递到相同节点组中的新节点上。**

### 维护节点

如果需要重启节点（例如内核升级、libc 升级、硬件维修等），且停机时间很短时，当 Kubelet 重启后，它将尝试重启调度到节点上的 pod。如果重启花费较长时间（默认时间为 5 分钟，由 controller-manager 的 `--pod-eviction-timeout` 控制），节点控制器将会结束绑定到这个不可用节点上的 pod。如果存在对应的 replica set（或者 replication controller）时，则将在另一个节点上启动 pod 的新副本。所以，如果所有的 pod 都是复制而来，那么在不是所有节点都同时停机的前提下，升级可以在不需要特殊调整情况下完成。

如果您希望更多的控制升级过程，可以使用下面的工作流程：

使用 `kubectl drain` 优雅的结束节点上的所有 pod 并同时标记节点为不可调度：

```shell
kubectl drain $NODENAME
```

在您正试图使节点离线时，这将阻止新的 pod 落到它们上面。

对于有 replica set 的 pod 来说，它们将会被新的 pod 替换并且将被调度到一个新的节点。此外，如果 pod 是一个 service 的一部分，则客户端将被自动重定向到新的 pod。

对于没有 replica set 的 pod，您需要手动启动 pod 的新副本，并且如果它不是 service 的一部分，您需要手动将客户端重定向到这个 pod。

在节点上执行维护工作。

重新使节点可调度：

```shell
kubectl uncordon $NODENAME
```

如果删除了节点的虚拟机实例并重新创建，那么一个新的可调度节点资源将被自动创建（只在您使用支持节点发现的云服务提供商时；当前只有 Google Compute Engine，不包括在 Google Compute Engine 上使用 kube-register 的 CoreOS）。相关详细信息，请查阅 [节点](https://kubernetes.io/docs/admin/node)。

### 高级主题

#### 升级到不同的 API 版本

当新的 API 版本发布时，您可能需要升级集群支持新的 API 版本（例如当 ‘v2’ 发布时从 ‘v1’ 切换到 ‘v2’）。

这不是一个经常性的事件，但需要谨慎的处理。这里有一系列升级到新 API 版本的步骤。

```
  1. 开启新 API 版本。
  2. 升级集群存储来使用新版本。
  3. 升级所有配置文件。识别使用旧 API 版本 endpoint 的用户。
  4. 运行 `cluster/update-storage-objects.sh` 升级存储中的现有对象为新版本。
  5. 关闭旧 API 版本。
```

#### 打开或关闭集群的 API 版本

可以在启动 API server 时传递 `--runtime-config=api/<version>` 标志来打开或关闭特定的 API 版本。例如：要关闭 v1 API，请传递 `--runtime-config=api/v1=false`。运行时配置还支持两个特殊键值：api/all 和 api/legacy，分别控制全部和遗留 API。例如要关闭除 v1 外全部 API 版本，请传递 `--runtime-config=api/all=false,api/v1=true`。对于这些标志来说，*legacy* API 指那些被显式废弃的 API（例如 `v1beta3`）。

#### 切换集群存储的 API 版本

存储于磁盘中，用于在集群内部代表 Kubernetes 活跃资源的对象使用特定的 API 版本书写。当支撑的 API 改变时，这些对象可能需要使用更新的 API 重写。重写失败将最终导致资源不再能够被 Kubernetes API server 解析或使用。

`kube-apiserver` 二进制文件的 `KUBE_API_VERSIONS` 环境变量控制了集群支持的 API 版本。列表中的第一个版本被用作集群的存储版本。因此，要设置特定的版本为存储版本，请将其放在 `KUBE_API_VERSIONS` 参数值版本列表的最前面。您需要重启 `kube-apiserver` 二进制以使这个变量的改动生效。

#### 切换配置文件为新 API 版本

可以使用 `kubectl convert` 命令对不同 API 版本的配置文件进行转换。

```shell
kubectl convert -f pod.yaml --output-version v1
```

更多选项请参考 [kubectl convert](https://kubernetes.io/docs/user-guide/kubectl/v1.6/#convert) 命令用法。





## 配置多个调度器

Kubernetes 自带了一个默认调度器，其详细描述请查阅[这里](https://v1-14.docs.kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)。 如果默认调度器不适合您的需求，您可以实现自己的调度器。 不仅如此，您甚至可以伴随着默认调度器同时运行多个调度器，并告诉 Kubernetes 为每个 pod 使用什么调度器。 让我们通过一个例子讲述如何在 Kubernetes 中运行多个调度器。

关于实现调度器的具体细节描述超出了本文范围。 请参考 kube-scheduler 的实现，规范示例代码位于 [pkg/scheduler](https://github.com/kubernetes/kubernetes/tree/v1.14.3/pkg/scheduler)。

### 准备工作

你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://v1-14.docs.kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/) –>

To check the version, enter `kubectl version`.

### 打包调度器

将调度器二进制文件打包到容器镜像中。出于示例目的，我们就使用默认调度器（kube-scheduler）作为我们的第二个调度器。 从 Github 克隆 [Kubernetes 源代码](https://github.com/kubernetes/kubernetes)，并编译构建源代码。

```shell
git clone https://github.com/kubernetes/kubernetes.git
cd kubernetes
make
```

创建一个包含 kube-scheduler 二进制文件的容器镜像。用于构建镜像的 `Dockerfile` 内容如下：

```docker
FROM busybox
ADD ./_output/dockerized/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler
```

将文件保存为 `Dockerfile`，构建镜像并将其推送到镜像仓库。 此示例将镜像推送到 [Google 容器镜像仓库（GCR）](https://cloud.google.com/container-registry/)。 有关详细信息，请阅读 GCR [文档](https://cloud.google.com/container-registry/docs/)。

```shell
docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 .
gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0
```

### 为调度器定义 Kubernetes Deployment

现在我们将调度器放在容器镜像中，我们可以为它创建一个 pod 配置，并在我们的 Kubernetes 集群中运行它。 但是与其在集群中直接创建一个 pod，不如使用 [Deployment](https://v1-14.docs.kubernetes.io/docs/concepts/workloads/controllers/deployment/)。[Deployment](https://v1-14.docs.kubernetes.io/docs/concepts/workloads/controllers/deployment/) 管理一个 [Replica Set](https://v1-14.docs.kubernetes.io/docs/concepts/workloads/controllers/replicaset/)，Replica Set 再管理 pod，从而使调度器能够适应故障。 以下是 Deployment 配置，被保存为 `my-scheduler.yaml`：

[`admin/sched/my-scheduler.yaml` ](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/sched/my-scheduler.yaml)

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-scheduler
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: my-scheduler-as-kube-scheduler
subjects:
- kind: ServiceAccount
  name: my-scheduler
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      component: scheduler
      tier: control-plane
  replicas: 1
  template:
    metadata:
      labels:
        component: scheduler
        tier: control-plane
        version: second
    spec:
      serviceAccountName: my-scheduler
      containers:
      - command:
        - /usr/local/bin/kube-scheduler
        - --address=0.0.0.0
        - --leader-elect=false
        - --scheduler-name=my-scheduler
        image: gcr.io/my-gcp-project/my-kube-scheduler:1.0
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10251
          initialDelaySeconds: 15
        name: kube-second-scheduler
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10251
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        volumeMounts: []
      hostNetwork: false
      hostPID: false
      volumes: []
```

这里需要注意的是，在该部署文件中 Container 的 spec 配置的调度器启动命令参数（–scheduler-name）指定的调度器名称应该是惟一的。 这个名称应该与 pods 上的可选参数 `spec.schedulerName` 的值相匹配，也就是说调度器名称的匹配关系决定了 pods 的调度任务由哪个调度器负责。

还要注意，我们创建了一个专用服务帐户 `my-scheduler` 并将集群角色 `system:kube-scheduler` 绑定到它，以便它可以获得与 `kube-scheduler` 相同的权限。

请参阅 [kube-scheduler 文档](https://v1-14.docs.kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)以获取其他命令行参数的详细说明。

### 在集群中运行第二个调度器

为了在 Kubernetes 集群中运行我们的第二个调度器，只需在 Kubernetes 集群中创建上面配置中指定的 Deployment：

```shell
kubectl create -f my-scheduler.yaml
```

验证调度器 pod 正在运行：

```shell
$ kubectl get pods --namespace=kube-system
NAME                                           READY     STATUS    RESTARTS   AGE
....
my-scheduler-lnf4s-4744f                       1/1       Running   0          2m
...
```

此列表中，除了默认的 kube-scheduler pod 之外，您应该还能看到处于 “Running” 状态的 my-scheduler pod。

要在启用了 leader 选举的情况下运行多调度器，您必须执行以下操作：

首先，更新上述 Deployment YAML（my-scheduler.yaml）文件中的以下字段：

-   `--leader-elect=true`
-   `--lock-object-namespace=lock-object-namespace`
-   `--lock-object-name=lock-object-name`

如果在集群上启用了 RBAC，则必须更新 `system：kube-scheduler` 集群角色。将调度器名称添加到应用于端点资源的规则的 resourceNames，如以下示例所示：

```
$ kubectl edit clusterrole system:kube-scheduler
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
    name: system:kube-scheduler
  rules:
  - apiGroups:
    - ""
    resourceNames:
    - kube-scheduler
    - my-scheduler
    resources:
    - endpoints
    verbs:
    - delete
    - get
    - patch
    - update
```

### 指定 pod 的调度器

现在我们的第二个调度器正在运行，让我们创建一些 pod，并指定它们由默认调度器或我们刚部署的调度器进行调度。 为了使用特定的调度器调度给定的 pod，我们在那个 pod 的 spec 中指定调度器的名称。让我们看看三个例子。

-   Pod spec 没有任何调度器名称 

[`admin/sched/pod1.yaml` ](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/sched/pod1.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-annotation
  labels:
    name: multischeduler-example
spec:
  containers:
  - name: pod-with-no-annotation-container
    image: k8s.gcr.io/pause:2.0
```

如果未提供调度器名称，则会使用 default-scheduler 自动调度 pod。

将此文件另存为 `pod1.yaml`，并将其提交给 Kubernetes 集群。

```shell
kubectl create -f pod1.yaml
```

-   Pod spec 设置为 `default-scheduler` 

[`admin/sched/pod2.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/sched/pod2.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: annotation-default-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulerName: default-scheduler
  containers:
  - name: pod-with-default-annotation-container
    image: k8s.gcr.io/pause:2.0
```

通过将调度器名称作为 `spec.schedulerName` 参数的值来指定调度器。在这种情况下，我们提供默认调度器的名称，即 `default-scheduler`。

将此文件另存为 `pod2.yaml`，并将其提交给 Kubernetes 集群。

```shell
kubectl create -f pod2.yaml
```

-   Pod spec 设置为 `my-scheduler` 

[`admin/sched/pod3.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/sched/pod3.yaml)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: annotation-second-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulerName: my-scheduler
  containers:
  - name: pod-with-second-annotation-container
    image: k8s.gcr.io/pause:2.0
```

在这种情况下，我们指定此 pod 使用我们部署的 `my-scheduler` 来调度。 请注意，`spec.schedulerName` 参数的值应该与 Deployment 中配置的提供给 scheduler 命令的参数名称匹配。

将此文件另存为 `pod3.yaml`，并将其提交给 Kubernetes 集群。

```shell
kubectl create -f pod3.yaml
```

确认所有三个 pod 都在运行。

```shell
kubectl get pods
```

#### 验证是否使用所需的调度器调度了 pod

为了更容易地完成这些示例，我们没有验证 pod 实际上是使用所需的调度程序调度的。 我们可以通过更改 pod 的顺序和上面的部署配置提交来验证这一点。 如果我们在提交调度器部署配置之前将所有 pod 配置提交给 Kubernetes 集群，我们将看到注解了 `annotation-second-scheduler` 的 pod 始终处于 “Pending” 状态，而其他两个 pod 被调度。 一旦我们提交调度器部署配置并且我们的新调度器开始运行，注解了 `annotation-second-scheduler` 的 pod 就能被调度。

或者，可以查看事件日志中的 “Scheduled” 条目，以验证是否由所需的调度器调度了 pod。

```shell
kubectl get events
```





## 配置资源不足时的处理方式

本页介绍了如何使用 `kubelet` 配置资源不足时的处理方式。

当可用计算资源较少时，`kubelet` 需要保证节点稳定性。这在处理如内存和硬盘之类的不可压缩资源时尤为重要。如果任意一种资源耗尽，节点将会变得不稳定。

### 驱逐策略

`kubelet` 能够主动监测和防止计算资源的全面短缺。在那种情况下，`kubelet` 可以主动地结束一个或多个 pod 以回收短缺的资源。当 `kubelet` 结束一个 pod 时，它将终止 pod 中的所有容器，而 pod 的 `PodPhase` 将变为 `Failed`。

#### 驱逐信号

`kubelet` 支持按照以下表格中描述的信号触发驱逐决定。每个信号的值在 description 列描述，基于 `kubelet` 摘要 API。

| 驱逐信号             | 描述                                                         |
| :------------------- | :----------------------------------------------------------- |
| `memory.available`   | `memory.available` := `node.status.capacity[memory]` - `node.stats.memory.workingSet` |
| `nodefs.available`   | `nodefs.available` := `node.stats.fs.available`              |
| `nodefs.inodesFree`  | `nodefs.inodesFree` := `node.stats.fs.inodesFree`            |
| `imagefs.available`  | `imagefs.available` := `node.stats.runtime.imagefs.available` |
| `imagefs.inodesFree` | `imagefs.inodesFree` := `node.stats.runtime.imagefs.inodesFree` |

上面的每个信号都支持字面值或百分比的值。基于百分比的值的计算与每个信号对应的总容量相关。

`memory.available` 的值从 cgroupfs 获取，而不是通过类似 `free -m` 的工具。这很重要，因为 `free -m` 不能在容器中工作，并且如果用户使用了[可分配节点](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)特性，资源不足的判定将同时在本地 cgroup 层次结构的终端用户 pod 部分和根节点做出。这个[脚本](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/out-of-resource/memory-available.sh)复现了与 `kubelet` 计算 `memory.available` 相同的步骤。`kubelet` 将 inactive_file（意即活动 LRU 列表上基于文件后端的内存字节数）从计算中排除，因为它假设内存在出现压力时将被回收。

`kubelet` 只支持两种文件系统分区。

1.  `nodefs` 文件系统，kubelet 将其用于卷和守护程序日志等。
2.  `imagefs` 文件系统，容器运行时用于保存镜像和容器可写层。

`imagefs` 可选。`kubelet` 使用 cAdvisor 自动发现这些文件系统。`kubelet` 不关心其它文件系统。当前不支持配置任何其它类型。例如，在专用`文件系统`中存储卷和日志是不可以的。

在将来的发布中，`kubelet` 将废除当前存在的[垃圾回收](https://v1-14.docs.kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/)机制，这种机制目前支持将驱逐操作作为对磁盘压力的响应。

#### 驱逐阈值

`kubelet` 支持指定驱逐阈值，用于触发 `kubelet` 回收资源。

每个阈值形式如下：

```
[eviction-signal][operator][quantity]
```

-   合法的 `eviction-signal` 标志如上所示。
-   `operator` 是所需的关系运算符，例如 `<`。
-   `quantity` 是驱逐阈值值标志，例如 `1Gi`。合法的标志必须匹配 Kubernetes 使用的数量表示。驱逐阈值也可以使用 `%` 标记表示百分比。

举例说明，如果一个节点有 `10Gi` 内存，希望在可用内存下降到 `1Gi` 以下时引起驱逐操作，则驱逐阈值可以使用下面任意一种方式指定（但不是两者同时）。

-   `memory.available<10%`
-   `memory.available<1Gi`

##### 软驱逐阈值

软驱逐阈值使用一对由驱逐阈值和管理员必须指定的宽限期组成的配置对。在超过宽限期前，`kubelet` 不会采取任何动作回收和驱逐信号关联的资源。如果没有提供宽限期，`kubelet` 启动时将报错。

此外，如果达到了软驱逐阈值，操作员可以指定从节点驱逐 pod 时，在宽限期内允许结束的 pod 的最大数量。如果指定了 `pod.Spec.TerminationGracePeriodSeconds` 值，`kubelet`将使用它和宽限期二者中较小的一个。如果没有指定，`kubelet` 将立即终止 pod，而不会优雅结束它们。

软驱逐阈值的配置支持下列标记：

-   `eviction-soft` 描述了驱逐阈值的集合（例如 `memory.available<1.5Gi`），如果在宽限期之外满足条件将触发 pod 驱逐。
-   `eviction-soft-grace-period` 描述了驱逐宽限期的集合（例如 `memory.available=1m30s`），对应于在驱逐 pod 前软驱逐阈值应该被控制的时长。
-   `eviction-max-pod-grace-period` 描述了当满足软驱逐阈值并终止 pod 时允许的最大宽限期值（秒数）。

##### 硬驱逐阈值

硬驱逐阈值没有宽限期，一旦察觉，`kubelet` 将立即采取行动回收关联的短缺资源。如果满足硬驱逐阈值，`kubelet` 将立即结束 pod 而不是优雅终止。

硬驱逐阈值的配置支持下列标记：

-   `eviction-hard` 描述了驱逐阈值的集合（例如 `memory.available<1Gi`），如果满足条件将触发 pod 驱逐。

`kubelet` 有如下所示的默认硬驱逐阈值：

-   `memory.available<100Mi`
-   `nodefs.available<10%`
-   `nodefs.inodesFree<5%`
-   `imagefs.available<15%`

#### 驱逐监控时间间隔

`kubelet` 根据其配置的整理时间间隔计算驱逐阈值。

-   `housekeeping-interval` 是容器管理时间间隔。

#### 节点状态

`kubelet` 会将一个或多个驱逐信号映射到对应的节点状态。

如果满足硬驱逐阈值，或者满足独立于其关联宽限期的软驱逐阈值时，`kubelet` 将报告节点处于压力下的状态。

下列节点状态根据相应的驱逐信号定义。

| 节点状态         | 驱逐信号                                                     | 描述                                                         |
| :--------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| `MemoryPressure` | `memory.available`                                           | Available memory on the node has satisfied an eviction threshold |
| `DiskPressure`   | `nodefs.available`, `nodefs.inodesFree`, `imagefs.available`, or `imagefs.inodesFree` | Available disk space and inodes on either the node’s root filesystem or image filesystem has satisfied an eviction threshold |

`kubelet` 将以 `--node-status-update-frequency` 指定的频率连续报告节点状态更新，其默认值为 `10s`。

#### 节点状态振荡

如果节点在软驱逐阈值的上下振荡，但没有超过关联的宽限期时，将引起对应节点的状态持续在 true 和 false 间跳变，并导致不好的调度结果。

为了防止这种振荡，可以定义下面的标志，用于控制 `kubelet` 从压力状态中退出之前必须等待的时间。

-   `eviction-pressure-transition-period` 是 `kubelet` 从压力状态中退出之前必须等待的时长。

`kubelet` 将确保在设定的时间段内没有发现和指定压力条件相对应的驱逐阈值被满足时，才会将状态变回 `false`。

#### 回收节点层级资源

如果满足驱逐阈值并超过了宽限期，`kubelet` 将启动回收压力资源的过程，直到它发现低于设定阈值的信号为止。

`kubelet` 将尝试在驱逐终端用户 pod 前回收节点层级资源。发现磁盘压力时，如果节点针对容器运行时配置有独占的 `imagefs`，`kubelet` 回收节点层级资源的方式将会不同。

##### 使用 `Imagefs`

如果 `nodefs` 文件系统满足驱逐阈值，`kubelet` 通过驱逐 pod 及其容器来释放磁盘空间。

如果 `imagefs` 文件系统满足驱逐阈值，`kubelet` 通过删除所有未使用的镜像来释放磁盘空间。

##### 未使用 `Imagefs`

如果 `nodefs` 满足驱逐阈值，`kubelet` 将以下面的顺序释放磁盘空间：

1.  删除停止运行的 pod/container
2.  删除全部没有使用的镜像

#### 驱逐最终用户的 pod

如果 `kubelet` 在节点上无法回收足够的资源，`kubelet` 将开始驱逐 pod。

`kubelet` 首先根据他们对短缺资源的使用是否超过请求来排除 pod 的驱逐行为，然后通过[优先级](https://v1-14.docs.kubernetes.io/docs/concepts/configuration/pod-priority-preemption/)，然后通过相对于 pod 的调度请求消耗急需的计算资源。

`kubelet` 按以下顺序对要驱逐的 pod 排名：

-   `BestEffort` 或 `Burstable`，其对短缺资源的使用超过了其请求，此类 pod 按优先级排序，然后使用高于请求。
-   `Guaranteed` pod 和 `Burstable` pod,其使用率低于请求，最后被驱逐。`Guaranteed` pod 只有为所有的容器指定了要求和限制并且它们相等时才能得到保证。由于另一个 pod 的资源消耗，这些 pod 保证永远不会被驱逐。如果系统守护进程（例如 `kubelet`、`docker`、和 `journald`）消耗的资源多于通过 `system-reserved` 或 `kube-reserved` 分配保留的资源，并且该节点只有 `Guaranteed` 或 `Burstable` pod 使用少于剩余的请求，然后节点必须选择驱逐这样的 pod 以保持节点的稳定性并限制意外消耗对其他 pod 的影响。在这种情况下，它将首先驱逐优先级最低的 pod。

必要时，`kubelet` 会在遇到 `DiskPressure` 时驱逐一个 pod 来回收磁盘空间。如果 `kubelet` 响应 `inode` 短缺，它会首先驱逐服务质量最低的 pod 来回收 `inodes`。如果 `kubelet` 响应缺少可用磁盘，它会将 pod 排在服务质量范围内，该服务会消耗大量的磁盘并首先结束这些磁盘。

##### 使用 `imagefs`

如果是 `nodefs` 触发驱逐，`kubelet` 将按 `nodefs` 用量 - 本地卷 + pod 的所有容器日志的总和对其排序。

如果是 `imagefs` 触发驱逐，`kubelet` 将按 pod 所有可写层的用量对其进行排序。

##### 未使用 `imagefs`

如果是 `nodefs` 触发驱逐，`kubelet` 会根据磁盘的总使用情况对 pod 进行排序 - 本地卷 + 所有容器的日志及其可写层。

#### 最小驱逐回收

在某些场景，驱逐 pod 会导致回收少量资源。这将导致 `kubelet` 反复碰到驱逐阈值。除此之外，对如 `disk` 这类资源的驱逐时比较耗时的。

为了减少这类问题，`kubelet` 可以为每个资源配置一个 `minimum-reclaim`。当 `kubelet` 发现资源压力时，`kubelet` 将尝试至少回收驱逐阈值之下 `minimum-reclaim` 数量的资源。

例如使用下面的配置：

```
--eviction-hard=memory.available<500Mi,nodefs.available<1Gi,imagefs.available<100Gi
--eviction-minimum-reclaim="memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi"`
```

如果 `memory.available` 驱逐阈值被触发，`kubelet` 将保证 `memory.available` 至少为 `500Mi`。对于 `nodefs.available`，`kubelet` 将保证 `nodefs.available` 至少为 `1.5Gi`。对于 `imagefs.available`，`kubelet` 将保证 `imagefs.available` 至少为 `102Gi`，直到不再有相关资源报告压力为止。

所有资源的默认 `eviction-minimum-reclaim` 值为 `0`。

#### 调度器

当资源处于压力之下时，节点将报告状态。调度器将那种状态视为一种信号，阻止更多 pod 调度到这个节点上。

| 节点状态         | 调度器行为                                          |
| :--------------- | :-------------------------------------------------- |
| `MemoryPressure` | No new `BestEffort` Pods are scheduled to the node. |
| `DiskPressure`   | No new Pods are scheduled to the node.              |

### 节点 OOM 行为

如果节点在 `kubelet` 回收内存之前经历了系统 OOM(内存不足) 事件，它将基于 [oom_killer](https://lwn.net/Articles/391222/) 做出响应。

`kubelet` 基于 pod 的 service 质量为每个容器设置一个 `oom_score_adj` 值。

| Service 质量 | oom_score_adj                                                |
| :----------- | :----------------------------------------------------------- |
| `Guaranteed` | -998                                                         |
| `BestEffort` | 1000                                                         |
| `Burstable`  | min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999) |

如果 `kubelet` 在节点经历系统 OOM 之前无法回收内存，`oom_killer` 将基于它在节点上使用的内存百分比算出一个 `oom_score`，并加上 `oom_score_adj` 得到容器的有效 `oom_score`，然后结束得分最高的容器。

预期的行为应该是拥有最低 service 质量并消耗和调度请求相关内存量最多的容器第一个被结束，以回收内存。

和 pod 驱逐不同，如果一个 pod 的容器是被 OOM 结束的，基于其 `RestartPolicy`，它可能会被 `kubelet` 重新启动。

### 最佳实践

以下部分描述了资源外处理的最佳实践。

#### 可调度资源和驱逐策略

考虑以下场景：

-   节点内存容量：`10Gi`
-   操作员希望为系统守护进程保留 10% 内存容量（内核、`kubelet` 等）。
-   操作员希望在内存用量达到 95% 时驱逐 pod，以减少对系统的冲击并防止系统 OOM 的发生。

为了促成这个场景，`kubelet` 将像下面这样启动：

```
--eviction-hard=memory.available<500Mi
--system-reserved=memory=1.5Gi
```

这个配置的暗示是理解“系统保留”应该包含被驱逐阈值覆盖的内存数量。

要达到这个容量，要么某些 pod 使用了超过它们请求的资源，要么系统使用的内存超过 `1.5Gi - 500Mi = 1Gi`。

这个配置将保证在 pod 使用量都不超过它们配置的请求值时，如果可能立即引起内存压力并触发驱逐时，调度器不会将 pod 放到这个节点上。

#### DaemonSet

我们永远都不希望 `kubelet` 驱逐一个从 `DaemonSet` 派生的 pod，因为这个 pod 将立即被重建并调度回相同的节点。

目前，`kubelet` 没有办法区分一个 pod 是由 `DaemonSet` 还是其他对象创建。如果/当这个信息可用时，`kubelet` 可能会预先将这些 pod 从提供给驱逐策略的候选集合中过滤掉。

总之，强烈推荐 `DaemonSet` 不要创建 `BestEffort` 的 pod，防止其被识别为驱逐的候选 pod。相反，理想情况下 `DaemonSet` 应该启动 `Guaranteed` 的 pod。

### 弃用现有特性标签以回收磁盘

`kubelet` 已经按需求清空了磁盘空间以保证节点稳定性。

当磁盘驱逐成熟时，下面的 `kubelet` 标志将被标记为废弃的，以简化支持驱逐的配置。

| 现有标签                                  | 新标签                                  |
| :---------------------------------------- | :-------------------------------------- |
| `--image-gc-high-threshold`               | `--eviction-hard` or `eviction-soft`    |
| `--image-gc-low-threshold`                | `--eviction-minimum-reclaim`            |
| `--maximum-dead-containers`               | deprecated                              |
| `--maximum-dead-containers-per-container` | deprecated                              |
| `--minimum-container-ttl-duration`        | deprecated                              |
| `--low-diskspace-threshold-mb`            | `--eviction-hard` or `eviction-soft`    |
| `--outofdisk-transition-frequency`        | `--eviction-pressure-transition-period` |

### 已知问题

以下部分描述了与资源外处理有关的已知问题。

#### kubelet 可能无法立即发现内存压力

`kubelet` 当前通过以固定的时间间隔轮询 `cAdvisor` 来收集内存使用数据。如果内存使用在那个时间窗口内迅速增长，`kubelet` 可能不能足够快的发现 `MemoryPressure`，`OOMKiller`将不会被调用。我们准备在将来的发行版本中通过集成 `memcg` 通知 API 来减小这种延迟。当超过阈值时，内核将立即告诉我们。

如果您想处理可察觉的超量使用而不要求极端精准，可以设置驱逐阈值为大约 75% 容量作为这个问题的变通手段。这将增强这个特性的能力，防止系统 OOM，并提升负载卸载能力，以再次平衡集群状态。

#### kubelet 可能会驱逐超过需求数量的 pod

由于状态采集的时间差，驱逐操作可能驱逐比所需的更多的 pod。将来可通过添加从根容器获取所需状态的能力 [(https://github.com/google/cadvisor/issues/1247)](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/[(https://github.com/google/cadvisor/issues/1247)](https://github.com/google/cadvisor/issues/1247)) 来减缓这种状况。





## 配置API对象的配额

本文讨论如何为 API 对象配置配额，包括 PersistentVolumeClaims 和 Services。 配额限制了可以在命名空间中创建的特定类型对象的数量。 您可以在 [ResourceQuota](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/#resourcequota-v1-core) 对象中指定配额。

### 准备工作

一个集群。

### 创建命名空间

创建一个命名空间以便本例中创建的资源和集群中的其余部分相隔离。

```shell
kubectl create namespace quota-object-example
```

### 创建 ResourceQuota

下面是一个 ResourceQuota 对象的配置文件：

[`admin/resource/quota-objects.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-objects.yaml)

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota-demo
spec:
  hard:
    persistentvolumeclaims: "1"
    services.loadbalancers: "2"
    services.nodeports: "0"
```

创建 ResourceQuota

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace=quota-object-example
```

查看 ResourceQuota 的详细信息：

```shell
kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml
```

输出结果表明在 quota-object-example 命名空间中，至多只能有一个 PersistentVolumeClaim，最多两个 LoadBalancer 类型的服务，不能有 NodePort 类型的服务。

```yaml
status:
  hard:
    persistentvolumeclaims: "1"
    services.loadbalancers: "2"
    services.nodeports: "0"
  used:
    persistentvolumeclaims: "0"
    services.loadbalancers: "0"
    services.nodeports: "0"
```

### 创建 PersistentVolumeClaim

下面是一个 PersistentVolumeClaim 对象的配置文件：

[`admin/resource/quota-objects-pvc.yaml` ](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-objects-pvc.yaml)

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-quota-demo
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```

创建 PersistentVolumeClaim：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace=quota-object-example
```

确认已创建完 PersistentVolumeClaim：

```shell
kubectl get persistentvolumeclaims --namespace=quota-object-example
```

输出信息表明 PersistentVolumeClaim 存在并且处于 Pending 状态：

```shell
NAME             STATUS
pvc-quota-demo   Pending
```

### 尝试创建第二个 PersistentVolumeClaim

下面是第二个 PersistentVolumeClaim 的配置文件：

[`admin/resource/quota-objects-pvc-2.yaml` ](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/resource/quota-objects-pvc-2.yaml)

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-quota-demo-2
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi
```

尝试创建第二个 PersistentVolumeClaim：

```shell
kubectl create -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace=quota-object-example
```

输出信息表明第二个 PersistentVolumeClaim 没有创建成功，因为这会超出命名空间的配额。

```
persistentvolumeclaims "pvc-quota-demo-2" is forbidden:
exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1,
used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1
```

### 注意事项

下面这些字符串可被用来标识那些能被配额限制的 API 资源：

| String                   | API Object                   |
| :----------------------- | :--------------------------- |
| "pods"                   | Pod                          |
| "services                | Service                      |
| "replicationcontrollers" | ReplicationController        |
| "resourcequotas"         | ResourceQuota                |
| "secrets"                | Secret                       |
| "configmaps"             | ConfigMap                    |
| "persistentvolumeclaims" | PersistentVolumeClaim        |
| "services.nodeports"     | Service of type NodePort     |
| "services.loadbalancers" | Service of type LoadBalancer |

### 清理

删除您的命名空间：

```shell
kubectl delete namespace quota-object-example
```

### 接下来

#### 集群管理员参考

-   [为命名空间配置默认的内存请求和限制](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/)
-   [为命名空间配置默认的 CPU 请求和限制](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/)
-   [为命名空间配置内存的最小和最大限制](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/)
-   [为命名空间配置 CPU 的最小和最大限制](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/)
-   [为命名空间配置 CPU 和内存配额](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/quota-memory-cpu-namespace/)
-   [为命名空间配置 Pod 配额](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/)

#### 应用开发者参考

-   [为容器和 Pod 分配内存资源](https://v1-14.docs.kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/)
-   [为容器和 Pod 分配 CPU 资源](https://v1-14.docs.kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/)
-   [为 Pod 配置服务质量](https://v1-14.docs.kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/)





## 控制节点上的CPU管理策略

按照设计，Kubernetes 对 pod 执行相关的很多方面进行了抽象，使得用户不必关心。然而，为了正常运行，有些工作负载要求在延迟和/或性能方面有更强的保证。 为此，kubelet 提供方法来实现更复杂的负载放置策略，同时保持抽象，避免显式的放置指令。

### CPU 管理策略

默认情况下，kubelet 使用 [CFS 配额](https://en.wikipedia.org/wiki/Completely_Fair_Scheduler) 来执行 pod 的 CPU 约束。当节点上运行了很多 CPU 密集的 pod 时，工作负载可能会迁移到不同的 CPU 核，这取决于调度时 pod 是否被扼制，以及哪些 CPU 核是可用的。许多工作负载对这种迁移不敏感，因此无需任何干预即可正常工作。

然而，有些工作负载的性能明显地受到 CPU 缓存亲和性以及调度延迟的影响，对此，kubelet 提供了可选的 CPU 管理策略，来确定节点上的一些分配偏好。

#### 配置

CPU 管理器（CPU Manager）作为 alpha 特性引入 Kubernetes 1.8 版本。 必须在 kubelet 特性开关中显式启用： `--feature-gates=CPUManager=true`。

CPU 管理策略通过 kubelet 参数 `--cpu-manager-policy` 来指定， 有两种支持策略：

-   `none`：默认策略，表示现有的调度行为。
-   `static`：允许为节点上具有某些资源特征的 pod 赋予增强的 CPU 亲和性和独占性。

CPU管理器定期通过 CRI 写入资源更新，以保证内存中 CPU 分配与 cgroupfs 一致。同步频率通过新增的 Kubelet 配置参数 `--cpu-manager-reconcile-period` 来设置。 如不指定，默认与 `--node-status-update-frequency` 的周期相同。

#### None 策略

`none` 策略显式地启用现有的默认 CPU 亲和方案，不提供操作系统调度器默认行为之外的亲和性策略。 通过 CFS 配额来实现 [Guaranteed pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/) 的 CPU 使用限制。

#### Static 策略

`static` 策略针对具有整数型 CPU `requests` 的 pod ，它允许该类 pod 中的容器访问节点上的独占 CPU 资源。这种独占性是使用 [cpuset cgroup 控制器](https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt) 来实现的。

>   **Note:** **注意:** 诸如容器运行时和 kubelet 本身的系统服务可以继续在这些独占 CPU 上运行。独占性仅针对其他 pod。

>   **Note:** **注意:** 该策略的 alpha 版本不保证 Kubelet 重启前后的静态独占性分配。

该策略管理一个共享 CPU 资源池，最初，该资源池包含节点上所有的 CPU 资源。可用的独占性 CPU 资源数量等于节点的 CPU 总量减去通过 `--kube-reserved` 或 `--system-reserved`参数保留的 CPU 。通过这些参数预留的 CPU 是以整数方式，按物理内核 ID 升序从初始共享池获取的。 共享池是 `BestEffort` 和 `Burstable` pod 运行的 CPU 集合。`Guaranteed` pod 中的容器，如果声明了非整数值的 CPU `requests` ，也将运行在共享池的 CPU 上。只有 `Guaranteed` pod 中，指定了整数型 CPU `requests` 的容器，才会被分配独占 CPU 资源。

>   **Note:** **注意:** 当启用 static 策略时，要求使用 `--kube-reserved` 和/或 `--system-reserved`来保证预留的 CPU 值大于零。 这是因为零预留 CPU 值可能使得共享池变空。

当 `Guaranteed` pod 调度到节点上时，如果其容器符合静态分配要求，相应的 CPU 会被从共享池中移除，并放置到容器的 cpuset 中。因为这些容器所使用的 CPU 受到调度域本身的限制，所以不需要使用 CFS 配额来进行 CPU 的绑定。换言之，容器 cpuset 中的 CPU 数量与 pod 规格中指定的整数型 CPU `limit` 相等。这种静态分配增强了 CPU 亲和性，减少了 CPU 密集的工作负载在节流时引起的上下文切换。

考虑以下 Pod 规格的容器：

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
```

该 pod 属于 `BestEffort` 服务质量类型，因为其未指定 `requests` 或 `limits` 值。 所以该容器运行在共享 CPU 池中。

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
```

该 pod 属于 `Burstable` 服务质量类型，因为其资源 `requests` 不等于 `limits`， 且未指定 `cpu` 数量。所以该容器运行在共享 CPU 池中。

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"
```

该 pod 属于 `Burstable` 服务质量类型，因为其资源 `requests` 不等于 `limits`。所以该容器运行在共享 CPU 池中。

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "200Mi"
        cpu: "2"
```

该 pod 属于 `Guaranteed` 服务质量类型，因为其 `requests` 值与 `limits`相等。 同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。所以，该 `nginx` 容器被赋予 2 个独占 CPU。

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "1.5"
      requests:
        memory: "200Mi"
        cpu: "1.5"
```

该 pod 属于 `Guaranteed` 服务质量类型，因为其 `requests` 值与 `limits`相等。但是容器对 CPU 资源的限制值是一个小数。所以该容器运行在共享 CPU 池中。

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
```

该 pod 属于 `Guaranteed` 服务质量类型，因其指定了 `limits` 值， 同时当未显式指定时，`requests` 值被设置为与 `limits` 值相等。同时，容器对 CPU 资源的限制值是一个大于或等于 1 的整数值。所以，该 `nginx` 容器被赋予 2 个独占 CPU。





## 定制DNS服务

这一节解释了如何配置你的DNSpod和自定义DNS解析过程。在Kubernetes 1.11及之后的版本中，CoreDNS位于GA中，默认情况下与kubeadm一起安装。请参阅 [CoreDNS ConfigMap 选项](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns-configmap-options) 和 [使用CoreDNS 进行服务发现](https://kubernetes.io/docs/tasks/administer-cluster/coredns/) 。

### 准备工作

-   一个集群

-   kubernetes版本1.6或更高。要使用CoreDNS，版本1.9或更高
-   适当的附加组件:kube-dns或CoreDNS。要使用kubeadm安装，请参阅[kubeadm参考文档](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha/#cmd-phase-addon) 。

### 简介

DNS是一个内置的Kubernetes服务，使用插件管理器 [cluster add-on](http://releases.k8s.io/master/cluster/addons/README.md) 自动启动。

自Kubernetes v1.12起，CoreDNS是推荐的DNS服务器，替代了kube-dns。但是，某些Kubernetes安装工具依然默认安装kube-dns。请参考安装程序提供的文档，了解默认安装的是哪个DNS服务器。

CoreDNS部署暴露为一个拥有静态IP的Kubernetes服务。CoreDNS和kube-dns服务在 `metadata.name` 字段中都叫 `kube-dns` 。这样做是为了与依赖遗留的 `kube-dns` 服务名称来解析集群内部地址的工作负载有更好的交互。 它抽象出了运行在公共端点后面的DNS提供者的实现细节。kubelet 使用 `--cluster-dns=<dns-service-ip>` 标志将DNS传递给每个容器。

DNS名称也需要域名。使用 `--cluster-domain=<default-local-domain>` 标志在kubelet中配置本地域名。

DNS服务器支持正向查找(A记录（不是一个记录))、端口查找(SRV记录)、反向IP地址查找(PTR记录)等。详见 [服务和pod的DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) 。

如果pod的 `dnsPolicy` 设置为 “`default`”，它将从运行Pod的节点继承名称解析配置。Pod的DNS解析应该与节点的表现相同。但是有[已知的问题](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues) 。

如果你不想这样，或者你想为pod配置一个不同的DNS配置，可以使用kubelet的 `--resolv-conf` 标志，将这个标志设置 "" 来阻止pod继承DNS。将其设置为一个有效的文件路径来指定用于DNS继承的 `/etc/resolv.conf` 之外的文件。

### CoreDNS

CoreDNS是一个通用的权威DNS服务器，可以作为集群DNS，符合[DNS规范](https://github.com/kubernetes/dns/blob/master/docs/specification.md)。

#### CoreDNS ConfigMap 配置

CoreDNS是一个模块化和可插拔的DNS服务器，每个插件都向CoreDNS添加了新的功能。这可以通过维护 [Corefile](https://coredns.io/2017/07/23/corefile-explained/) (CoreDNS配置文件)来配置。集群管理员可以修改ConfigMap来配置CoreDNS Corefile，以更改服务发现的工作方式。

在Kubernetes中，CoreDNS使用以下默认的Corefile配置文件来安装。

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

Corefile配置包括以下核心CoreDNS [插件](https://coredns.io/plugins/) ：

-   [errors](https://coredns.io/plugins/errors/): 错误信息被记录到标准输出（stdout）
-   [health](https://coredns.io/plugins/health/): CoreDNS 的健康状况上报在 http://localhost:8080/health
-   [kubernetes](https://coredns.io/plugins/kubernetes/): CoreDNS将基于服务的IP和基于kubernetes的pod来回应DNS查询。详见[这里](https://coredns.io/plugins/kubernetes/)

 `pods insecure` 选项用于向下兼容kube-dns。您可以使用 `pods verified` 选项，该选项仅当在相同的名称空间中存在与IP匹配的pod时才返回A记录。如果不使用pod记录，可以使用 `pods disabled` 选项。

-   [prometheus(普罗米修斯)](https://coredns.io/plugins/prometheus/):  CoreDNS的度量标准可以在http://localhost:9153/metrics上的 [Prometheus](https://prometheus.io/) 中找到
-   [forward(转发)](https://coredns.io/plugins/forward/): 不在Kubernetes集群域名内的任何查询将被转发到预定义的解析器(/etc/resolv.conf)
-   [cache(缓存)](https://coredns.io/plugins/cache/): 这个用来开启前端缓存
-   [loop(循环)](https://coredns.io/plugins/loop/): 检测简单的转发循环，如果发现循环，则停止CoreDNS进程。 
-   [reload(重载)](https://coredns.io/plugins/reload): 允许自动重载已更改的核心文件。在编辑ConfigMap配置之后，允许两分钟的时间来让更改生效。
-   [loadbalance(负载均衡)](https://coredns.io/plugins/loadbalance): 这是一个循环DNS负载均衡器，随机化答案中的A、AAAA和MX记录的顺序。

您可以通过修改ConfigMap来修改默认的CoreDNS行为。

#### 使用CoreDNS配置存根域(Stub-domain)和上游域名服务器(upstream nameserver)

CoreDNS能够使用 [forward 插件](https://coredns.io/plugins/forward/) 来配置存根域和上游域名服务器。

（这里的nameserver直译为名称服务器，为了方便理解翻译为域名服务器，但其实域名服务器是指 Domain Name Server，也就是DNS）

##### 示例

假设集群操作员(cluster operator)在10.150.0.1上有一个 [Consul](https://www.consul.io/) 域名服务器，并且所有的Consul 的名称都有 .consul.local 后缀，要在CoreDNS中配置他，需要集群管理员在CoreDNS ConfigMap中创建以下节。.

```yaml
consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
```

要明确地强制所有非集群DNS查找都经由指定的域名服务器172.16.0.1，将 `forward` 指向域名服务器，而不是 `/etc/resolv.conf`。

```
forward .  172.16.0.1
```

最终的ConfigMap以及默认的 `Corefile` 配置如下：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . 172.16.0.1
        cache 30
        loop
        reload
        loadbalance
    }
    consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
```

在Kubernetes 1.10及更高版本中，kubeadm支持从kube-dns ConfigMap自动转换为CoreDNS  ConfigMap。***注意:虽然kube-dns接受存根域和域名服务器(例如:ns.foo.com)的FQDN，但是CoreDNS不支持这个特性。在转换期间，CoreDNS配置中将省略所有FQDN域名服务器。***

### Kube-dns

Kube-dns现在作为一个可用的可选的DNS服务器，因为现在默认是CoreDNS。运行中的DNS Pod包含3个容器：

-   “`kubedns`“: 监视Kubernetes主服务器中服务和端点的更改，并维护内存中的查找结构来服务DNS请求
-   “`dnsmasq`“: 增加DNS缓存以提高性能
-   “`sidecar`“: 提供一个健康检查端点来执行 `dnsmasq` 和 `kubedns` 的健康检查。

#### 配置存根域和上游DNS服务器(stub-domain and upstream DNS servers)

集群管理员可以通过为kube-dns提供一个ConfigMap(`kube-system:kube-dns`)来指定自定义存根域和上游域名服务器。

例如，下面的ConfigMap将DNS设置为使用一个存根域和两个上游域名服务器：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  stubDomains: |
    {"acme.local": ["1.2.3.4"]}
  upstreamNameservers: |
    ["8.8.8.8", "8.8.4.4"]
```

带有 “.acme.local” 后缀的DNS 请求会被转发到监听1.2.3.4的DNS上。谷歌公共DNS会服务上游查询。

下表描述了具有特定域名的查询如何映射到它们的目标DNS服务器：

| 域名                                 | 响应查询的服务器                       |
| :----------------------------------- | :------------------------------------- |
| kubernetes.default.svc.cluster.local | kube-dns                               |
| foo.acme.local                       | custom DNS (1.2.3.4)                   |
| widget.com                           | upstream DNS (one of 8.8.8.8, 8.8.4.4) |

有关配置文件选项格式的详细信息，请参阅 [ConfigMap 选项](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#configmap-options) 。

##### 作用于pod

自定义上有域名服务器和存根域不会影响 `dnsPolicy` 设置为 “`Default`” 或 “`None`”.的pod。

如果Pod的 `dnsPolicy` 设置为 “`ClusterFirst`”，则根据配置的 存根域和上游DNS服务器 的不同，对其名称解析的处理也不同。

**没有自定义配置：**任何与配置的集群域名后缀不匹配的查询(如“www.kubernetes.io”)都会被转发到从节点继承的上游域名服务器。

**自定义配置：**如果配置了存根域和上游DNS服务器，DNS查询将按照以下流程路由：

1.  查询首先发送到kube-dns中的DNS缓存层。
2.  在缓存层，根据以下情况检查请求的后缀，然后转发到相应的DNS：
    -   *名称带有集群后缀的*，比如 “.cluster.local”：请求发送到kube-dns
    -   *名称带有存根域后缀的*，比如 “.acme.local”：请求发送到配置的正在监听的自定义DNS解析器（示例中是1.2.3.4）
    -   *不匹配后缀的*，比如 “widget.com”：请求转发到上有DNS，比如谷歌公共DNS服务器8.8.8.8 和 8.8.4.4。

![DNS lookup flow](https://d33wubrfki0l68.cloudfront.net/340889cb80e81dcd19a16bc34697a7907e2b229a/24ad0/docs/tasks/administer-cluster/dns-custom-nameservers/dns.png)

#### ConfigMap 选项

kube-dns `kube-system:kube-dns` 的ConfigMap的选项：

| 字段                                        | 格式                                                         | 描述                                                         |
| :------------------------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| `stubDomains` (存根域，可选)                | 一个JSON map，键是DNS后缀，比如"acme.local"，值是由DNS 的IP组成的JSON数组。 | 目标域名服务器可以是Kubernetes服务本身。例如，您可以运行自己的dnsmasq副本，将自定义DNS名称导出到ClusterDNS命名空间。 |
| `upstreamNameservers`(上游域名服务器，可选) | 由DNS 的IP组成的JSON数组。                                   | 如果指定了，这些值将替换默认从节点的`/etc/resolv.conf` 中获取的名称服务器。限制:最多可以指定三个上游名称服务器。 |

##### 示例

###### 示例：存根域(Stub domain)

在这个示例中，用户有一个Consul DNS服务发现系统，并希望与kube-dns集成。Consul 的域名服务器位于10.150.0.1，所有Consul 名称的后缀都是 `.consul.local` 。要配置Kubernetes，集群管理员需要创建以下ConfigMap：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  stubDomains: |
    {"consul.local": ["10.150.0.1"]}
```

注意，集群管理员不希望覆盖节点的上游域名服务器，因此他们没有指定可选的 `upstreamNameservers` 字段。

###### 示例：上游明明服务器

在本例中，集群管理员希望明确地强制所有非集群DNS查找都经由他们自己在172.16.0.1的域名服务器。在本例中，他们创建了一个ConfigMap，其中 `upstreamNameservers` 字段指定了想要的域名服务器：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  upstreamNameservers: |
    ["172.16.0.1"]
```

### CoreDNS 的配置跟kube-dns差不多

CoreDNS支持kube-dns等功能。在kube-dns创建的支持 `StubDomains`and `upstreamNameservers` 的ConfigMap，转化为CoreDNS中的forward插件。同样地，kube-dns中的 `Federations` 插件转换为CoreDNS中的 `federation` 插件。

#### 示例

这个kubedns 的ConfigMap指定了federations, stubdomains 和 upstreamnameservers（联合、存根域、上游域名服务器）：

```yaml
apiVersion: v1
data:
  federations: |
    {"foo" : "foo.feddomain.com"}
  stubDomains: |
    {"abc.com" : ["1.2.3.4"], "my.cluster.local" : ["2.3.4.5"]}
  upstreamNameservers: |
    ["8.8.8.8", "8.8.4.4"]
kind: ConfigMap
```

等效于在CoreDNS中创建Corefile：

-   对于federations：

    ```yaml
    federation cluster.local {
           foo foo.feddomain.com
        }
    ```

-   对于stubDomains：

    ```yaml
    abc.com:53 {
        errors
        cache 30
        forward . 1.2.3.4
    }
    my.cluster.local:53 {
        errors
        cache 30
        forward . 2.3.4.5
    }
    ```

带有默认插件的完整Corefile：

```yaml
.:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
        }
        federation cluster.local {
           foo foo.feddomain.com
        }
        prometheus :9153
        forward .  8.8.8.8 8.8.4.4
        cache 30
    }
    abc.com:53 {
        errors
        cache 30
        forward . 1.2.3.4
    }
    my.cluster.local:53 {
        errors
        cache 30
        forward . 2.3.4.5
    }
```

### 迁移到 CoreDNS

T要从 kube-dns 迁移到CoreDNS，可以使用[一个详细的博客](https://coredns.io/2018/05/21/migration-from-kube-dns-to-coredns/) 帮助用户调整CoreDNS来替代kube-dns。集群管理员还可以使用 [部署脚本](https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh).进行迁移。

### 下一步

-   [调试DNS 解析](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)





## 调试DNS解析

此页面提供诊断DNS问题的提示。

### 准备工作

-   一个集群

-   Kubernetes 版本1.6及以上
-   集群必须配置为使用 `coredns` (或 `kube-dns`) 插件

#### 创建一个简单的pod作为测试环境

创建一个名为busybox.yaml 的文件，内容如下：



```yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
```

使用这个文件创建一个pod并验证他的状态：

```shell
kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml
pod/busybox created

kubectl get pods busybox
NAME      READY     STATUS    RESTARTS   AGE
busybox   1/1       Running   0          <some-time>
```

pod运行之后，您就可以在该环境中执行c `nslookup` 。如果您看到类似下面的内容，就表示DNS在正常工作。

```shell
kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
```

如果 `nslookup` 命令执行失败，请检查以下内容：

#### 首先检查本地DNS配置

看看resolv.conf文件的内部。(有关更多信息，请参阅 [从节点继承DNS](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node) 和下面的 [已知问题](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/#known-issues)）

```shell
kubectl exec busybox cat /etc/resolv.conf
```

验证搜索路径和名称服务器是否设置如下(注意，搜索路径可能因不同的云提供商而不同)：

```
search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5
```

以下错误表明coredns/kube-dns附加组件或相关服务存在问题：

```
kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can't resolve 'kubernetes.default'
```

或

```
kubectl exec -ti busybox -- nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'kubernetes.default'
```

#### 检查DNS pod是否正在运行

使用 `kubectl get pods` 命令验证DNS pod是否在运行。

对于CoreDNS：

```shell
kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
```

对于kube-dns：

```shell
kubectl get pods --namespace=kube-system -l k8s-app=kube-dns
NAME                    READY     STATUS    RESTARTS   AGE
...
kube-dns-v19-ezo1y      3/3       Running   0           1h
...
```

如果您看到没有运行中的pod或pod失败(failed)/完成(completed)，当前环境可能不会默认部署DNS附加组件，您需要手动部署它。

#### 检查DNS pod中的错误

使用 `kubectl logs` 命令查看DNS容器的日志。

对于CoreDNS：

```shell
for p in $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name); do kubectl logs --namespace=kube-system $p; done
```

下面是一个健康的CoreDNS日志的例子：

```verilog
.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c
```

对于kube-dns，有3组日志：

```shell
kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c kubedns

kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c dnsmasq

kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name | head -1) -c sidecar
```

查看日志中是否有任何可疑的错误消息。在kube-dns中，行首的 ‘`W`’, ‘`E`’ ,  ‘`F`’ 表示警告、错误、失败。请搜索具有这些日志级别的条目，并使用 [kubernetes issues](https://github.com/kubernetes/kubernetes/issues) 报告异常错误。

#### DNS服务启动了吗?

使用 `kubectl get service` 命令验证DNS服务是否启动。

```shell
kubectl get svc --namespace=kube-system
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      <none>        53/UDP,53/TCP        1h
...
```

注意，对于CoreDNS和kube-dns部署，服务名称都是“kube-dns”。如果您已经创建了服务，或者在默认情况下应该创建服务，但是它没有出现，请参阅 [调试服务](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/) 以获得更多信息。

#### DNS端点是否暴露?

可以使用 `kubectl get endpoints` 命令验证DNS端点是否暴露。

```shell
kubectl get ep kube-dns --namespace=kube-system
NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
```

如果您没有看到端点，请参阅 [调试服务](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/) 文档中的端点部分。

有关其他Kubernetes DNS示例，请参阅Kubernetes GitHub存储库中的 [cluster-dns 示例](https://github.com/kubernetes/examples/tree/master/staging/cluster-dns) 。

#### 是否接收/处理DNS查询?

通过将 `log` 插件添加到CoreDNS配置文件(又名Corefile)，您可以验证查询是否被CoreDNS接收。CoreDNS Corefile保存在名为 `coredns` 的ConfigMap中。要编辑它，使用命令…

```
kubectl -n kube-system edit configmap coredns
```

然后根据下面的示例在Corefile部分添加 `log` 。

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        log
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

保存更改后，Kubernetes可能需要一到两分钟的时间将这些更改传播到CoreDNS pod。

接下来，进行一些查询并查看本文档中上述各部分的日志。如果CoreDNS pods正在接收查询，您应该在日志中看到它们。

下面是日志中的一个查询示例。

```verilog
.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 "A IN kubernetes.default.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd,ra 106 0.000066649s
```

### 已知问题

一些Linux发行版(例如Ubuntu)默认使用本地DNS解析器(systemd-resolved)。Systemd-resolved使用存根文件移动和替换 `/etc/resolv.conf` ，这在解析上游服务器中的名称时可能导致致命的转发循环。可以手动地将kubelet的 `--resolv-conf` 标志指向正确 `resolv.conf`(在 `systemd-resolved` 中是 `/run/systemd/resolve/resolv.conf`)来解决这个问题。kubeadm(>= 1.11)会自动检测 `systemd-resolved` ，并相应地调整kubelet标志。

Kubernetes安装之后默认情况下不会将节点的 `resolv.conf` 文件配置为使用集群DNS，因为这种操作本质上是特定于分布式（distribution-specific）的。这可能最终会实现。

限制了只有3条DNS `nameserver` 记录和6条DNS `search` 记录，Linux的libc不可思议地卡住了 ([参见这个2005年的bug](https://bugzilla.redhat.com/show_bug.cgi?id=168253))。Kubernetes需要使用1条 `nameserver` 记录和3条 `search` 记录。这意味着，如果本地安装已经使用了3个 `nameserver`s 或使用了3个以上的 `search`es，其中一些设置将会丢失。作为部分解决方案，节点可以运行 `dnsmasq` ，它将提供更多的 `nameserver` 条目，但不提供更多的 `search` 条目。您还可以使用kubelet的 `--resolv-conf` 标志。

如果您使用Alpine 3.3或更早的版本作为基础镜像，DNS可能无法正常工作，因为Alpine存在一个已知的问题。点击 [这里](https://github.com/kubernetes/kubernetes/issues/30215) 查看更多信息。

### Kubernetes 集群联合 (支持多站点)

1.3发行版引入了对多站点Kubernetes(multi-site Kubernetes)安装的集群联合支持。这需要对Kubernetes集群DNS服务器处理DNS查询的方式进行一些小的(向下兼容的)更改，以方便查询联合服务(跨多个Kubernetes集群)。有关集群联合和多站点支持的详细信息，请参阅[集群联合管理员指南](https://kubernetes.io/docs/concepts/cluster-administration/federation/) 。

### 参考

-   [服务和pod的DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
-   [kube-dns DNS 集群附件的文档](http://releases.k8s.io/master/cluster/addons/dns/kube-dns/README.md)

### 下一步

-   [在集群中自动缩放DNS服务](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/).





## 网络政策声明

本文可以帮助您开始使用 Kubernetes 的 [NetworkPolicy API](https://kubernetes.io/docs/concepts/services-networking/network-policies/) 声明网络策略去管理 Pod 之间的通信

### 准备工作

您首先需要有一个支持网络策略的 Kubernetes 集群。已经有许多支持 NetworkPolicy 的网络提供商，包括：

-   [Calico](https://kubernetes.io/docs/tasks/configure-pod-container/calico-network-policy/)
-   [Romana](https://kubernetes.io/docs/tasks/configure-pod-container/romana-network-policy/)
-   [Weave 网络](https://kubernetes.io/docs/tasks/configure-pod-container/weave-network-policy/)

**注意**：以上列表是根据产品名称按字母顺序排序，而不是按推荐或偏好排序。下面示例对于使用了上面任何提供商的 Kubernetes 集群都是有效的

### 创建一个`nginx` deployment 并且通过服务将其暴露

为了查看 Kubernetes 网络策略是怎样工作的，可以从创建一个`nginx` deployment 并且通过服务将其暴露开始

```console
$ kubectl run nginx --image=nginx --replicas=2
deployment "nginx" created
$ kubectl expose deployment nginx --port=80 
service "nginx" exposed
```

在 default 命名空间下运行了两个 `nginx` pod，而且通过一个名字为 `nginx` 的服务进行了暴露

```console
$ kubectl get svc,pod
NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
svc/kubernetes              10.100.0.1    <none>        443/TCP    46m
svc/nginx                   10.100.0.16   <none>        80/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
po/nginx-701339712-e0qfq    1/1           Running       0          35s
po/nginx-701339712-o00ef    1/1           Running       0          35s
```

### 测试服务能够被其它的 pod 访问

您应该可以从其它的 pod 访问这个新的 `nginx` 服务。为了验证它，从 default 命名空间下的其它 pod 来访问该服务。请您确保在该命名空间下没有执行孤立动作。

启动一个 busybox 容器，然后在容器中使用 `wget` 命令去访问 `nginx` 服务：

```console
$ kubectl run busybox --rm -ti --image=busybox /bin/sh
Waiting for pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: false

Hit enter for command prompt

/ # wget --spider --timeout=1 nginx
Connecting to nginx (10.100.0.16:80)
/ #
```

### 限制访问 `nginx` 服务

如果说您想限制 `nginx` 服务，只让那些拥有标签 `access: true` 的 pod 访问它，那么您可以创建一个只允许从那些 pod 连接的 `NetworkPolicy`：

```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-nginx
spec:
  podSelector:
    matchLabels:
      run: nginx
  ingress:
  - from:
    - podSelector:
        matchLabels:
          access: "true"
```

### 为服务指定策略

使用 kubectl 工具根据上面的 nginx-policy.yaml 文件创建一个 NetworkPolicy：

```console
$ kubectl create -f nginx-policy.yaml
networkpolicy "access-nginx" created
```

### 当访问标签没有定义时测试访问服务

如果您尝试从没有设定正确标签的 pod 中去访问 `nginx` 服务，请求将会超时：

```console
$ kubectl run busybox --rm -ti --image=busybox /bin/sh
Waiting for pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: false

Hit enter for command prompt

/ # wget --spider --timeout=1 nginx 
Connecting to nginx (10.100.0.16:80)
wget: download timed out
/ #
```

### 定义访问标签后再次测试

创建一个拥有正确标签的 pod，您将看到请求是被允许的：

```console
$ kubectl run busybox --rm -ti --labels="access=true" --image=busybox /bin/sh
Waiting for pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: false

Hit enter for command prompt

/ # wget --spider --timeout=1 nginx
Connecting to nginx (10.100.0.16:80)
/ #
```





## 开发云控制器管理器

**FEATURE STATE:** `Kubernetes v1.11` [beta](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/developing-cloud-controller-manager/#)

在即将发布的版本中，云控制器管理器将是把 Kubernetes 与任何云集成的首选方式。 这将确保驱动可以独立于核心 Kubernetes 发布周期开发其功能。

**FEATURE STATE:** `Kubernetes 1.8` [alpha](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/developing-cloud-controller-manager/#)

在讨论如何构建自己的云控制器管理器之前，了解有关它如何工作的一些背景知识是有帮助的。云控制器管理器是来自 `kube-controller-manager` 的代码，利用 Go 接口允许插入任何云的实现。大多数框架和通用控制器的实现在 core，但只要满足 [云提供者接口](https://github.com/kubernetes/cloud-provider/blob/master/cloud.go#L42-L62)，它就会始终执行它所提供的云接口。

为了深入了解实施细节，所有云控制器管理器都将从 Kubernetes 核心导入依赖包，唯一的区别是每个项目都会通过调用 [cloudprovider.RegisterCloudProvider](https://github.com/kubernetes/cloud-provider/blob/master/plugins.go#L56-L66) 来注册自己的驱动，更新可用驱动的全局变量。

-   [开发](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/developing-cloud-controller-manager/#%e5%bc%80%e5%8f%91)

### 开发

#### Out of Tree

要为您的云构建一个 out-of-tree 云控制器管理器，请按照下列步骤操作：

1.  使用满足 [cloudprovider.Interface](https://git.k8s.io/kubernetes/pkg/cloudprovider/cloud.go) 的实现创建一个 go 包。
2.  使用来自 Kubernetes 核心包的 [cloud-controller-manager 中的 main.go](https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/controller-manager.go) 作为 main.go 的模板。如上所述，唯一的区别应该是将导入的云包。
3.  在 `main.go` 中导入你的云包，确保你的包有一个 `init` 块来运行 cloudprovider.RegisterCloudProvider。

用现有的 out-of-tree 云驱动作为例子可能会有所帮助。你可以在这里找到 [清单](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller.md#examples)。

#### In Tree

对于 in-tree 驱动，您可以将 in-tree 云控制器管理器作为群集中的 [Daemonset](https://v1-14.docs.kubernetes.io/examples/admin/cloud/ccm-example.yaml) 运行。有关详细信息，请参阅 [运行的云控制器管理器文档](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller.md)。





## 静态(Rest)加密 Secret 数据(加密Rest?)

本文展示如何启用和配置静态 Secret 数据的加密

### 准备工作

你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://v1-14.docs.kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/) –>

To check the version, enter `kubectl version`.

### 配置并确定是否已启用静态数据加密

`kube-apiserver` 的参数 `--experimental-encryption-provider-config` 控制 API 数据在 etcd 中的加密方式。 下面提供一个配置示例。

### 理解静态数据加密

```yaml
kind: EncryptionConfiguration
apiVersion: apiserver.config.k8s.io/v1
resources:
  - resources:
    - secrets
    providers:
    - identity: {}
    - aesgcm:
        keys:
        - name: key1
          secret: c2VjcmV0IGlzIHNlY3VyZQ==
        - name: key2
          secret: dGhpcyBpcyBwYXNzd29yZA==
    - aescbc:
        keys:
        - name: key1
          secret: c2VjcmV0IGlzIHNlY3VyZQ==
        - name: key2
          secret: dGhpcyBpcyBwYXNzd29yZA==
    - secretbox:
        keys:
        - name: key1
          secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
```

每个 `resources` 数组项目是一个单独的完整的配置。 `resources.resources` 字段是要加密的 Kubernetes 资源名称（`resource` 或 `resource.group`）的数组。 `providers` 数组是可能的加密 provider 的有序列表。每个条目只能指定一个 provider 类型（可以是 `identity` 或 `aescbc`，但不能在同一个项目中同时指定）。

列表中的第一个提供者用于加密进入存储的资源。当从存储器读取资源时，与存储的数据匹配的所有提供者将尝试按顺序解密数据。 如果由于格式或密钥不匹配而导致提供者无法读取存储的数据，则会返回一个错误，以防止客户端访问该资源。

**重要：** 如果通过加密配置无法读取资源（因为密钥已更改），唯一的方法是直接从基础 etcd 中删除该密钥。任何尝试读取资源的调用将会失败，直到它被删除或提供有效的解密密钥。

#### Providers:

| 名称        | 加密类型                                                     | 强度                 | 速度 | 密钥长度            | 其它事项                                                     |
| :---------- | :----------------------------------------------------------- | :------------------- | :--- | :------------------ | :----------------------------------------------------------- |
| `identity`  | 无                                                           | N/A                  | N/A  | N/A                 | 不加密写入的资源。当设置为第一个 provider 时，资源将在新值写入时被解密。 |
| `aescbc`    | 填充 PKCS#7 的 AES-CBC                                       | 最强                 | 快   | 32字节              | 建议使用的加密项，但可能比 `secretbox` 稍微慢一些。          |
| `secretbox` | XSalsa20 和 Poly1305                                         | 强                   | 更快 | 32字节              | 较新的标准，在需要高度评审的环境中可能不被接受。             |
| `aesgcm`    | 带有随机数的 AES-GCM                                         | 必须每 200k 写入一次 | 最快 | 16, 24, 或者 32字节 | 建议不要使用，除非实施了自动密钥循环方案。                   |
| `kms`       | 使用信封加密方案：数据使用带有 PKCS#7 填充的 AES-CBC 通过 data encryption keys（DEK）加密，DEK 根据 Key Management Service（KMS）中的配置通过 key encryption keys（KEK）加密 | 最强                 | 快   | 32字节              | 建议使用第三方工具进行密钥管理。为每个加密生成新的 DEK，并由用户控制 KEK 轮换来简化密钥轮换。[配置 KMS 提供程序](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/kms-provider/) |

每个 provider 都支持多个密钥 - 在解密时会按顺序使用密钥，如果是第一个 provider，则第一个密钥用于加密。

### 加密您的数据

创建一个新的加密配置文件：

```yaml
kind: EncryptionConfiguration
apiVersion: apiserver.config.k8s.io/v1
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: <BASE 64 ENCODED SECRET>
    - identity: {}
```

遵循如下步骤来创建一个新的 secret：

1.  生成一个 32 字节的随机密钥并进行 base64 编码。如果您在 Linux 或 Mac OS X 上，请运行以下命令：

    ```
    head -c 32 /dev/urandom | base64
    ```

1.  将这个值放入到 secret 字段中。
2.  设置 `kube-apiserver` 的 `--experimental-encryption-provider-config` 参数，将其指定到配置文件所在位置。
3.  重启您的 API server。

**重要：** 您的配置文件包含可以解密 etcd 内容的密钥，因此您必须正确限制主设备的权限，以便只有能运行 kube-apiserver 的用户才能读取它。

### 验证数据是否被加密

数据在写入 etcd 时会被加密。重新启动你的 `kube-apiserver` 后，任何新创建或更新的密码在存储时都应该被加密。 如果想要检查，你可以使用 `etcdctl` 命令行程序来检索你的加密内容。

1.  创建一个新的 secret，名称为 `secret1`，命名空间为 `default`：

    ```
    kubectl create secret generic secret1 -n default --from-literal=mykey=mydata
    ```

1.  使用 etcdctl 命令行，从 etcd 中读取 secret：

    ```
       ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C
    ```

```
这里的 `[...]` 是用来连接 etcd 服务的额外参数。
```

1.  验证存储的密钥前缀是否为 `k8s:enc:aescbc:v1:`，这表明 `aescbc` provider 已加密结果数据。

2.  通过 API 检索，验证 secret 是否被正确解密：

    ```
    kubectl describe secret secret1 -n default
    ```

```
必须匹配 `mykey: mydata`
```

### 确保所有 secret 都被加密

由于 secret 是在写入时被加密，因此对 secret 执行更新也会加密该内容。

```
kubectl get secrets --all-namespaces -o json | kubectl replace -f -
```

上面的命令读取所有 secret，然后使用服务端加密来进行更新。 如果由于冲突写入而发生错误，请重试该命令。 对于较大的集群，您可能希望通过命名空间或更新脚本来分割 secret。

### 回滚解密密钥

在不发生停机的情况下更改 secret 需要多步操作，特别是在有多个 `kube-apiserver` 进程正在运行的高可用部署的情况下。

1.  生成一个新密钥并将其添加为所有服务器上当前提供程序的第二个密钥条目
2.  重新启动所有 `kube-apiserver` 进程以确保每台服务器都可以使用新密钥进行解密
3.  将新密钥设置为 `keys` 数组中的第一个条目，以便在配置中使用其进行加密
4.  重新启动所有 `kube-apiserver` 进程以确保每个服务器现在都使用新密钥进行加密
5.  运行 `kubectl get secrets --all-namespaces -o json | kubectl replace -f -` 以用新密钥加密所有现有的秘密
6.  在使用新密钥备份 etcd 后，从配置中删除旧的解密密钥并更新所有密钥

如果只有一个 `kube-apiserver`，第 2 步可能可以忽略。

### 解密所有数据

要禁用 rest 加密，请将 `identity` provider 作为配置中的第一个条目：

```yaml
kind: EncryptionConfiguration
apiVersion: apiserver.config.k8s.io/v1
resources:
  - resources:
    - secrets
    providers:
    - identity: {}
    - aescbc:
        keys:
        - name: key1
          secret: <BASE 64 ENCODED SECRET>
```

并重新启动所有 `kube-apiserver` 进程。然后运行命令 `kubectl get secrets --all-namespaces -o json | kubectl replace -f -` 强制解密所有 secret。





## 保证外部关键pod的调度

### 概览

除了 Kubernetes 核心组件，像运行在 master 机器上的 api-server、scheduler、controller-manager，由于各种原因，还有很多插件必须运行在一个普通的集群节点上（而不是 Kubernetes master）。 这些插件中的一些对于一个功能完备的集群来说是非常关键的，例如 Heapster、DNS 以及 UI。 如果一个关键的插件被移除（或者手动，或是类似升级这样具有副作用的其它操作），或者变成挂起状态（例如，当集群利用率过高，以及或者其它被调度到该空间中的挂起 Pod 被清理关键插件 Pod 给移除，或者由于其它原因导致节点上可用资源的总量发生变化），集群可能会停止正常工作。

请注意，将pod标记为critical并不意味着完全防止驱逐;它只能防止pod永久不可用。对于静态pod，这意味着它不能被清除，但是对于非静态pod，这只是意味着它们总是被重新调度。

#### 标记为关键插件

在v1.11之前，关键pod必须在kube-system命名空间中运行，这个限制在v1.11之后被移除，任何命名空间中的pod都可以配置为关键pod，方法如下:

-   确保启用了PodPriority特性开关(feature gates)。priorityClassName是否设置为“system-cluster-critical”或“system-node-critical”(后者是整个集群中最高的)，这是自v1.10+以来可用的两个优先级类名

-   或者，确保启用了PodPriority和experimental alcriticalpodannotation特性开关，您可以添加`scheduler.alpha.kubernetes.io/critical-pod`作为键，空字符串作为pod的值，但是这个注释在1.13版本中被弃用，并将在未来的版本中删除。





## IP伪装代理指南

这个页面显示了如何配置和启用IP伪装代理（ip-masq-agent）。

### 准备工作

一个集群

### 创建ip-masq-agent

要创建ip-masq-agent，执行下面的kubectl命令：

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml
```

您还必须将适当的节点标签应用于您希望在上面运行代理的集群中的任何节点。

```
kubectl label nodes my-node beta.kubernetes.io/masq-agent-ds-ready=true
```

更多信息可以在 [here](https://github.com/kubernetes-incubator/ip-masq-agent) 的ip-masq-agent文档中找到。

在大多数情况下，默认的规则集应该够用了；但是，如果您的集群不是这种情况，您可以创建并应用 [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) 来定制受影响的IP范围。例如，要让ip-masq-agent只考虑10.0.0.0/8，您可以创建一个名为“config”的 [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) 。

>   **注意：**
>
>   这个文件叫config很重要，因为在默认情况下，它将被ip-masq-agent用作查找的键：
>
>   ```
>   nonMasqueradeCIDRs:
>     - 10.0.0.0/8
>   resyncInterval: 60s
>   ```

运行以下命令将config map添加到集群：

```
kubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system
```

这将更新位于 */etc/config/ip-masq-agent* 的文件，该文件每隔一个 *重新同步间隔* 就会被定期检查并应用于集群节点。在重新同步间隔过后，您应该看到iptables规则反映了您的更改：

```
iptables -t nat -L IP-MASQ-AGENT
Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
```

默认情况下，本地链接范围(link local range)(169.254.0.0/16)也由ip-masq-agent处理，它设置了适当的iptables规则。要让ip-masq-agent忽略本地链接，可以在config map中将 *masqLinkLocal* 设置为true。

```
nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
masqLinkLocal: true
```

### IP Masquerade Agent 用户指南

（IP Masquerade Agent就是上面的ip-masq-agent，IP伪装代理）

ip-masq-agent 通过配置iptables规则来将pod的IP地址隐藏在集群节点的IP地址后面。这通常是在将流量发送到集群的pod [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) 范围之外的目的地时完成的。

#### **关键术语**

-   **NAT (Network Address Translation，网络地址转换)** 是一种通过修改IP头中的源地址和/或目标地址信息将一个IP地址映射到另一个IP地址的方法。通常由执行IP路由的设备执行。
-   **Masquerading，伪装** 是NAT的一种形式，通常用于执行多个对一个地址的转换，其中多个源IP地址隐藏在一个地址后面，这通常由执行IP路由的设备完成。在Kubernetes中是节点的IP地址（是说多个源地址伪装成节点的IP地址）。
-   **CIDR (Classless Inter-Domain Routing，无类别域间路由)** 基于可变长度子网掩码，允许指定任意长度的前缀。CIDR引入了一种新的IP地址表示方法，现在通常称为**CIDR表示法**，在这种表示法中，地址或路由前缀用后缀表示前缀的位数，如192.168.2.0/24。（24是后缀，表示前缀长度为24，有兴趣可以看[维基百科上的CIDR](https://zh.wikipedia.org/wiki/无类别域间路由)）
-   **Link Local** 本地连接地址（link-local address）是一个网络地址，它只对主机所连接的网络段或广播域内的通信有效。IPv4的本地连接地址在CIDR表示法中的地址块169.254.0.0/16中定义。

当向集群节点IP和集群IP范围之外的目的地发送流量时，IP -masq-agent配置iptables规则来处理伪装的节点/pod IP地址。这实际上将pod IP地址隐藏在集群节点的IP地址后面。在某些环境中，到”外部地址“的通信必须来自一个已知的机器地址(machine address)。例如，在谷歌云(Google Cloud)中，任何到internet的流量都必须来自VM 的IP。当使用容器时，如在谷歌Kubernetes引擎中，Pod IP将被拒绝外出(rejected for egress)。为了避免这种情况，我们必须将Pod IP隐藏在VM自己的IP地址后面——通常称为masquerade（伪装）。代理被配置为将这三个由 [RFC 1918](https://tools.ietf.org/html/rfc1918) 指定的私有IP范围作为非伪装CIDR(non-masquerade [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing))。这些范围是10.0.0.0/8、172.16.0.0/12和192.168.0.0/16。默认情况下，代理还将把本地连接(169.254.0.0/16)视为非伪装CIDR。代理被配置为每60秒从 */etc/config/ip-masq-agent* 重载其配置，这也是可选的。

![masq/non-masq example](https://d33wubrfki0l68.cloudfront.net/ea2a999040d282706bcc530f3a14af55e90a132b/a0c9d/images/docs/ip-masq.png)

代理配置文件必须使用YAML或JSON语法编写，可以包含三个可选键：

-   **nonMasqueradeCIDRs:**   [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) 表示法中指定非伪装范围的字符串列表。
-   **masqLinkLocal:** 一个布尔值(true / false)，指定是否将流量伪装成本地连接前缀169.254.0.0/16。默认为false。
-   **resyncInterval:** 代理尝试从磁盘重新加载配置的时间间隔。“30s”，其中“s”是秒，“ms”是毫秒，等等...

到10.0.0.0/8,172.16.0.0/12和192.168.0.0/16)范围的流量不会被伪装。任何其他流量(假定为internet)都会被伪装。比如，pod中的本地目的地的一个示例可以是其所在节点的IP地址，也可以另一个节点的地址或集群的IP范围内的一个的地址。默认情况下，任何其他流量都将被伪装。下面的条目显示了ip-masq-agent应用的默认规则集：

```sh
iptables -t nat -L IP-MASQ-AGENT
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
```

默认情况下，在从Kubernetes 版本1.7.0开始的GCE/谷歌Kubernetes引擎中，如果启用了网络策略，或者您使用的集群CIDR不在10.0.0.0/8范围内，ip-masq-agent将在您的集群中运行。如果您在另一个环境中运行，您可以将ip-masq-agent  [守护进程集(DaemonSet)](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) 添加到您的集群中。





## Kubernetes云控制器管理器

**FEATURE STATE:** `Kubernetes v1.15` [beta](https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#)

Kubernetes v1.6 包含一个新的二进制文件，叫做 `cloud-controller-manager`。`cloud-controller-manager` 是一个嵌入了特定云服务控制循环逻辑的守护进程。这些特定云服务控制循环逻辑最初存在于 `kube-controller-manager` 中。由于云服务提供商开发和发布的速度与 Kubernetes 项目不同，将服务提供商专用代码从 `cloud-controller-manager`二进制中抽象出来有助于云服务厂商在 Kubernetes 核心代码之外独立进行开发。

`cloud-controller-manager` 可以被链接到任何满足 [cloudprovider.Interface](https://github.com/kubernetes/cloud-provider/blob/master/cloud.go) 约束的云服务提供商。为了兼容旧版本，Kubernetes 核心项目中提供的 [cloud-controller-manager](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager) 使用和 `kube-controller-manager` 相同的云服务类库。已经在 Kubernetes 核心项目中支持的云服务提供商预计将通过使用 in-tree 的 cloud-controller-manager 过渡到 Kubernetes 核心之外。在将来的 Kubernetes 发布中，所有的云管理控制器将在 Kubernetes 核心项目之外，由 sig 领导者或者云服务厂商进行开发。

-   [管理](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/running-cloud-controller/#%e7%ae%a1%e7%90%86)
-   [示例](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/running-cloud-controller/#%e7%a4%ba%e4%be%8b)
-   [限制](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/running-cloud-controller/#%e9%99%90%e5%88%b6)
-   [开发自己的云管理控制器](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/running-cloud-controller/#%e5%bc%80%e5%8f%91%e8%87%aa%e5%b7%b1%e7%9a%84%e4%ba%91%e7%ae%a1%e7%90%86%e6%8e%a7%e5%88%b6%e5%99%a8)

### 管理

#### 需求

每个云服务都有一套各自的需求用于系统平台的集成，这不应与运行 `kube-controller-manager` 的需求有太大差异。作为经验法则，你需要：

-   云服务认证 / 授权：您的云服务可能需要使用令牌或者 IAM 规则以允许对其 API 的访问
-   kubernetes 认证 / 授权：cloud-controller-manager 可能需要 RBAC 规则以访问 kubernetes apiserver
-   高可用：类似于 kube-controller-manager，您可能希望通过主节点选举（默认开启）配置一个高可用的云管理控制器。

#### 运行云管理控制器

您需要对集群配置做适当的修改以成功地运行云管理控制器：

-   一定不要为 `kube-apiserver` 和 `kube-controller-manager` 指定 `--cloud-provider`标志。这将保证它们不会运行任何云服务专用循环逻辑，这将会由云管理控制器运行。未来这个标记将被废弃并去除。
-   `kubelet` 必须使用 `--cloud-provider=external` 运行。这是为了保证让 kubelet 知道在执行任何任务前，它必须被云管理控制器初始化。

请记住，设置群集使用云管理控制器将用多种方式更改群集行为：

-   指定了 `--cloud-provider=external` 的 kubelet 将被添加一个 `node.cloudprovider.kubernetes.io/uninitialized` 的污点，导致其在初始化过程中不可调度（`NoSchedule`）。这将标记该节点在能够正常调度前，需要外部的控制器进行二次初始化。请注意，如果云管理控制器不可用，集群中的新节点会一直处于不可调度的状态。这个污点很重要，因为调度器可能需要关于节点的云服务特定的信息，比如他们的区域或类型（high cpu, gpu, high memory, spot instance 等）。
-   集群中节点的云服务信息将不再能够从本地元数据中获取，取而代之的是所有获取节点信息的 API 调用都将通过云管理控制器。这意味着你可以通过限制到 kubelet 云服务 API 的访问来提升安全性。在更大的集群中您可能需要考虑云管理控制器是否会遇到速率限制，因为它现在负责集群中几乎所有到云服务的 API 调用。

对于 v1.8 版本，云管理控制器可以实现：

-   node 控制器 - 负责使用云服务 API 更新 kubernetes 节点并删除在云服务上已经删除的 kubernetes 节点。
-   service 控制器 - 负责在云服务上为类型为 LoadBalancer 的 service 提供负载均衡器。
-   route 控制器 - 负责在云服务上配置网络路由。
-   如果您使用的是 out-of-tree 提供商，请按需实现其余任意特性。

### 示例

如果当前 Kubernetes 内核支持您使用的云服务，并且想要采用云管理控制器，请参见 [kubernetes 内核中的云管理控制器](https://github.com/kubernetes/kubernetes/tree/master/cmd/cloud-controller-manager)。

对于不在 Kubernetes 内核中的云管理控制器，您可以在云服务厂商或 sig 领导者的源中找到对应的项目。

-   [DigitalOcean](https://github.com/digitalocean/digitalocean-cloud-controller-manager)
-   [keepalived](https://github.com/munnerz/keepalived-cloud-provider)
-   [Oracle Cloud Infrastructure](https://github.com/oracle/oci-cloud-controller-manager)
-   [Rancher](https://github.com/rancher/rancher-cloud-controller-manager)

对于已经存在于 Kubernetes 内核中的提供商，您可以在集群中将 in-tree 云管理控制器作为守护进程运行。请使用如下指南：

[`admin/cloud/ccm-example.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/cloud/ccm-example.yaml)

```yaml
# This is an example of how to setup cloud-controller-manger as a Daemonset in your cluster.
# It assumes that your masters can run pods and has the role node-role.kubernetes.io/master
# Note that this Daemonset will not work straight out of the box for your cloud, this is
# meant to be a guideline.

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cloud-controller-manager
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: system:cloud-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: cloud-controller-manager
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    k8s-app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cloud-controller-manager
  template:
    metadata:
      labels:
        k8s-app: cloud-controller-manager
    spec:
      serviceAccountName: cloud-controller-manager
      containers:
      - name: cloud-controller-manager
        # for in-tree providers we use k8s.gcr.io/cloud-controller-manager
        # this can be replaced with any other image for out-of-tree providers
        image: k8s.gcr.io/cloud-controller-manager:v1.8.0
        command:
        - /usr/local/bin/cloud-controller-manager
        - --cloud-provider=<YOUR_CLOUD_PROVIDER>   # Add your own cloud provider here!
        - --leader-elect=true
        - --use-service-account-credentials
        # these flags will vary for every cloud provider
        - --allocate-node-cidrs=true
        - --configure-cloud-routes=true
        - --cluster-cidr=172.17.0.0/16
      tolerations:
      # this is required so CCM can bootstrap itself
      - key: node.cloudprovider.kubernetes.io/uninitialized
        value: "true"
        effect: NoSchedule
      # this is to have the daemonset runnable on master nodes
      # the taint may vary depending on your cluster setup
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      # this is to restrict CCM to only run on master nodes
      # the node selector may vary depending on your cluster setup
      nodeSelector:
        node-role.kubernetes.io/master: ""
```

### 限制

运行云管理控制器会有一些可能的限制。虽然以后的版本将处理这些限制，但是知道这些生产负载的限制很重要。

#### 对 Volume 的支持

云管理控制器未实现 `kube-controller-manager` 中的任何 volume 控制器，因为和 volume 的集成还需要与 kubelet 协作。由于我们引入了 CSI (容器存储接口，container storage interface) 并对弹性 volume 插件添加了更强大的支持，云管理控制器将添加必要的支持，以使云服务同 volume 更好的集成。请在 [这里](https://github.com/kubernetes/features/issues/178) 了解更多关于 out-of-tree CSI volume 插件的信息。

#### 可扩展性

在以前为云服务提供商提供的架构中，我们依赖 kubelet 的本地元数据服务来获取关于它本身的节点信息。通过这个新的架构，现在我们完全依赖云管理控制器来获取所有节点的信息。对于非常大的集群，您需要考虑可能的瓶颈，例如资源需求和 API 速率限制。

#### 鸡和蛋的问题

云管理控制器的目标是将云服务特性的开发从 Kubernetes 核心项目中解耦。不幸的是，Kubernetes 项目的许多方面都假设云服务提供商的特性同项目紧密结合。因此，这种新架构的采用可能导致某些场景下，当一个请求需要从云服务提供商获取信息时，在该请求没有完成的情况下云管理控制器不能返回那些信息。

Kubelet 中的 TLS 引导特性是一个很好的例子。目前，TLS 引导认为 Kubelet 有能力从云提供商（或本地元数据服务）获取所有的地址类型（私有、公用等），但在被初始化之前，云管理控制器不能设置节点地址类型，而这需要 kubelet 拥有 TLS 证书以和 apiserver 通信。

随着这种措施的演进，将来的发行版中将作出改变来解决这些问题。

### 开发自己的云管理控制器

要构建和开发您自己的云管理控制器，请阅读 [开发云管理控制器](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/developing-cloud-controller-manager.md) 文档。







## 限制存储消耗

此示例演示了一种限制命名空间中存储使用量的简便方法。

演示中用到了以下资源：[ResourceQuota](https://v1-14.docs.kubernetes.io/docs/concepts/policy/resource-quotas/)，[LimitRange](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/) 和 [PersistentVolumeClaim](https://v1-14.docs.kubernetes.io/docs/concepts/storage/persistent-volumes/)。

### 准备工作

一个集群。

### 场景：限制存储消耗

集群管理员代表用户群操作集群，管理员希望控制单个命名空间可以消耗多少存储空间以控制成本。

管理员想要限制：

1.  命名空间中持久卷申领（persistent volume claims）的数量
2.  每个申领（claim）可以请求的存储量
3.  命名空间可以具有的累计存储量

### 使用 LimitRange 限制存储请求

将 `LimitRange` 添加到命名空间会为存储请求大小强制设置最小值和最大值。存储是通过 `PersistentVolumeClaim` 来发起请求的。执行限制范围控制的准入控制器会拒绝任何高于或低于管理员所设阈值的 PVC。

在此示例中，请求 10Gi 存储的 PVC 将被拒绝，因为它超过了最大 2Gi。

```
apiVersion: v1
kind: LimitRange
metadata:
  name: storagelimits
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 2Gi
    min:
      storage: 1Gi
```

当底层存储提供程序需要某些最小值时，将会用到所设置最小存储请求值。例如，AWS EBS volumes 的最低要求为 1Gi。

### 使用 StorageQuota 限制 PVC 数目和累计存储容量

管理员可以限制某个命名空间中的 PVCs 个数以及这些 PVCs 的累计容量。新 PVCs 请求如果超过任一上限值将被拒绝。

在此示例中，命名空间中的第 6 个 PVC 将被拒绝，因为它超过了最大计数 5。或者，当与上面的 2Gi 最大容量限制结合在一起时，意味着 5Gi 的最大配额不能支持 3 个都是 2Gi 的 PVC。后者实际上是向命名空间请求 6Gi 容量，而该命令空间已经设置上限为 5Gi。

```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storagequota
spec:
  hard:
    persistentvolumeclaims: "5"
    requests.storage: "5Gi"
```

### 小结

限制范围对象可以用来设置可请求的存储量上限，而资源配额对象则可以通过申领计数和累计存储容量有效地限制命名空间耗用的存储量。这两种机制使得集群管理员能够规划其集群存储预算而不会发生任一项目超量分配的风险。





## 命名空间攻略

kubernetes命名空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。有助于不同的项目、团队或客户去共享 Kubernetes 集群。

名字空间通过以下方式实现这点：

1.  为[名字](https://v1-14.docs.kubernetes.io/docs/concepts/overview/working-with-objects/names/)设置作用域.
2.  为集群中的部分资源关联鉴权和策略的机制。

使用多个命名空间是可选的。

此示例演示了如何使用 Kubernetes 命名空间细分群集。

### 准备工作

一个集群。

### 环境准备

此示例作如下假设：

1.  您已拥有一个 [配置好的 Kubernetes 集群](https://v1-14.docs.kubernetes.io/docs/setup/)。
2.  您已对 Kubernetes 的 *Pods*, *Services* 和 *Deployments* 有基本理解。

1.  理解默认命名空间

默认情况下，Kubernetes 集群会在配置集群时实例化一个默认命名空间，用以存放集群所使用的默认 Pods、Services 和 Deployments 集合。

假设您有一个新的集群，您可以通过执行以下操作来检查可用的命名空间：

```shell
kubectl get namespaces
NAME      STATUS    AGE
default   Active    13m
```

### 创建新的命名空间

在本练习中，我们将创建两个额外的 Kubernetes 命名空间来保存我们的内容。

我们假设一个场景，某组织正在使用共享的 Kubernetes 集群来支持开发和生产：

开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pods、Services 和 Deployments 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。

运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，对谁可以或谁不可以操作运行生产站点的 Pods、Services 和 Deployments 集合进行控制。

该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个命名空间：development 和 production。

让我们创建两个新的命名空间来保存我们的工作。

文件 [`namespace-dev.json`](https://v1-14.docs.kubernetes.io/examples/admin/namespace-dev.json) 描述了 development 命名空间:

[`admin/namespace-dev.json`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/namespace-dev.json)

```json
{
  "kind": "Namespace",
  "apiVersion": "v1",
  "metadata": {
    "name": "development",
    "labels": {
      "name": "development"
    }
  }
}
```

使用 kubectl 创建 development 命名空间。

```shell
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
```

将下列的内容保存到文件 [`namespace-prod.json`](https://v1-14.docs.kubernetes.io/examples/admin/namespace-prod.json) 中，这些内容是对 production 命名空间的描述：

[`admin/namespace-prod.json` ](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/namespace-prod.json)

```json
{
  "kind": "Namespace",
  "apiVersion": "v1",
  "metadata": {
    "name": "production",
    "labels": {
      "name": "production"
    }
  }
}
```

让我们使用 kubectl 创建 production 命名空间。

```shell
kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
```

为了确保一切正常，我们列出集群中的所有命名空间。

```shell
kubectl get namespaces --show-labels
NAME          STATUS    AGE       LABELS
default       Active    32m       <none>
development   Active    29s       name=development
production    Active    23s       name=production
```

### 在每个命名空间中创建 pod

Kubernetes 命名空间为集群中的 Pods、Services 和 Deployments 提供了作用域。

与一个命名空间交互的用户不会看到另一个命名空间中的内容。

为了演示这一点，让我们在 development 命名空间中启动一个简单的 Deployment 和 Pod。

我们首先检查一下当前的上下文：

```shell
kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://130.211.122.180
  name: lithe-cocoa-92103_kubernetes
contexts:
- context:
    cluster: lithe-cocoa-92103_kubernetes
    user: lithe-cocoa-92103_kubernetes
  name: lithe-cocoa-92103_kubernetes
current-context: lithe-cocoa-92103_kubernetes
kind: Config
preferences: {}
users:
- name: lithe-cocoa-92103_kubernetes
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: 65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b
- name: lithe-cocoa-92103_kubernetes-basic-auth
  user:
    password: h5M0FtUUIflBSdI7
    username: admin
kubectl config current-context
lithe-cocoa-92103_kubernetes
```

下一步是为 kubectl 客户端定义一个上下文，以便在每个命名空间中工作。”cluster” 和 “user” 字段的值将从当前上下文中复制。

```shell
kubectl config set-context dev --namespace=development \
  --cluster=lithe-cocoa-92103_kubernetes \
  --user=lithe-cocoa-92103_kubernetes

kubectl config set-context prod --namespace=production \
  --cluster=lithe-cocoa-92103_kubernetes \
  --user=lithe-cocoa-92103_kubernetes
```

默认地，上述命令会添加两个上下文到 `.kube/config` 文件中。 您现在可以查看上下文并根据您希望使用的命名空间并在这两个新的请求上下文之间切换。

查看新的上下文：

```shell
kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://130.211.122.180
  name: lithe-cocoa-92103_kubernetes
contexts:
- context:
    cluster: lithe-cocoa-92103_kubernetes
    user: lithe-cocoa-92103_kubernetes
  name: lithe-cocoa-92103_kubernetes
- context:
    cluster: lithe-cocoa-92103_kubernetes
    namespace: development
    user: lithe-cocoa-92103_kubernetes
  name: dev
- context:
    cluster: lithe-cocoa-92103_kubernetes
    namespace: production
    user: lithe-cocoa-92103_kubernetes
  name: prod
current-context: lithe-cocoa-92103_kubernetes
kind: Config
preferences: {}
users:
- name: lithe-cocoa-92103_kubernetes
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: 65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b
- name: lithe-cocoa-92103_kubernetes-basic-auth
  user:
    password: h5M0FtUUIflBSdI7
    username: admin
```

让我们切换到 development 命名空间进行操作。

```shell
kubectl config use-context dev
```

您可以使用下列命令验证当前上下文：

```shell
kubectl config current-context
dev
```

此时，我们从命令行向 Kubernetes 集群发出的所有请求都限定在 development 命名空间中。

让我们创建一些内容。

```shell
kubectl run snowflake --image=kubernetes/serve_hostname --replicas=2
```

我们刚刚创建了一个副本大小为 2 的 deployment，该 deployment 运行名为 snowflake 的 pod，其中包含一个仅提供主机名服务的基本容器。请注意，`kubectl run` 仅在 Kubernetes 集群版本 >= v1.2 时创建 deployment。如果您运行在旧版本上，则会创建 replication controller。如果期望执行旧版本的行为，请使用 `--generator=run/v1` 创建 replication controller。 参见 [`kubectl run`](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#run) 获取更多细节。

```shell
kubectl get deployment
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
snowflake   2         2         2            2           2m
kubectl get pods -l run=snowflake
NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
```

这很棒，开发人员可以做他们想要的事情，而不必担心影响 production 命名空间中的内容。

让我们切换到 production 命名空间，展示一个命名空间中的资源如何对另一个命名空间不可见。

```shell
kubectl config use-context prod
```

`production` 命名空间应该是空的，下列命令应该返回的内容为空。

```shell
kubectl get deployment
kubectl get pods
```

生产环境需要运行 cattle，让我们创建一些名为 cattle 的 pods。

```shell
kubectl run cattle --image=kubernetes/serve_hostname --replicas=5

kubectl get deployment
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
cattle    5         5         5            5           10s

kubectl get pods -l run=cattle
NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
```

此时，应该很清楚的展示了用户在一个命名空间中创建的资源对另一个命名空间是不可见的。

随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个命名空间提供不同的授权规则。





## 为Kubernetes运行etcd集群

etcd是一个一致的、高可用的键值存储，用作Kubernetes对所有集群数据的备份存储。

如果您的Kubernetes集群使用etcd作为其备份存储，请确保您对这些数据有一个 [备份](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster) 计划。

您可以在官方 [文档](https://etcd.io/docs/) 中找到关于etcd的详细信息。

### 准备工作

一个集群

### 先决条件

-   以奇数成员？(odd members)集群的形式运行etcd。
-   etcd是一个基于领导(leader-based)的分布式系统。确保领导定期向所有追随者发送心跳，以保持集群稳定。
-   确保不会发生资源匮乏。

集群的性能和稳定性对网络和磁盘IO非常敏感。任何资源不足都可能导致心跳超时，导致集群不稳定。不稳定的etcd表示没有选出领导。在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的pod。

-   保持稳定的etcd集群对Kubernetes集群的稳定性至关重要。因此，在专用机器或隔离环境上运行etcd集群，来[保证资源需求(guaranteed resource requirements)](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/hardware.md#hardware-recommendations)。
-   在生产环境中推荐运行的etcd版本是 `3.2.10+`。

### 资源需求

在资源有限的情况下运行etcd只适合用于测试。要在生产环境中部署，需要高级硬件配置。在生产环境中部署etcd之前，请参阅[资源需求参考文档](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/hardware.md#example-hardware-configurations)。

### 运行etcd 集群

本节介绍如何启动单节点和多节点etcd集群。

#### 单节点 etcd 集群

只在测试中使用单节点etcd集群。

1.  运行：

    ```sh
    ./etcd --listen-client-urls=http://$PRIVATE_IP:2379 --advertise-client-urls=http://$PRIVATE_IP:2379
    ```

2.  使用 `--etcd-servers=$PRIVATE_IP:2379` 标志启动Kubernetes API server，将 `PRIVATE_IP` 替换为你的etcd客户端IP。

#### 多节点 etcd 集群

为了持久性和高可用性，在生产环境中将etcd作为一个多节点集群运行，并定期对其进行备份。在生产中建议使用5个成员的集群。有关更多信息，请参见 [FAQ 文档](https://github.com/coreos/etcd/blob/master/Documentation/faq.md#what-is-failure-tolerance)。

可以通过静态成员信息或动态发现的方式配置 etcd 集群。有关集群的详细信息，请参阅 [etcd 集群文档](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md)。

例如，考虑运行以下客户端 URL 的五个成员的 etcd 集群：`http://$IP1:2379`, `http://$IP2:2379`, `http://$IP3:2379`, `http://$IP4:2379`, and `http://$IP5:2379`. 要启动 Kubernetes API 服务器：

1.  运行以下命令：

    ```sh
    ./etcd --listen-client-urls=http://$IP1:2379, http://$IP2:2379, http://$IP3:2379, http://$IP4:2379, http://$IP5:2379 --advertise-client-urls=http://$IP1:2379, http://$IP2:2379, http://$IP3:2379, http://$IP4:2379, http://$IP5:2379
    ```

2.  使用 `--etcd-servers=$IP1:2379, $IP2:2379, $IP3:2379, $IP4:2379, $IP5:2379` 标志来启动kubernetes API server，

    将 `IP` 替换为你的客户端IP地址。

#### 使用负载均衡的多节点 etcd 集群

要运行负载均衡的 etcd 集群：

1.  建立一个 etcd 集群。
2.  在 etcd 集群前面配置负载均衡器。例如，让负载均衡器的地址为 `$LB`。
3.  使用标志 `--etcd-servers=$LB:2379` 启动 Kubernetes API 服务器。

### 安全的 etcd 集群

对 etcd 的访问相当于集群中的 root 权限，因此理想情况下只有 API 服务器才能访问它。考虑到数据的敏感性，建议只向需要访问 etcd 集群的节点授予权限。

想要确保 etcd 的安全，可以设置防火墙规则或使用 etcd 提供的安全特性，这些安全特性依赖于 x509 公钥基础设施（PKI）。首先，通过生成密钥和证书对来建立安全的通信通道。例如，使用密钥对 `peer.key` 和 `peer.cert` 来保护 etcd 成员之间的通信，而 `client.cert`和 `client.cert` 用于保护 etcd 与其客户端之间的通信。请参阅 etcd 项目提供的 [示例脚本](https://github.com/coreos/etcd/tree/master/hack/tls-setup)，以生成用于客户端身份验证的密钥对和 CA 文件。

#### 安全通信

若要使用安全对等通信对 etcd 进行配置，请指定标志 `--peer-key-file=peer.key` 和 `--peer-cert-file=peer.cert`，并使用 https 作为 URL 模式。

类似地，要使用安全客户端通信对 etcd 进行配置，请指定标志 `--key-file=k8sclient.key`和 `--cert-file=k8sclient.cert`，并使用 https 作为 URL 模式。

#### 限制 etcd 集群的访问

配置安全通信后，将 etcd 集群的访问限制在 Kubernetes API 服务器上。使用 TLS 身份验证来完成此任务。

例如，考虑由 CA `etcd.ca` 信任的密钥对 `k8sclient.key` 和 `k8sclient.cert`。当 etcd 配置为 `--client-cert-auth` 和 TLS 时，它使用系统 CA 或由 `--trusted-ca-file` 标志传入的 CA 验证来自客户端的证书。指定标志 `--client-cert-auth=true` 和 `--trusted-ca-file=etcd.ca` 将限制对具有证书 `k8sclient.cert` 的客户端的访问。

一旦正确配置了 etcd，只有具有有效证书的客户端才能访问它。要让 Kubernetes API 服务器访问，可以使用标志 `--etcd-certfile=k8sclient.cert` 和 `--etcd-keyfile=k8sclient.key` 配置它。

**注意**：Kubernetes 目前不支持 etcd 身份验证。想要了解更多信息，请参阅相关的问题 [支持 etcd v2 的基本认证](https://github.com/kubernetes/kubernetes/issues/23398)。

### 替换失败的 etcd 成员

etcd 集群通过容忍少数成员故障实现高可用性。但是，要改善集群的整体健康状况，请立即替换失败的成员。当多个成员失败时，逐个替换它们。替换失败成员需要两个步骤：删除失败成员和添加新成员。

虽然 etcd 在内部保留唯一的成员 ID，但建议为每个成员使用唯一的名称，以避免人为错误。例如，考虑一个三成员的 etcd 集群。让 URL 为：member1=http://10.0.0.1， member2=http://10.0.0.2 和 member3=http://10.0.0.3。当 member1 失败时，将其替换为 member4=http://10.0.0.4。

1.  获取失败的 member1 的成员 ID：

    `etcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list`

    显示以下信息：

    ```
     8211f1d0f64f3269, started, member1, http://10.0.0.1:12380, http://10.0.0.1:2379
     91bc3c398fb3c146, started, member2, http://10.0.0.1:2380, http://10.0.0.2:2379
     fd422379fda50e48, started, member3, http://10.0.0.1:2380, http://10.0.0.3:2379
    ```

2.  移除失败的成员

    `etcdctl member remove 8211f1d0f64f3269`

    显示以下信息：

    ```
    Removed member 8211f1d0f64f3269 from cluster
    ```

3.  增加新成员

    `./etcdctl member add member4 --peer-urls=http://10.0.0.4:2380`

    显示以下信息：

    ```
    Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
    ```

4.  在 IP 为 `10.0.0.4` 的机器上启动新增加的成员：

    ```
     export ETCD_NAME="member4"
     export ETCD_INITIAL_CLUSTER="member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380"
     export ETCD_INITIAL_CLUSTER_STATE=existing
     etcd [flags]
    ```

5.  做以下事情之一：

    1.  更新其 `--etcd-servers` 标志，使 Kubernetes 知道配置进行了更改，然后重新启动 Kubernetes API 服务器。
    2.  如果在部署中使用了负载均衡，更新负载均衡配置。

有关集群重新配置的详细信息，请参阅 [etcd 重构文档](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#remove-a-member)。

### 备份 etcd 集群

所有 Kubernetes 对象都存储在 etcd 上。定期备份 etcd 集群数据对于在灾难场景（例如丢失所有主节点）下恢复 Kubernetes 集群非常重要。快照文件包含所有 Kubernetes 状态和关键信息。为了保证敏感的 Kubernetes 数据的安全，可以对快照文件进行加密。

备份 etcd 集群可以通过两种方式完成：etcd 内置快照和卷快照。

#### 内置快照

etcd 支持内置快照，因此备份 etcd 集群很容易。快照可以从使用 `etcdctl snapshot save`命令的活动成员中获取，也可以通过从 etcd [数据目录](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/configuration.md#--data-dir) 复制 `member/snap/db` 文件，该 etcd 数据目录目前没有被 etcd 进程使用。`datadir` 位于 `$DATA_DIR/member/snap/db`。获取快照通常不会影响成员的性能。

下面是一个示例，用于获取 `$ENDPOINT` 所提供的键空间的快照到文件 `snapshotdb`：

```
ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshotdb
# exit 0

# 验证快照
ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+
```

#### 卷快照

如果 etcd 运行在支持备份的存储卷（如 Amazon 弹性块存储）上，则可以通过获取存储卷的快照来备份 etcd 数据。

### 扩大 etcd 集群

通过交换性能，扩展 etcd 集群可以提高可用性。缩放不会提高集群性能和能力。一般情况下不要扩大或缩小 etcd 集群的集合。不要为 etcd 集群配置任何自动缩放组。强烈建议始终在任何官方支持的规模上运行生产 Kubernetes 集群时使用静态的五成员 etcd 集群。

合理的扩展是在需要更高可靠性的情况下，将三成员集群升级为五成员集群。请参阅 [etcd 重新配置文档](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#remove-a-member) 以了解如何将成员添加到现有集群中的信息。

### 恢复 etcd 集群

etcd 支持从 [major.minor](http://semver.org/) 或其他不同 patch 版本的 etcd 进程中获取的快照进行恢复。还原操作用于恢复失败的集群的数据。

在启动还原操作之前，必须有一个快照文件。它可以是来自以前备份操作的快照文件，也可以是来自剩余 [数据目录](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/configuration.md#--data-dir) 的快照文件。`datadir` 位于 `$DATA_DIR/member/snap/db`。有关从快照文件还原集群的详细信息和示例，请参阅 [etcd 灾难恢复文档](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md#restoring-a-cluster)。

如果还原的集群的访问 URL 与前一个集群不同，则必须相应地重新配置 Kubernetes API 服务器。在本例中，使用标志 `--etcd-servers=$NEW_ETCD_CLUSTER` 而不是标志 `--etcd-servers=$OLD_ETCD_CLUSTER` 重新启动 Kubernetes API 服务器。用相应的 IP 地址替换 `$NEW_ETCD_CLUSTER` 和 `$OLD_ETCD_CLUSTER`。如果在 etcd 集群前面使用负载平衡，则可能需要更新负载均衡器。

如果大多数 etcd 成员永久失败，则认为 etcd 集群失败。在这种情况下，Kubernetes 不能对其当前状态进行任何更改。虽然已调度的 pod 可能继续运行，但新的 pod 无法调度。在这种情况下，恢复 etcd 集群并可能需要重新配置 Kubernetes API 服务器以修复问题。

### 升级和回滚 etcd 集群

从Kubernetes v1.13.0开始，etcd2不再支持作为新的或现有Kubernetes集群的存储后端。Kubernetes支持etcd2和etcd3的时间表如下:

-   Kubernetes v1.0: 只支持etcd2
-   Kubernetes v1.5.1: 添加对etcd3的支持，新集群依然默认使用etcd2
-   Kubernetes v1.6.0: 使用 `kube-up.sh` 创建的新集群默认使用etcd3，且 `kube-apiserver` 默认使用etcd3
-   Kubernetes v1.9.0: 宣布弃用etcd2存储后端
-   Kubernetes v1.13.0: 删除了etcd2存储后端， `kube-apiserver` 将拒绝使用 `--storage-backend=etcd2` 来启动，并输出信息 `etcd2 is no longer a supported storage backend`

在使用 `--storage-backend=etcd2` 将v1.12.x的kube-apiserver更新到 v1.13.x之前，etcd v2 的数据**必须**迁移到v3存储后端，并且kube-apiserver调用变为使用 `--storage-backend=etcd3`。

从etcd2迁移到etcd3的过程高度依赖于etcd集群的部署和配置方式，以及Kubernetes集群的部署和配置方式。我们建议您参考集群提供者的文档，看看是否有预定义的解决方案。

如果你的集群使用 `kube-up.sh` 来创建并且仍在使用etcd2作为存储后端，请参考 [Kubernetes v1.12 etcd 集群更新文档](https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#upgrading-and-rolling-back-etcd-clusters)。





## 重新配置活动集群中节点的Kubelet

**FEATURE STATE:** `Kubernetes v1.11` [beta](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#)

[动态kubelet配置](https://github.com/kubernetes/enhancements/issues/281) 允许通过部署ConfigMap来改变活动中的kubernetes集群中每个kubelet的配置并为每个节点配置使用。

>   **警告：**所有的 Kubelet 配置参数都可以动态修改，但一些参数的更改不太安全。在决定动态修改参数之前，你需要深刻理解这个改变会怎样影响集群的行为。一定要在一小组节点上仔细测试配置更改。关于配置特定字段的建议可以在内嵌的 `KubeletConfiguration` [类型文档](https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go)中找到。

### 准备工作

-   master和节点的版本都在Kubernetes v1.11 或以上
-   kubectl v1.11 或更高，设置好可以与集群通信
-   Kubelet的 `--dynamic-config-dir` 标志必须为节点上的可写目录

### 重新配置集群活动节点上的 Kubelet

#### 基本工作流程概览

配置活动集群中的 kubelet 的基本工作流程如下：

1.  编写一个包含 Kubelet 配置的 YAML 或 JSON 格式的配置文件。
2.  用 ConfigMap 包装该文件并保存至 Kubernetes 控制平面。
3.  更新 Kubelet 对应的节点对象以使用此 ConfigMap。

每个 Kubelet 都会监控各自节点对象上的配置引用。当这个引用发生改变时，Kubelet 将下载新的配置并退出。为了使这个功能正常工作，您必须运行一个进程管理器（例如 systemd），以便在 Kubelet 退出时自动对其进行重启。当 Kubelet 重新启动后，它将开始使用新的配置。

新配置将会完全覆盖 `--config` 提供的配置，并由命令行标志覆盖。新配置中未指定的值将接收与配置的版本相适应的默认值(例如， `kubelet.config.k8s.io/v1beta1`)，除非被标志覆盖。

节点的Kubelet配置状态通过 `Node.Spec.Status.Config` 报告。一旦您更新了一个节点以使用新的ConfigMap，您就可以观察这个状态来确认该节点正在使用预期的配置。

本文档使用 `kubectl edit` 来描述编辑中的节点。还有其他方法可以修改节点的规范，比如 `kubectl patch` ，它可以促进脚本化的工作流。

本文仅仅介绍了单个节点使用各种 ConfigMap 的场景。这也适用于多个节点使用相同 ConfigMap 的场景。

>   **警告：**当 *能够* 通过就地(in-place)更新ConfigMap改变配置，但是这将导致所有配置了ConfigMap的kubelet同时更新。按照惯例将ConfigMaps做为不可变的要安全得多，这得益于 `kubectl` 的 `--append-hash` 选项，并逐步将更新滚动到 `Node.Spec.ConfigSource` 。

#### 节点授权器自动配置RBAC规则

之前，您需要手动创建RBAC规则，以允许节点访问它们分配的ConfigMap。现在，节点授权器自动配置这些规则。

#### 生成一个包含当前配置的文件

动态 Kubelet 配置功能允许您对整个配置对象进行覆盖，而不只是单个字段的替换。这种更为简单的模式使得追踪配置值的来源及调试问题变得更加容易。但是，随之而来的问题是您必须知道现存的配置，以确保只对需要的配置字段进行了修改。

将来，Kubelet 将使用磁盘上的文件进行引导（请参考 [通过配置文件设置 Kubelet 参数](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file)），您可以简单的通过编辑这个文件的副本（作为最佳实践，应该对其进行版本控制）来创建第一个 Kubelet ConfigMap。目前，Kubelet是用**这个文件和命令行标志**的组合来启动的，可以覆盖文件中的配置。作为一种解决方案，您可以通过kubectl代理访问Kubelet服务器的 `configz` 端点来生成包含节点当前配置的配置文件。在其当前实现中，此端点仅用于作为调试辅助。对于生产场景，不要依赖于此端点的行为。下面的示例使用 `jq` 命令简化JSON的工作。要按照下面的任务执行，您需要安装 `jq` ，但是如果您喜欢手动提取 `kubeletconfig` 子对象，则可以调整这些任务。

##### 生成配置文件

1.  选择一个需要重新配置的节点。我们将使用  `NODE_NAME` 指代这个节点的名字。

2.  使用 `bash kubectl proxy --port=8001 &` 命令在后台启动 kubectl proxy。

3.  运行下列命令以从  `configz` 端点下载并解包配置。命令很长，所以复制粘贴时要小心。**如果你使用zsh**，请注意，常见的zsh配置会添加反斜杠，以转义URL中变量名称前后的花括号。例如: `${NODE_NAME}` 将在粘贴时重写为 `$\{NODE_NAME\}` 。在运行命令之前，必须删除反斜杠，否则命令将失败。

    ```bash
      NODE_NAME="the-name-of-the-node-you-are-reconfiguring"; curl -sSL "http://localhost:8001/api/v1/nodes/${NODE_NAME}/proxy/configz" | jq '.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"' > kubelet_configz_${NODE_NAME}
    ```

>   **注意：** 你需要手动为下载的对象添加 `kind` 和 `apiVersion` ，因为 `configz` 端点没有报告他们。

##### 编辑配置文件

使用一个文本编辑器，更改前面过程生成的文件中的一个参数。例如，您可以编辑QPS参数 `eventRecordQPS` 。

##### 将配置文件推送到控制面板

用以下命令将编辑好的配置文件推送到控制面板：

```bash
kubectl -n kube-system create configmap my-node-config --from-file=kubelet=kubelet_configz_${NODE_NAME} --append-hash -o yaml
```

下面是一个有效回复的例子:

```none
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: 2017-09-14T20:23:33Z
  name: my-node-config-gkt4c2m4b2
  namespace: kube-system
  resourceVersion: "119980"
  selfLink: /api/v1/namespaces/kube-system/configmaps/my-node-config-gkt4c2m4b2
  uid: 946d785e-998a-11e7-a8dd-42010a800006
data:
  kubelet: |
    {...}
```

ConfigMap是在 `kube-system` 命名空间中创建的，因为这个ConfigMap配置了一个Kubelet，它是Kubernetes系统组件。

 `--append-hash` 选项将ConfigMap内容的"短校验和"追加到名称。这对于编辑然后推送工作流非常方便，因为它自动地、但又确定地为新的ConfigMaps生成新名称。在下面的示例中，包含生成的散列的名称称为 `CONFIG_MAP_NAME` 。

##### 设置节点使用新的配置

使用下列命令编辑节点的引用，使其指向新的 ConfigMap。

```bash
kubectl edit node ${NODE_NAME}
```

使用一个编辑器，在 `spec` 下添加如下 YAML：

```yaml
configSource:
    configMap:
        name: CONFIG_MAP_NAME
        namespace: kube-system
        kubeletConfigKey: kubelet
```

您必须全部指定 `name`, `namespace`, and `kubeletConfigKey`。 `kubeletConfigKey`.参数显示了ConfigMap的哪个键包含它的配置。

##### 观察节点是否开始使用新配置

使用 `kubectl get node ${NODE_NAME} -o yaml` 命令来检索节点并检查 `Node.Status.Config`。 状态中报告与`active`, `assigned`, 和 `lastKnownGood` 配置对应的配置源。

-    `active` 配置是Kubelet当前运行的版本。
-    `assigned` 配置是Kubelet基于 `Node.Spec.ConfigSource` 解析的最新版本。
-    `lastKnownGood` 是在 `Node.Spec.ConfigSource` 中分配了一个无效的配置时Kubelet将返回的版本。

如果将 `lastKnownGood` 配置设置为默认值(与节点一起部署的本地配置)，则可能不存在 `lastKnownGood` 配置。在Kubelet适应配置之后，状态将更新 `lastKnownGood` 以匹配有效的 `assigned` 配置。Kubelet如何确定哪个配置应该成为 `lastKnownGood` 配置的细节并没有得到API的保证，但是目前实现为10分钟的宽限期。

您可以使用以下命令(使用 `jq`)过滤到配置状态:

```bash
kubectl get no ${NODE_NAME} -o json | jq '.status.config'
```

一个响应示例：

```json
{
  "active": {
    "configMap": {
      "kubeletConfigKey": "kubelet",
      "name": "my-node-config-9mbkccg2cc",
      "namespace": "kube-system",
      "resourceVersion": "1326",
      "uid": "705ab4f5-6393-11e8-b7cc-42010a800002"
    }
  },
  "assigned": {
    "configMap": {
      "kubeletConfigKey": "kubelet",
      "name": "my-node-config-9mbkccg2cc",
      "namespace": "kube-system",
      "resourceVersion": "1326",
      "uid": "705ab4f5-6393-11e8-b7cc-42010a800002"
    }
  },
  "lastKnownGood": {
    "configMap": {
      "kubeletConfigKey": "kubelet",
      "name": "my-node-config-9mbkccg2cc",
      "namespace": "kube-system",
      "resourceVersion": "1326",
      "uid": "705ab4f5-6393-11e8-b7cc-42010a800002"
    }
  }
}
```

如果发生错误，Kubelet将在 `Node.Status.Config.Error` 结构中报告它。在[理解Node.Status.Config.Error 信息](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#understanding-node-status-config-error-messages).中列出了可能的错误。你可以在Kubelet日志中搜索相同的文本，以获得关于错误的更多细节和上下文。

##### 更多更改

按照上面的工作流程进行更多的更改，然后再次推送它们。每次使用新内容推送ConfigMap时，-append-hash kubectl选项都会创建一个具有新名称的ConfigMap。最安全的滚动策略是首先创建一个新的ConfigMap，然后更新节点以使用新的ConfigMap。

##### 重置节点以使用其本地默认配置

要重置节点以使用它提供的配置，请使用 `kubectl edit node ${NODE_NAME}` 编辑节点并删除 `Node.Spec.ConfigSource` 字段。

##### 观察节点是否使用其本地缺省配置

删除此子字段后，`Node.Status.Config` 最终变为空，因为所有配置源都被重置为 `nil`，这表明本地默认配置是 `assigned`, `active`, 和 `lastKnownGood` ，并且没有报告错误。

### Kubectl Patch 示例

有很多种改变节点配置源的方式。这是一个使用 `kubectl patch` 命令的示例：  

```bash
kubectl patch node ${NODE_NAME} -p "{\"spec\":{\"configSource\":{\"configMap\":{\"name\":\"${CONFIG_MAP_NAME}\",\"namespace\":\"kube-system\",\"kubeletConfigKey\":\"kubelet\"}}}}"
```

### 理解Kubelet检查点的配置

当向节点分配新配置时，Kubelet下载并将配置有效负载解包为本地磁盘上的一组文件。Kubelet还记录元数据(metadata)，这些元数据在本地跟踪assigned 和 last-known-good 配置源，这样即使API服务器不可用，Kubelet也知道在重启时使用哪个配置。在检查配置和相关元数据之后，如果kubelet检测到所分配的配置发生了更改，它会退出。当os级别的服务管理器(例如 `systemd`)重新启动Kubelet时，它将读取新的元数据并使用新的配置。

所记录的元数据被完全解析，意味着它包含选择特定配置版本所需的所有信息——通常是 `UID` 和 `ResourceVersion`。与 `Node.Spec.ConfigSource` 相反的是，其中预期的配置通过标识目标ConfigMap的幂等 `namespace/name` 来声明;Kubelet尝试使用这个ConfigMap的最新版本。

在调试节点上的问题时，可以检查Kubelet的配置元数据和检查点。kubelet检查点目录的结构为:

```none
- --dynamic-config-dir (root for managing dynamic config)
| - meta
  | - assigned (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the assigned config)
  | - last-known-good (encoded kubeletconfig/v1beta1.SerializedNodeConfigSource object, indicating the last-known-good config)
| - checkpoints
  | - uid1 (dir for versions of object identified by uid1)
    | - resourceVersion1 (dir for unpacked files from resourceVersion1 of object with uid1)
    | - ...
  | - ...
```

### 理解 Node.Status.Config.Error 信息

下表描述了使用动态Kubelet配置时可能发生的错误消息。您可以在Kubelet日志中搜索相同的文本，以获得关于错误的更多细节和上下文。

| Error Message                                                | Possible Causes                                              |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| failed to load config, see Kubelet log for details           | Kubelet可能无法解析下载的配置有效负载，或者在试图从磁盘加载有效负载时遇到文件系统错误。 |
| failed to validate config, see Kubelet log for details       | 有效负载中的配置，以及任何命令行标志的覆盖，以及来自标志、配置文件和远程有效负载的特性门的总和，被Kubelet确定为无效。 |
| invalid NodeConfigSource, exactly one subfield must be non-nil, but all were nil | 因为Node.Spec.ConfigSource被API服务器验证为至少包含一个非nil子字段，这可能意味着Kubelet比API服务器更老，并且不能识别新的源类型。 |
| failed to sync: failed to download config, see Kubelet log for details | Kubelet无法下载配置。有可能无法将Node.Spec.ConfigSource解析为具体的API对象，或者网络错误中断了下载尝试。在此错误状态下，Kubelet将重试下载。 |
| failed to sync: internal failure, see Kubelet log for details | Kubelet遇到了一些内部问题，因此未能更新其配置。示例包括文件系统错误和从内部informer缓存读取对象。 |
| internal failure, see Kubelet log for details                | Kubelet在处理配置同步循环之外的配置时遇到了一些内部问题。    |







## 为系统守护进程保留计算资源

Kubernetes 的节点可以按照 `Capacity` 调度。默认情况下 pod 能够使用节点全部可用容量。这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程（system daemons）。除非为这些系统守护进程留出资源，否则它们将与 pod 争夺资源并导致节点资源短缺问题。

`kubelet` 公开了一个名为 `Node Allocatable` 的特性，有助于为系统守护进程预留计算资源。Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 `Node Allocatable`。

### 准备工作

一个集群

### 节点可分配

```text
      Node Capacity
---------------------------
|     kube-reserved       |
|-------------------------|
|     system-reserved     |
|-------------------------|
|    eviction-threshold   |
|-------------------------|
|                         |
|      allocatable        |
|   (available for pods)  |
|                         |
|                         |
---------------------------
```

Kubernetes 节点上的 `Allocatable` 被定义为 pod 可用计算资源量。调度器不会超额申请`Allocatable`。目前支持 `CPU`, `memory` 和 `storage` 这几个参数。

Node Allocatable 暴露为 API 中 `v1.Node` 对象的一部分，也是 CLI 中 `kubectl describe node` 的一部分。

在 `kubelet` 中，可以为两类系统守护进程预留资源。

### 启用 QoS 和 Pod 级别的 cgroups

为了恰当的在节点范围实施 node allocatable，您必须通过 `--cgroups-per-qos` 标志启用新的 cgroup 层次结构。这个标志是默认启用的。启用后，`kubelet` 将在其管理的 cgroup 层次结构中创建所有终端用户的 pod。

### 配置 cgroup 驱动

`kubelet` 支持在主机上使用 cgroup 驱动操作 cgroup 层次结构。驱动通过 `--cgroup-driver` 标志配置。

支持的参数值如下：

-   `cgroupfs` 是默认的驱动，在主机上直接操作 cgroup 文件系统以对 cgroup 沙箱进行管理。
-   `systemd` 是可选的驱动，使用 init 系统支持的资源的瞬时切片管理 cgroup 沙箱。

取决于相关容器运行时（container runtime）的配置，操作员可能需要选择一个特定的 cgroup 驱动来保证系统正常运行。例如如果操作员使用 `docker` 运行时提供的 cgroup 驱动时，必须配置 `kubelet` 使用 `systemd` cgroup 驱动。

### Kube Reserved

-   **Kubelet Flag**: `--kube-reserved=[cpu=100m][,][memory=100Mi][,][storage=1Gi]`
-   **Kubelet Flag**: `--kube-reserved-cgroup=`

`kube-reserved` 是为了给诸如 `kubelet`、`container runtime`、`node problem detector` 等 kubernetes 系统守护进程争取资源预留。这并不代表要给以 pod 形式运行的系统守护进程保留资源。`kube-reserved` 通常是节点上的一个 `pod 密度（pod density）` 功能。 [这个性能仪表盘](http://node-perf-dash.k8s.io/#/builds) 从 pod 密度的多个层面展示了 `kubelet`和 `docker engine` 的 `cpu` 和 `memory` 使用情况。

要选择性的在系统守护进程上执行 `kube-reserved`，需要把 kubelet 的 `--kube-reserved-cgroup` 标志的值设置为 kube 守护进程的父控制组。

推荐将 kubernetes 系统守护进程放置于顶级控制组之下（例如 systemd 机器上的 `runtime.slice`）。理想情况下每个系统守护进程都应该在其自己的子控制组中运行。请参考[这篇文档](https://git.k8s.io/community/contributors/design-proposals/node-allocatable.md#recommended-cgroups-setup)，获取更过关于推荐控制组层次结构的细节。

请注意，如果 `--kube-reserved-cgroup` 不存在，Kubelet 将**不会**创建它。如果指定了一个无效的 cgroup，Kubelet 将会失败。

### 系统预留值（System Reserved）

-   **Kubelet Flag**: `--system-reserved=[cpu=100mi][,][memory=100Mi][,][storage=1Gi]`
-   **Kubelet Flag**: `--system-reserved-cgroup=`

`system-reserved` 用于为诸如 `sshd`、`udev` 等系统守护进程争取资源预留。`system-reserved` 也应该为 `kernel` 预留 `内存`，因为目前 `kernel` 使用的内存并不记在 Kubernetes 的 pod 上。同时还推荐为用户登录会话预留资源（systemd 体系中的 `user.slice`）。

要想在系统守护进程上可选地执行 `system-reserved`，请指定 `--system-reserved-cgroup` kubelet 标志的值为 OS 系统守护进程的父级控制组。

推荐将 OS 系统守护进程放在一个顶级控制组之下（例如 systemd 机器上的`system.slice`）。

请注意，如果 `--system-reserved-cgroup` 不存在，Kubelet **不会**创建它。如果指定了无效的 cgroup，Kubelet 将会失败。

### 驱逐阈值（Eviction Thresholds）

-   **Kubelet Flag**: `--eviction-hard=[memory.available<500Mi]`

节点级别的内存压力将导致系统内存不足（System OOMs），这将影响到整个节点及其上运行的所有 pod。节点可以暂时离线直到内存已经回收为止。为了防止（或减少可能性）系统内存不足，kubelet 提供了 [`资源不足（Out of Resource）`](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/) 管理。驱逐（Eviction）操作只支持`memory` 和 `storage`。通过 `--eviction-hard` 标志预留一些内存后，当节点上的可用内存降至保留值以下时，`kubelet` 将尝试 `驱逐` pod。假设，如果节点上不存在系统守护进程，pod 将不能使用超过 `capacity-eviction-hard` 的资源。因此，为驱逐而预留的资源对 pod 是不可用的。

### 执行节点 Allocatable

-   **Kubelet Flag**: `--enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]`

调度器将 `Allocatable` 按 pod 的可用 `capacity` 对待。

`kubelet` 默认在 pod 中执行 `Allocatable`。无论何时，如果所有 pod 的总用量超过了 `Allocatable`，驱逐 pod 的措施将被执行。有关驱逐策略的更多细节可以在 [这里](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#eviction-policy) 找到。请通过设置 kubelet `--enforce-node-allocatable` 标志值为 `pods` 控制这个措施。

可选的，通过在相同标志中同时指定 `kube-reserved` 和 `system-reserved` 值能够使 `kubelet` 执行 `kube-reserved` 和 `system-reserved`。请注意，要想执行 `kube-reserved`或者 `system-reserved`时，需要分别指定 `--kube-reserved-cgroup` 或者 `--system-reserved-cgroup`。

### 一般原则

系统守护进程期望被按照类似 `Guaranteed` pod 一样对待。系统守护进程可以在其范围控制组中爆发式增长，您需要将这个行为作为 kubernetes 部署的一部分进行管理。例如，`kubelet` 应该有它自己的控制组并和容器运行时（container runtime）共享 `Kube-reserved`资源。然而，如果执行了 `kube-reserved`，则 kubelet 不能突然爆发并耗尽节点的所有可用资源。

在执行 `system-reserved` 预留操作时请加倍小心，因为它可能导致节点上的关键系统服务 CPU 资源短缺或因为内存不足（OOM）而被终止。

-   在 `pods` 上执行 `Allocatable` 作为开始。
-   一旦足够用于追踪系统守护进程的监控和告警的机制到位，请尝试基于用量探索（usage heuristics）方式执行 `kube-reserved`。
-   随着时间推进，如果绝对必要，可以执行 `system-reserved`。

随着时间的增长以及越来越多特性的加入，kube 系统守护进程对资源的需求可能也会增加。以后 kubernetes 项目将尝试减少对节点系统守护进程的利用，但目前那并不是优先事项。所以，请期待在将来的发布中将 `Allocatable` 容量降低。

### 示例场景

这是一个用于说明节点 Allocatable 计算方式的示例：

-   节点拥有 `32Gi 内存`，`16 核 CPU` 和 `100Gi 存储`
-   `--kube-reserved` 设置为 `cpu=1,memory=2Gi,storage=1Gi`
-   `--system-reserved` 设置为 `cpu=500m,memory=1Gi,storage=1Gi`
-   `--eviction-hard` 设置为 `memory.available<500Mi,nodefs.available<10%`

在这个场景下，`Allocatable` 将会是 `14.5 CPUs`、`28.5Gi` 内存以及 `98Gi` 存储。调度器保证这个节点上的所有 pod 请求的内存总量不超过 `28.5Gi`，存储不超过 `88Gi`。当 pod 的内存使用总量超过 `28.5Gi` 或者磁盘使用总量超过 `88Gi` 时，Kubelet 将会驱逐它们。如果节点上的所有进程都尽可能多的使用 CPU，则 pod 加起来不能使用超过 `14.5 CPUs` 的资源。

当没有执行 `kube-reserved` 和/或 `system-reserved` 且系统守护进程使用量超过其预留时，如果节点内存用量高于 `31.5Gi` 或存储大于 `90Gi`，`kubelet` 将会驱逐 pod。

### 可用特性

截至 Kubernetes 1.2 版本，已经可以**可选**的指定 `kube-reserved` 和 `system-reserved` 预留。当在相同的发布中都可用时，调度器将转为使用 `Allocatable` 替代 `Capacity`。

截至 Kubernetes 1.6 版本，`eviction-thresholds` 是通过计算 `Allocatable` 进行考虑。要使用旧版本的行为，请设置 `--experimental-allocatable-ignore-eviction` kubelet 标志为 `true`。

截至 Kubernetes 1.6 版本，`kubelet` 使用控制组在 pod 上执行 `Allocatable`。要使用旧版本行为，请取消设置 `--enforce-node-allocatable` kubelet 标志。请注意，除非 `--kube-reserved` 或者 `--system-reserved` 或者 `--eviction-hard` 标志没有默认参数，否则 `Allocatable` 的实施不会影响已经存在的 deployment。

截至 Kubernetes 1.6 版本，`kubelet` 在 pod 自己的 cgroup 沙箱中启动它们，这个 cgroup 沙箱在 kubelet 管理的 cgroup 层次结构中的一个独占部分中。在从前一个版本升级 `kubelet` 之前，要求操作员 drain 节点，以保证 pod 及其关联的容器在 cgroup 层次结构中合适的部分中启动。

截至 Kubernetes 1.7 版本，`kubelet` 支持指定 `storage` 为 `kube-reserved` 和 `system-reserved` 的资源。

截至 Kubernetes 1.8 版本，键名 `storage` 更改为alpha版本的 `ephemeral-storage` 。







## 在考虑pod中断策略(PodDisruptionBudget)的前提下，安全地清空节点

本页展示如何遵循使用 PodDisruptionBudget 指定的应用级中断策略安全地移除节点。

### 准备工作

操作前，要求下列先决条件已经满足：

-   您正在使用的 Kubernetes 必须是 1.5 或者更高版本。
-   下面都要求满足：
    1.  在节点删除期间，不能要求您的应用是高可用的
    2.  您已经了解了 [PodDisruptionBudget 的概念](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)，并且为有需要的应用[配置了 PodDisruptionBudgets](https://kubernetes.io/docs/tasks/run-application/configure-pdb/)。

### 使用 `kubectl drain` 从集群中移除节点

对节点执行维护操作之前（例如：内核升级，硬件维护等），您可以使用 `kubectl drain` 安全驱逐节点上面所有的 pod。安全驱逐的方式将会允许 pod 里面的容器遵循指定的 `PodDisruptionBudgets` 执行[优雅的中止](https://kubernetes.io/docs/tasks/#lifecycle-hooks-and-termination-notice)。

**注：** 默认情况下，`kubectl drain` 会忽略那些不能杀死的系统类型的 pod，如果您想了解更多详细的内容，请参考[kubectl drain](https://kubernetes.io/docs/user-guide/kubectl/v1.9/#drain)

`kubectl drain` 返回成功表明所有的 pod （除了前面排除的那些）已经被安全驱逐（遵循期望优雅的中止期，并且没有违反任何应用程序级别的中断策略）。然后，通过对物理机断电或者在云平台上删除节点所在的虚拟机，都能安全的将节点移除。

首先，需要确定希望移除的节点的名称。您可以通过下面命令列出集群里面所有的节点：

```
kubectl get nodes
```

接下来，告知 Kubernetes 移除节点：

```
kubectl drain <node name>
```

执行完成后，如果没有任何错误返回，您可以关闭节点（如果是在云平台上，可以删除支持该节点的虚拟机）。如果在维护操作期间想要将节点留在集群，那么您需要运行下面命令：

```
kubectl uncordon <node name>
```

然后，它将告知 Kubernetes 允许调度新的 pod 到该节点。

### 并行的移除多个节点

`kubectl drain` 命令只允许一次处理单个节点。但是，您可以在不同终端或者以后台运行的方式，使用不同的节点名称运行多次 `kubectl drain` 命令。多个移除节点命令同时运行时，依然会遵循您指定的 `PodDisruptionBudget`。

例如，假如您有一个 StatefulSet，它有 3 个副本，并设置了一个指定 `minAvailable: 2` 的 `PodDisruptionBudget`。在 3 个 pod 都准备好的情况下，`kubectl drain` 只会从这个 StatefulSet 中驱逐一个 pod。如果有多个驱逐命令并行执行，Kubernetes 也会遵循该 PodDisruptionBudget 的设定，在任何时间内，都只有一个 pod 是不可用的。任何可能导致副本数低于该预算指定值的移除都会被阻塞。

### 驱逐 API

如果不喜欢使用 [kubectl drain](https://kubernetes.io/docs/user-guide/kubectl/v1.9/#drain)（例如，期望避免调用外部命令，或者期望对 pod 驱逐过程进行更好的控制），您也可以使用驱逐 API 通过编程的方式触发驱逐。

您首先应该熟悉使用 [Kubernetes 语言客户端](https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api)。

一个 pod 的驱逐子资源可以被认为是一种政策-控制删除 pod 本身的操作。为了试图完成驱逐（也许更确切的说，是试图 *创建* 一个驱逐），您需要发布一个 POST 操作。下面是操作消息的示例：

```
{
  "apiVersion": "policy/v1beta1",
  "kind": "Eviction",
  "metadata": {
    "name": "quux",
    "namespace": "default"
  }
}
```

您能够使用 `curl` 尝试驱逐：

```
$ curl -v -H 'Content-type: application/json' http://127.0.0.1:8080/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
```

API 将以下面三种方式之一进行响应：

-   如果驱逐是允许的，那么 pod 被删除完成，就好像您发送了一个删除 pod 的请求命令并且获得一个 `200 OK` 回应一样。
-   如果目前的状况不允许被预算中的规定所驱逐，您将获得 `429 Too Many Requests` 的回应。这通常是用于 *任何* 请求的通用速率限制，但这里我们的意思是 *现在* 不允许这种请求，以后可能允许。目前，请求者不会得到 *稍后重试* 的提示，但在以后的版本中可能会。
-   如果存在配置错误，比如多个预算指向同一个 pod，您将获得 `500 Internal Server Error` 的回应。

对于一个给定的驱逐请求，有两种情况：

-   没有可以匹配该 pod 的预算。在这种情况下，服务器总是返回 `200 OK`。
-   至少有一个预算满足要求。在这种情况下，上面三种响应都有可能返回。

在某些情况下，应用程序可能会达到一个失败的状态，在该状态下，它将永远不会返回除了 429 或者 500 以外的任何内容。例如，应用程序控制器创建的替代的 pod 没有准备好，或者最后一个被驱逐的 pod 有很长的中止宽限期，都可能发生这种情况。

在这种情况下，有两个可能的解决方案：

-   中止或者暂停自动化操作。分析卡住应用程序的原因，再重启自动化操作。
-   经过长时间等待之后，`删除` pod 而不是调用驱逐 API。

Kubernetes 没有具体说明在这种情况下应该怎么做；它取决于应用程序所有者和集群所有者建立的在这种情况下的行为协议。

### 下一步

-   按照[配置 pod 中断策略](https://kubernetes.io/docs/tasks/run-application/configure-pdb/)的步骤来保护您的应用程序。





## 保护集群

本文档涉及与保护集群免受意外或恶意访问有关的主题，并对总体安全性提出建议。

### 准备工作

一个集群

### 控制对 Kubernetes API 的访问

因为 Kubernetes 是完全通过 API 驱动的，所以，控制和限制谁可以通过 API 访问集群，以及允许这些访问者执行什么样的 API 动作，就成为了安全控制的第一道防线。

#### 为 API 交互提供传输层安全（TLS）

Kubernetes 期望集群中所有的 API 通信在默认情况下都使用 TLS 加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。请注意，某些组件和安装方法可能使用 HTTP 来访问本地端口， 管理员应该熟悉每个组件的设置，以识别潜在的不安全的流量。

#### API 认证

安装集群时，选择一个 API 服务器的身份验证机制，去使用与之匹配的公共访问模式。例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。更大的集群则可能希望整合现有的、OIDC、LDAP 等允许用户分组的服务器。

所有 API 客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。这些客户端通常使用[服务帐户](https://kubernetes.io/docs/admin/service-accounts-admin/)或 X509 客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置。

如果您希望获取更多信息，请参考[认证参考文档](https://kubernetes.io/docs/admin/authentication/)。

#### API 授权

一旦使用授权，每个 API 的调用都将通过授权检查。Kubernetes 集成[基于访问控制（RBAC）](https://kubernetes.io/docs/admin/authorization/rbac/)的组件，将传入的用户或组与一组绑定到角色的权限匹配。这些权限将动作（get，create，delete）和资源（pod，service, node）在命名空间或者集群范围内结合起来，根据客户可能希望执行的操作，提供了一组提供合理的违约责任分离的外包角色。建议您将[节点](https://kubernetes.io/docs/admin/authorization/node/)和 [RBAC](https://kubernetes.io/docs/admin/authorization/rbac/) 一起作为授权者，再与 [NodeRestriction](https://kubernetes.io/docs/admin/admission-controllers/#noderestriction) 准入插件结合使用。

与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，可能需要将团队划分成有更多角色限制的单独的命名空间。

使用授权时，理解怎么样更新一个对象可能导致在其它地方的发生什么样的行为是非常重要的。例如，用户可能不能直接创建 pod，但允许他们通过创建一个 deployment 来创建这些 pod，这将让他们间接创建这些 pod。同样地，从 API 删除一个节点将导致调度到这些节点上的 pod 被中止，并在其他节点上重新创建。外包角色代表了灵活性和常见用例之间的平衡，但有限制的角色应该仔细审查，以防止意外升级。如果外包角色不满足您的需求，则可以为用例指定特定的角色。

如果您希望获取更多信息，请参考[授权参考部分](https://kubernetes.io/docs/admin/authorization)。

### 控制运行时负载或用户的能力

Kubernetes 中的授权故意设置为了高层级，它侧重于对资源的粗行为。更强大的控制是以通过用例限制这些对象如何作用于集群、自身和其他资源上的**策略**存在的。

#### 限制集群上的资源使用

[资源配额](https://kubernetes.io/docs/concepts/policy/resource-quotas/)限制了授予命名空间的资源的数量或容量。这通常用于限制命名空间可以分配的 CPU、内存或持久磁盘的数量，但也可以控制每个命名空间中有多少个 pod、服务或卷的存在。

[限制范围](https://kubernetes.io/docs/admin/limitrange)限制了上述某些资源的最大值或者最小值，以防止用户使用类似内存这样的通用保留资源时请求不合理的过高或过低的值，或者在没有指定的情况下提供默认限制。

#### 控制容器运行的特权

pod 定义包含了一个[安全上下文](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)，用于描述允许它请求访问某个节点上的特定 Linux 用户（如 root）、获得特权或访问主机网络、以及允许它在主机节点上不受约束地运行的其它控件。[Pod 安全策略](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)可以限制哪些用户或服务帐户可以提供危险的安全上下文设置。例如，pod 的安全策略可以限制卷挂载，尤其是 `hostpath`，这些都是 pod 应该控制的一些方面。

一般来说，大多数应用程序需要限制对主机资源的访问，他们可以在不能访问主机信息的情况下成功以根进程（UID 0）运行。但是，考虑到与 root 用户相关的特权，在编写应用程序容器时，您应该使用非 root 用户运行。类似地，希望阻止客户端应用程序逃避其容器的管理员，应该使用限制性的 pod 安全策略。

#### 限制网络访问

基于命名空间的[网络策略](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/)允许应用程序作者限制其它命名空间中的哪些 pod 可以访问它们命名空间内的 pod 和端口。现在已经有许多支持网络策略的 [Kubernetes 网络供应商](https://kubernetes.io/docs/concepts/cluster-administration/networking/)。

对于可以控制用户的应用程序是否在集群之外可见的许多集群，配额和限制范围也可用于控制用户是否可以请求节点端口或负载均衡服务。

在插件或者环境基础上控制网络规则可以增加额外的保护措施，比如节点防火墙、物理分离群集节点以防止串扰、或者高级的网络策略。

#### 控制哪些节点和 pod 可以访问

默认情况下，对哪些节点可以运行 pod 没有任何限制。Kubernetes 给最终用户提供了[一组丰富的策略用于控制 pod 放在节点上的位置](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)，以及[基于 pod 位置和驱逐的污点](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration)。对于许多集群，可以约定由作者采用或者强制通过工具使用这些策略来分离工作负载。

作为管理员，β 特性的准入插件 `podnodeselector` 可用于强制命名空间中的 pod 使用默认或需要使用特定的节点选择器。如果最终用户无法改变命名空间，这可以强烈地限制所有的 pod 在特定工作负载的位置。

### 保护集群组件免受破坏

本节描述保护集群免受破坏的一些常见模式。

#### 限制访问 etcd

对于 API 来说，拥有 etcd 后端的写访问权限，相当于获得了整个集群的 root 权限。从 API 服务器访问它们的 etcd 服务器，管理员应该使用广受信任的凭证，如通过 TLS 客户端证书的相互认证。往往，我们建议将 etcd 服务器隔离到只有API服务器可以访问的防火墙后面。

**注意：** 允许集群中其它组件拥有读或写全空间的权限去访问 etcd 实例，相当于授予群集管理员访问的权限。对于非 master 组件，强烈推荐使用单独的 etcd 实例，或者使用 etcd 的访问控制列表去限制只能读或者写空间的一个子集。

#### 开启审计日志

[审计日志](https://kubernetes.io/docs/admin/audit/)是 α 特性，记录了 API 在发生破坏时进行后续分析的操作。建议启用审计日志，并将审计文件归档到安全服务器上。

#### 限制使用 α 和 β 特性

Kubernetes 的 α 和 β 特性还在积极发展当中，可能存在导致安全漏洞的缺陷或错误。要始终评估 α 和 β 特性可能为您的安全态势带来的风险。当您怀疑存在风险时，可以禁用那些不需要使用的特性。

#### 频繁回收基础证书

一个 secret 或凭据的寿命越短，攻击者就越难使用该凭据，在证书上设置短生命周期并实现自动回收，是控制安全的一个好方法。因此，使用身份验证提供程序时，应该要求可以控制发布令牌的可用时间，并尽可能使用短寿命。如果在外部集成中使用服务帐户令牌，则应该频繁地回收这些令牌。例如，一旦引导阶段完成，就应该撤销用于设置节点的引导令牌，或者取消它的授权。

#### 开启集成前审查第三方

许多集成到 Kubernetes 的第三方都可以改变您集群的安全配置。启用集成时，在授予访问权限之前，您应该始终检查扩展请求的权限。例如，许多安全集成可以请求访问来查看集群上的所有 secret，从而有效地使该组件成为集群管理。当有疑问时，如果可能的话，将集成限制在单个命名空间中运行。

如果组件创建的 pod 能够在命名空间中做一些类似 `kube-system` 命名空间中的事情，那么它也可能是出乎意料的强大。因为这些 pod 可以访问服务账户的 secret，或者，如果这些服务帐户被授予访问许可的 [POD 安全策略](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)的权限，它们能以高权限运行。

#### 使用加密的 REST

一般情况下，etcd 数据库包含了通过 Kubernetes API 可以访问到的所有信息，并且可以授予攻击者对集群状态的可见性。始终使用经过良好审查的备份和加密解决方案来加密备份，并考虑在可能的情况下使用全磁盘加密。

Kubernetes 1.7 包含了[ rest 加密](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)，它是一个 α 特性，会加密 etcd 里面的 `Secret` 资源，以防止某一方通过查看这些 secret 的内容获取 etcd 的备份。虽然目前这还只是实验性的功能，但是在备份没有加密或者攻击者获取到 etcd 的读访问权限的时候，它能提供额外的防御层级。

#### 接收安全更新和报告漏洞的警报

加入 [kubernetes-announce](https://groups.google.com/forum/#!forum/kubernetes-announce) 组，能够获取有关安全公告的邮件。有关如何报告漏洞的更多信息，请参见[安全报告](https://kubernetes.io/security/)页面。





## 通过配置文件设置Kubelet参数

**FEATURE STATE:** `Kubernetes v1.15` [feature-state-alpha.txt](https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/#)

在 Kubernetes 1.8 版本上，除了可以通过命令行参数外，还可以通过保存在硬盘的配置文件设置 Kubelet 的配置子集。 将来，大部分现存的命令行参数都将被废弃，取而代之以配置文件的方式提供参数，以简化节点部署过程。

### 准备工作

-   需要安装 1.8 版本或更高版本的 Kubelet 二进制文件。

### 创建配置文件

`KubeletConfiguration` 结构体定义了可以通过文件配置的 Kubelet 配置子集，该结构体在 [这里（v1beta1）](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go) 可以找到, 配置文件必须是这个结构体中参数的 JSON 或 YAML 表现形式。

在单独的文件夹中创建一个名为 `kubelet` 的文件，并保证 Kubelet 可以读取该文件夹及文件。您应该在这个 `kubelet` 文件中编写 Kubelet 配置。

这是一个 Kubelet 配置文件示例：

```
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
evictionHard:
    memory.available:  "200Mi"
```

在这个示例中, 当可用内存低于200Mi 时, Kubelet 将会开始驱逐 Pods。 没有声明的其余配置项都将使用默认值, 命令行中的 flags 将会覆盖配置文件中的对应值。

作为一个小技巧，您可以从活动节点生成配置文件，相关方法请查看 [重新配置活动集群节点的 Kubelet](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet)。

### 启动通过配置文件配置的 Kubelet 进程

启动 Kubelet 需要将其 `--init-config-dir` 标志设置为包含 `kubelet` 文件的文件夹路径。Kubelet 将从 `kubelet` 文件中读取由 `KubeletConfiguration` 定义的参数，而不是从参数相关的命令行标志中读取。

### 与动态 Kubelet 配置的关系

如果您正在使用 [动态 Kubelet 配置（Dynamic Kubelet Configuration）](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet) 特性，那么自动回滚机制将认为通过 `--init-config-dir` 提供的配置是“最后已知正常（last known good）”的配置。

请注意，`--init-config-dir` 文件的布局结构镜像了 ConfigMap 中用于动态 Kubelet 配置的数据结构；文件命名和 ConfigMap 的 key 相同，文件的内容是 ConfigMap 中相同数据结构的 JSON 或 YAML 表现形式。虽然以后可能会出现更多，但目前只有 kubelet:KubeletConfiguration 配置对。更多信息请查阅 [重新配置活动集群节点的 Kubelet](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet)。





## 设置高可用性的Kubernetes master

**FEATURE STATE:** `Kubernetes 1.5` [alpha](https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/#)

可以在 `kube-up` 或 `kube-down` 脚本中为GCE(Google Compute Engine)复制kubernetes master。 该文档描述了如何使用 kube-up/down 脚本去管理高可用（HA）master，以及 HA master 如何实现并在 GCE 中使用。

### 准备工作

一个集群

### 启动 HA-兼容集群

创建新的 HA-兼容集群，必须在 `kube-up` 脚本中设置如下标志：

-   `MULTIZONE=true` - 防止 master 副本从某些 zone 中被移除，这不同于 server 的默认 zone。 如果希望在不同的 zone 中运行 master 副本，该标志是推荐的，而且也是必需的。
-   `ENABLE_ETCD_QUORUM_READS=true` - 为了确保来自所有 API server 的读请求将会返回最新的数据。 如果为 true，读将直接请求到 etcd 的 Leader 副本上。 将该值设置为 true 是可选的：读将非常可靠但会比较慢。

可选地，可以指定一个 GCE zone，在这里创建第一个 master 副本。 设置下面的标志：

-   `KUBE_GCE_ZONE=zone` - 第一个 master 副本将运行所在的 zone。

下面的示例命令在 GCE zone `europe-west1-b` 中建立了一个 HA-兼容集群：

```shell
$ MULTIZONE=true KUBE_GCE_ZONE=europe-west1-b  ENABLE_ETCD_QUORUM_READS=true ./cluster/kube-up.sh
```

注意，上面的命令创建了具有 1 个 master 的集群。 然而，可以使用后续的命令向集群中添加新的 master 副本。

### 新增 master 副本

创建一个 HA-兼容的集群后，可以向集群中添加 master 副本。 新增 master 副本，可以使用带有如下标志的 `kube-up` 脚本：

-   `KUBE_REPLICATE_EXISTING_MASTER=true` - 为已存在 master 创建一个副本。
-   `KUBE_GCE_ZONE=zone` - 指定 zone，master 副本将运行在其中。 必须像其他副本的 zone 一样，在相同的区域中。

没有必要设置 `MULTIZONE` 或 `ENABLE_ETCD_QUORUM_READS` 这两个标志，因为那些标志会从启动的 HA-兼容集群继承而来。

下面示例命令，在一个已存在的 HA-兼容集群中复制 master：

```shell
$ KUBE_GCE_ZONE=europe-west1-c KUBE_REPLICATE_EXISTING_MASTER=true ./cluster/kube-up.sh
```

### 删除 master 副本

通过使用带有如下标志的 `kube-down` 脚本，可以从 HA 集群中删除 master 副本：

-   `KUBE_DELETE_NODES=false` - 限制删除 kubelet。
-   `KUBE_GCE_ZONE=zone` - 对应的 zone，master 副本将被从该 zone 中删除。
-   `KUBE_REPLICA_NAME=replica_name` - （可选）要删除的 master 副本的名称。 如果为空：任何来自给定 zone 的副本都将被删除。

下面的示例命令，从一个已存在的 HA 集群中删除一个 master 副本：

```shell
$ KUBE_DELETE_NODES=false KUBE_GCE_ZONE=europe-west1-c ./cluster/kube-down.sh
```

### 处理 master 副本失败

如果 HA 集群中某个 master 副本失败了，最佳的实践是从集群中删除该副本，并在同一个 zone 中新增一个副本。 下面的示例命令演示了这个流程：

1.  删除失败的副本：

```shell
$ KUBE_DELETE_NODES=false KUBE_GCE_ZONE=replica_zone KUBE_REPLICA_NAME=replica_name ./cluster/kube-down.sh
```

1.  新增一个副本来替换旧的副本：

```shell
$ KUBE_GCE_ZONE=replica-zone KUBE_REPLICATE_EXISTING_MASTER=true ./cluster/kube-up.sh
```

### HA 集群复制 master 最佳实践

-   尝试在不同的 zone 中放置 master 副本。如果 zone 失败，则所有放置在该 zone 中的 master 都将失败。 为了使 zone 从失败中恢复，也需要在多个 zone 中放置节点（查看 [多个 zone](https://kubernetes.io/docs/admin/multiple-zones/) 获取更多详情）。
-   不要使用两个 master 副本的集群。 两副本集群的一致性要求，当改变持久状态时，这两个副本都能够运行。 结果两个副本是必需的，任何一个副本失败都会导致集群变成大多数失败的状态。 因此就 HA 而言，两副本集群不如单副本集群。
-   当添加一个 master 副本时，集群状态（etcd）被拷贝到一个新的实例上。 如果集群很大，它可能需要花费很长时间去复制自己的状态。 通过迁移 etcd 数据目录，可能会加快这个操作的速度，查看 [这里](https://coreos.com/etcd/docs/latest/admin_guide.html#member-migration) 给出的说明（将来我们可能考虑增加对 etcd 数据目录迁移的支持）。

### 实现需要注意的事项

![ha-master-gce](https://d33wubrfki0l68.cloudfront.net/fb4611540cdb78a38934d20e594c77324338beaa/5c51f/images/docs/ha-master-gce.png)

#### 概览

每个 master 副本将在下面模式下运行下列组件：

-   etcd 实例：所有实例将基于一致性算法被聚集起来；
-   API server：每个 server 将与本地 etcd 通信 - 集群中所有 API server 都可用；
-   controller、scheduler 和 集群 auto-scaler：将使用租约机制 - 集群中有且仅有它们中的一个被激活；
-   插件管理器：每个管理器将独立起作用，设法保持插件同步。

另外，在这些 API server 前面将有一个负载均衡器，它将内部和外部的流量路由到这些 API server。

#### 负载均衡

当启动第二个 master 副本时，将创建包含 2 个副本的负载均衡器，第一个副本的 IP 地址将被提升为负载均衡器的 IP。 类似地，当倒数第二个 master 副本删除以后，负载均衡器将被移除，它的 IP 地址将被指派给最后留下的副本。 请注意，创建和删除负载均衡器是复杂的操作，它可能会花费一些时间（大约 20 分钟）去传播它们。

#### Master Service 和 kubelet

在 Kubernetes Service 中设法保持一个最新的 apiserver 列表，作为替代，系统直接将全部流量指向指定外部 IP：

-   在单 master 集群，IP 指向单个 master。
-   在多 master 集群，IP 指向这些 master 前面的负载均衡器。

类似地，外部 IP 将被 kubelet 用来与 master 通信。

#### Master 证书

Kubernetes 为外部公共 IP 和本地每个副本的 IP 生成 Master TLS 证书。 不会为副本的临时公共 IP 生成证书。 基于临时公共 IP 访问一个副本，必须要跳过 TLS 验证。

#### 聚集 etcd

为了允许 etcd 聚集，需要在 etcd 实例之间进行通信的端口将被打开（为了在集群内部通信）。 为了使得部署安全，在 etcd 实例之间的通信需要使用 SSL 进行授权。

### 进一步阅读

[HA master 自动化部署 - 设计文档](https://git.k8s.io/community/contributors/design-proposals/cluster-lifecycle/ha_master.md)









## 通过命名空间共享集群

本页展示了如何查看、使用和删除命名空间。

命名空间是 Kubernetes 为了在同一物理集群上支持多个虚拟集群而使用的一种抽象。

本页同时展示了如何使用 Kubernetes 命名空间去细分集群。

### 准备工作

-   您已拥有一个 [配置好的 Kubernetes 集群](https://v1-14.docs.kubernetes.io/docs/setup/).
-   您已对 Kubernetes 的 *Pods*, *Services*, 和 *Deployments* 有基本理解。

### 查看命名空间

1.  列出集群中现有的命名空间：

```shell
kubectl get namespaces
NAME          STATUS    AGE
default       Active    11d
kube-system   Active    11d
kube-public   Active    11d
```

初始状态下，Kubernetes 具有三个名字空间：

-   `default` 无命名空间对象的默认命名空间
-   `kube-system` 由 Kubernetes 系统创建的对象的命名空间
-   `kube-public` 自动创建且被所有用户可读的命名空间（包括未经身份认证的）。此命名空间通常在某些资源在整个集群中可见且可公开读取时被集群使用。此命名空间的公共方面只是一个约定，而不是一个必要条件。

您还可以通过下列命令获取特定命名空间的摘要：

```shell
kubectl get namespaces <name>
```

或获取详细信息：

```shell
kubectl describe namespaces <name>
Name:           default
Labels:         <none>
Annotations:    <none>
Status:         Active

No resource quota.

Resource Limits
 Type       Resource    Min Max Default
 ----               --------    --- --- ---
 Container          cpu         -   -   100m
```

请注意，这些详情同时显示了资源配额（如果存在）以及资源限制区间。

资源配额跟踪并聚合 *Namespace* 中资源的使用情况，并允许集群运营者定义 *Namespace* 可能消耗的 *Hard* 资源使用限制。

限制区间定义了单个实体在一个 *Namespace* 中可使用的最小/最大资源量约束。

参阅 [准入控制: 限制区间](https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md)

命名空间可以处于下列两个阶段中的一个:

-   `Active` 命名空间使用中
-   `Terminating` 命名空间正在被删除，且不能被用于新对象。

参见 [设计文档](https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#phases) 查看更多细节。

### 创建命名空间

1.  新建一个名为 `my-namespace.yaml` 的 YAML 文件，并写入下列内容：

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: <insert-namespace-name-here>
```

然后运行：

```shell
kubectl create -f ./my-namespace.yaml
```

请注意，命名空间的名称必须是 DNS 兼容的标签。

可选字段 `finalizers` 允许观察者们在命名空间被删除时清除资源。记住如果指定了一个不存在的终结器，命名空间仍会被创建，但如果用户试图删除它，它将陷入 `Terminating` 状态。

更多有关 `finalizers` 的信息请查阅 [设计文档](https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#finalizers) 中命名空间部分。

### 删除命名空间

1.  删除命名空间使用命令

```shell
kubectl delete namespaces <insert-some-namespace-name>
```

>   Warning:
>
>   这会删除命名空间下的 *所有内容* ！

删除是异步的,所以有一段时间你会看到命名空间处于 `Terminating` 状态。

### 使用 Kubernetes 命名空间细分您的集群

1.  理解默认命名空间

默认情况下，Kubernetes 集群会在配置集群时实例化一个默认命名空间，用以存放集群所使用的默认 Pods、Services 和 Deployments 集合。

假设您有一个新的集群，您可以通过执行以下操作来内省可用的命名空间

```shell
kubectl get namespaces
NAME      STATUS    AGE
default   Active    13m
```

1.  创建新的命名空间

在本练习中，我们将创建两个额外的 Kubernetes 命名空间来保存我们的内容。

在某组织使用共享的 Kubernetes 集群进行开发和生产的场景中：

开发团队希望在集群中维护一个空间，以便他们可以查看用于构建和运行其应用程序的 Pods、Services 和 Deployments 列表。在这个空间里，Kubernetes 资源被自由地加入或移除，对谁能够或不能修改资源的限制被放宽，以实现敏捷开发。

运维团队希望在集群中维护一个空间，以便他们可以强制实施一些严格的规程，对谁可以或不可以操作运行生产站点的 Pods、Services 和 Deployments 集合进行控制。

该组织可以遵循的一种模式是将 Kubernetes 集群划分为两个命名空间：development 和 production。

让我们创建两个新的命名空间来保存我们的工作。

文件 [`namespace-dev.json`](https://v1-14.docs.kubernetes.io/examples/admin/namespace-dev.json) 描述了 development 命名空间:

[`admin/namespace-dev.json`](https://raw.githubusercontent.com/kubernetes/website/master/content/zh/examples/admin/namespace-dev.json)

```json
{
  "kind": "Namespace",
  "apiVersion": "v1",
  "metadata": {
    "name": "development",
    "labels": {
      "name": "development"
    }
  }
}
```

使用 kubectl 创建 `development` 命名空间。

```shell
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
```

让我们使用 kubectl 创建 `production` 命名空间。

```shell
kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
```

为了确保一切正常，列出集群中的所有命名空间。

```shell
kubectl get namespaces --show-labels
NAME          STATUS    AGE       LABELS
default       Active    32m       <none>
development   Active    29s       name=development
production    Active    23s       name=production
```

1.  在每个命名空间中创建 pod

Kubernetes 命名空间为集群中的 Pods、Services 和 Deployments 提供了作用域。

与一个命名空间交互的用户不会看到另一个命名空间中的内容。

为了演示这一点，让我们在 `development` 命名空间中启动一个简单的 Deployment 和 Pod。

我们首先检查一下当前的上下文：

```shell
kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://130.211.122.180
  name: lithe-cocoa-92103_kubernetes
contexts:
- context:
    cluster: lithe-cocoa-92103_kubernetes
    user: lithe-cocoa-92103_kubernetes
  name: lithe-cocoa-92103_kubernetes
current-context: lithe-cocoa-92103_kubernetes
kind: Config
preferences: {}
users:
- name: lithe-cocoa-92103_kubernetes
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
    token: 65rZW78y8HbwXXtSXuUw9DbP4FLjHi4b
- name: lithe-cocoa-92103_kubernetes-basic-auth
  user:
    password: h5M0FtUUIflBSdI7
    username: admin
kubectl config current-context
lithe-cocoa-92103_kubernetes
```

下一步是为 kubectl 客户端定义一个上下文，以便在每个命名空间中工作。”cluster” 和 “user” 字段的值将从当前上下文中复制。

```shell
kubectl config set-context dev --namespace=development --cluster=lithe-cocoa-92103_kubernetes --user=lithe-cocoa-92103_kubernetes
kubectl config set-context prod --namespace=production --cluster=lithe-cocoa-92103_kubernetes --user=lithe-cocoa-92103_kubernetes
```

上述命令提供了两个可以替代的请求上下文，具体取决于您希望使用的命名空间。

让我们切换到 `development` 命名空间进行操作。

```shell
kubectl config use-context dev
```

您可以使用下列命令验证当前上下文：

```shell
kubectl config current-context
dev
```

此时，我们从命令行向 Kubernetes 集群发出的所有请求都限定在 `development` 命名空间中。

让我们创建一些内容。

```shell
kubectl run snowflake --image=kubernetes/serve_hostname --replicas=2
```

我们刚刚创建了一个副本大小为 2 的 deployment，该 deployment 运行名为 snowflake 的 pod，其中包含一个仅提供主机名服务的基本容器。请注意，`kubectl run` 仅在 Kubernetes 集群版本 >= v1.2 时创建 deployment。如果您运行在旧版本上，则会创建 replication controllers。如果期望执行旧版本的行为，请使用 `--generator=run/v1` 创建 replication controllers。 参见 [`kubectl run`](https://v1-14.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#run) 获取更多细节。

```shell
kubectl get deployment
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
snowflake   2         2         2            2           2m
kubectl get pods -l run=snowflake
NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
```

这很棒，开发人员可以做他们想要的事情，而不必担心影响 `production` 命名空间中的内容。

让我们切换到 `production` 命名空间，展示一个命名空间中的资源如何对另一个命名空间不可见。

```shell
kubectl config use-context prod
```

`production` 命名空间应该是空的，下列命令应该返回的内容为空。

```shell
kubectl get deployment
kubectl get pods
```

生产环境需要运行 cattle，让我们创建一些名为 cattle 的 pods。

```shell
kubectl run cattle --image=kubernetes/serve_hostname --replicas=5

kubectl get deployment
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
cattle    5         5         5            5           10s
kubectl get pods -l run=cattle
NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
```

此时，应该很清楚的展示了用户在一个命名空间中创建的资源对另一个命名空间是隐藏的。

随着 Kubernetes 中的策略支持的发展，我们将扩展此场景，以展示如何为每个命名空间提供不同的授权规则。

### 理解使用命名空间的动机

单个集群应该能满足多个用户及用户组的需求（以下称为 “用户社区”）。

Kubernetes *命名空间* 帮助不同的项目、团队或客户去共享 Kubernetes 集群。

名字空间通过以下方式实现这点：

1.  为[名字](https://v1-14.docs.kubernetes.io/docs/concepts/overview/working-with-objects/names/)设置作用域.
2.  为集群中的部分资源关联鉴权和策略的机制。

使用多个命名空间是可选的。

每个用户社区都希望能够与其他社区隔离开展工作。

每个用户社区都有:

1.  资源（pods, services, replication controllers, 等等）
2.  策略（谁能或不能在他们的社区里执行操作）
3.  约束（该社区允许多少配额，等等）

集群运营者可以为每个唯一用户社区创建命名空间。

命名空间为下列内容提供唯一的作用域：

1.  命名资源（避免基本的命名冲突）
2.  将管理权限委派给可信用户
3.  限制社区资源消耗的能力

用例包括:

1.  作为集群运营者, 我希望能在单个集群上支持多个用户社区。
2.  作为集群运营者，我希望将集群分区的权限委派给这些社区中的受信任用户。
3.  作为集群运营者，我希望能限定每个用户社区可使用的资源量，以限制对使用同一集群的其他用户社区的影响。
4.  作为群集用户，我希望与我的用户社区相关的资源进行交互，而与其他用户社区在该集群上执行的操作无关。

### 理解命名空间和 DNS

当您创建 [Service](https://v1-14.docs.kubernetes.io/docs/concepts/services-networking/service/) 时，它会创建相应的 [DNS 条目](https://v1-14.docs.kubernetes.io/docs/concepts/services-networking/dns-pod-service/)。此条目的格式为 `<service-name>。<namespace-name> .svc.cluster.local`，这意味着如果容器只使用 `<service-name>`，它将解析为本地服务到命名空间。 这对于在多个命名空间（如开发，暂存和生产）中使用相同的配置非常有用。 如果要跨命名空间访问，则需要使用完全限定的域名（FQDN）。

### 接下来

-   了解更多 [设置命名空间首选项](https://v1-14.docs.kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-preference) 的内容。
-   了解更多 [设置请求的命名空间](https://v1-14.docs.kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#setting-the-namespace-for-a-request) 的内容。
-   参见 [命名空间设计](https://github.com/kubernetes/community/blob/v1.14.3/contributors/design-proposals/architecture/namespaces.md)。





## 静态pod

**如果你正在运行Kubernetes集群并且使用静态pods在每个节点上起一个pod，那么最好使用DaemonSet!**

*静态pods*直接由特定节点上的kubelet进程来管理，不通过主控节点上的API服务器。静态pod不关联任何replication controller，它由kubelet进程自己来监控，当pod崩溃时重启该pod。对于静态pod没有健康检查。静态pod始终绑定在某一个kubelet，并且始终运行在同一个节点上。

Kubelet自动为每一个静态pod在Kubernetes的API服务器上创建一个镜像Pod（Mirror Pod），因此可以在API服务器查询到该pod，但是不被API服务器控制（例如不能删除）。

### 静态pod创建

静态pod有两种创建方式：用配置文件或者通过HTTP。

#### 配置文件

配置文件就是放在特定目录下的标准的JSON或YAML格式的pod定义文件。用`kubelet --pod-manifest-path=<the directory>`来启动kubelet进程，kubelet将会周期扫描这个目录，根据这个目录下出现或消失的YAML/JSON文件来创建或删除静态pod。

下面例子用静态pod的方式启动一个nginx的Web服务器：

1.  选择一个节点来运行静态pod。这个例子中就是`my-node1`。

    ```
    [joe@host ~] $ ssh my-node1
    ```

2.  选择一个目录，例如/etc/kubelet.d，把web服务器的pod定义文件放在这个目录下，例如`/etc/kubelet.d/static-web.yaml`:

    ```
    [root@my-node1 ~] $ mkdir /etc/kubelet.d/
    [root@my-node1 ~] $ cat <<EOF >/etc/kubelet.d/static-web.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: static-web
      labels:
        role: myrole
    spec:
      containers:
        - name: web
          image: nginx
          ports:
            - name: web
              containerPort: 80
              protocol: TCP
    EOF
    ```

3.配置节点上的kubelet使用这个目录，kubelet启动时增加`--pod-manifest-path=/etc/kubelet.d/`参数。如果是Fedora系统，在Kubelet配置文件/etc/kubernetes/kubelet中添加下面这行：

```
​```
KUBELET_ARGS="--cluster-dns=10.254.0.10 --cluster-domain=kube.local --pod-manifest-path=/etc/kubelet.d/"
​```
```

如果是其它Linux发行版或者其它Kubernetes安装方式，配置方法可能会不一样。

1.  重启kubelet。如果是Fedora系统，就是：

    ```
    [root@my-node1 ~] $ systemctl restart kubelet
    ```

### 通过HTTP创建静态Pods

Kubelet周期地从–manifest-url=参数指定的地址下载文件，并且把它翻译成JSON/YAML格式的pod定义。此后的操作方式与–pod-manifest-path=相同，kubelet会不时地重新下载该文件，当文件变化时对应地终止或启动静态pod（如下）。

### 静态pods的动作行为

kubelet启动时，由`--pod-manifest-path=` or `--manifest-url=`参数指定的目录下定义的所有pod都会自动创建，例如，我们示例中的static-web。 （可能要花些时间拉取nginx镜像，耐心等待…）

```shell
[joe@my-node1 ~] $ docker ps
CONTAINER ID IMAGE         COMMAND  CREATED        STATUS         PORTS     NAMES
f6d05272b57e nginx:latest  "nginx"  8 minutes ago  Up 8 minutes             k8s_web.6f802af4_static-web-fk-node1_default_67e24ed9466ba55986d120c867395f3c_378e5f3c
```

如果我们查看Kubernetes的API服务器（运行在主机 `my-master`），可以看到这里创建了一个新的镜像Pod：

```shell
[joe@host ~] $ ssh my-master
[joe@my-master ~] $ kubectl get pods
NAME                       READY     STATUS    RESTARTS   AGE
static-web-my-node1        1/1       Running   0          2m
```

静态pod的标签会传递给镜像Pod，可以用来过滤或筛选。

需要注意的是，我们不能通过API服务器来删除静态pod（例如，通过 [`kubectl`](https://kubernetes.io/docs/user-guide/kubectl/) 命令），kebelet不会删除它。

```shell
[joe@my-master ~] $ kubectl delete pod static-web-my-node1
pods/static-web-my-node1
[joe@my-master ~] $ kubectl get pods
NAME                       READY     STATUS    RESTARTS   AGE
static-web-my-node1        1/1       Running   0          12s
```

返回`my-node1`主机，我们尝试手动终止容器，可以看到kubelet很快就会自动重启容器。

```shell
[joe@host ~] $ ssh my-node1
[joe@my-node1 ~] $ docker stop f6d05272b57e
[joe@my-node1 ~] $ sleep 20
[joe@my-node1 ~] $ docker ps
CONTAINER ID        IMAGE         COMMAND                CREATED       ...
5b920cbaf8b1        nginx:latest  "nginx -g 'daemon of   2 seconds ago ...
```

### 静态pods的动态增加和删除

运行中的kubelet周期扫描配置的目录（我们这个例子中就是`/etc/kubelet.d`）下文件的变化，当这个目录中有文件出现或消失时创建或删除pods。

```shell
[joe@my-node1 ~] $ mv /etc/kubelet.d/static-web.yaml /tmp
[joe@my-node1 ~] $ sleep 20
[joe@my-node1 ~] $ docker ps
// no nginx container is running
[joe@my-node1 ~] $ mv /tmp/static-web.yaml  /etc/kubelet.d/
[joe@my-node1 ~] $ sleep 20
[joe@my-node1 ~] $ docker ps
CONTAINER ID        IMAGE         COMMAND                CREATED           ...
e7a62e3427f1        nginx:latest  "nginx -g 'daemon of   27 seconds ago
```





## 使用CoreDNS进行服务发现

此页面介绍了 CoreDNS 升级过程以及如何安装 CoreDNS 而不是 kube-dns。

### 准备工作

一个集群。

### 关于 CoreDNS

[CoreDNS](https://coredns.io/) 是一个灵活可扩展的 DNS 服务器，可以作为 Kubernetes 集群 DNS。与 Kubernetes 一样，CoreDNS 项目由 CNCF([http://www.cncf.io](http://www.cncf.io/)) 持有。

通过在现有的集群中替换 kube-dns，可以在集群中使用 CoreDNS 代替 kube-dns 部署，或者使用 kubeadm 等工具来为您部署和升级集群。

### 安装 CoreDNS

有关手动部署或替换 kube-dns，请参阅 [CoreDNS GitHub 工程](https://github.com/coredns/deployment/tree/master/kubernetes)。

### 使用 kubeadm 升级现有集群

在 Kubernetes 1.10 及更高版本中，当您使用 `kubeadm` 升级使用 `kube-dns` 的集群时，您还可以迁移到 CoreDNS。 在本例中 `kubeadm` 将生成 CoreDNS 配置（”Corefile”）基于 `kube-dns` ConfigMap，保存联邦、存根域和上游域名服务器的配置。

如果您正在从 kube-dns 迁移到 CoreDNS，请确保在升级期间将 `CoreDNS` 特性门设置为 `true`。例如，`v1.11.0` 升级应该是这样的:

```
kubeadm upgrade apply v1.11.0 --feature-gates=CoreDNS=true
```

在 1.11 之前的版本中，核心文件将被升级过程中创建的文件覆盖。 **如果已对其进行自定义，则应保存现有的 ConfigMap。** 在新的 ConfigMap 启动并运行后，您可以重新应用自定义。

如果您在 Kubernetes 1.11 及更高版本中运行 CoreDNS，则在升级期间，将保留现有的 Corefile。

### 使用 kubeadm 安装 kube-dns 而不是 CoreDNS

>   Note:
>
>   在 Kubernetes 1.11 中，CoreDNS 已经升级到通用可用性(GA)，并默认安装。

若要安装 kube-dns，请将 `CoreDNS` 特性门值设置为 `false`：

```
kubeadm init --feature-gates=CoreDNS=false
```

### CoreDNS 调优

当涉及到资源利用时，优化内核的配置可能是有用的。有关详细信息，请参阅 [关于扩展 CoreDNS 的文档](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/coredns/(https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md))。

### 接下来

您可以通过修改 `Corefile` 来配置 [CoreDNS](https://coredns.io/)，以支持比 ku-dns 更多的用例。有关更多信息，请参考 [CoreDNS 网站](https://coredns.io/2017/05/08/custom-dns-entries-for-kubernetes/)。







## （未翻译）在Kubernetes集群中使用节点上的DNS缓存(NodeLocal DNSCache)

This page provides an overview of NodeLocal DNSCache feature in Kubernetes.

###  准备工作

一个集群

### Introduction

NodeLocal DNSCache improves Cluster DNS performance by running a dns caching agent on cluster nodes as a DaemonSet. In today’s architecture, Pods in ClusterFirst DNS mode reach out to a kube-dns serviceIP for DNS queries. This is translated to a kube-dns/CoreDNS endpoint via iptables rules added by kube-proxy. With this new architecture, Pods will reach out to the dns caching agent running on the same node, thereby avoiding iptables DNAT rules and connection tracking. The local caching agent will query kube-dns service for cache misses of cluster hostnames(cluster.local suffix by default).

### Motivation

-   With the current DNS architecture, it is possible that Pods with the highest DNS QPS have to reach out to a different node, if there is no local kube-dns/CoreDNS instance.
    Having a local cache will help improve the latency in such scenarios.
-   Skipping iptables DNAT and connection tracking will help reduce [conntrack races](https://github.com/kubernetes/kubernetes/issues/56903) and avoid UDP DNS entries filling up conntrack table.
-   Connections from local caching agent to kube-dns servie can be upgraded to TCP. TCP conntrack entries will be removed on connection close in contrast with UDP entries that have to timeout ([default](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt) `nf_conntrack_udp_timeout` is 30 seconds)
-   Upgrading DNS queries from UDP to TCP would reduce tail latency attributed to dropped UDP packets and DNS timeouts usually up to 30s (3 retries + 10s timeout). Since the nodelocal cache listens for UDP DNS queries, applications don’t need to be changed.
-   Metrics & visibility into dns requests at a node level.
-   Negative caching can be re-enabled, thereby reducing number of queries to kube-dns service.

### Architecture Diagram

This is the path followed by DNS Queries after NodeLocal DNSCache is enabled:

![NodeLocal DNSCache flow](https://d33wubrfki0l68.cloudfront.net/70532f3eae66b1e0f4d8b9bfb85cb02d78cf2fae/d61fa/images/docs/nodelocaldns.jpg)

#### Nodelocal DNSCache flow

This image shows how NodeLocal DNSCahe handles DNS queries.

### Configuration

This feature can be enabled using the command:

```
KUBE_ENABLE_NODELOCAL_DNS=true go run hack/e2e.go -v --up
```

This works for e2e clusters created on GCE. On all other environments, the following steps will setup NodeLocal DNSCache:

-   A yaml similar to [this](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml) can be applied using `kubectl create -f` command.
-   –cluster-dns flag to kubelet needs to be modified to use the LOCAL_DNS IP that NodeLocal DNSCache is listening on (169.254.20.10 by default)

Once enabled, node-local-dns Pods will run in the kube-system namespace on each of the cluster nodes. This Pod runs [CoreDNS](https://github.com/coredns/coredns) in cache mode, so all CoreDNS metrics exposed by the different plugins will be available on a per-node basis.

#### Feature availability

The addon can be applied using the yaml specified above in any k8s version. The feature support is as described:

| k8s version | Feature support               |
| :---------- | :---------------------------- |
| 1.15        | Beta(Not enabled by default)  |
| 1.13        | Alpha(Not enabled by default) |







## 使用KMS提供程序进行数据加密

本页展示了如何配置秘钥管理服务—— Key Management Service (KMS) 提供商和插件以启用数据加密。

### 准备工作

-   一个集群。

-   需要 Kubernetes 1.10.0 或更新版本

-   需要 etcd v3 或更新版本

**FEATURE STATE:** `Kubernetes v1.12` [beta](https://v1-14.docs.kubernetes.io/zh/docs/tasks/administer-cluster/kms-provider/#)

KMS 加密提供商使用封套加密模型来加密 etcd 中的数据。数据使用数据加密秘钥（DEK）加密；每次加密都生成一个新的 DEK。这些 DEK 经一个秘钥加密秘钥（KEK）加密后在一个远端的 KMS 中存储和管理。KMS 提供商使用 gRPC 与一个特定的 KMS 插件通信。这个 KMS 插件作为一个 gRPC 服务器被部署在 Kubernetes 主服务器的同一个主机上，负责与远端 KMS 的通信。

### 配置 KMS 提供商

为了在 API 服务器上配置 KMS 提供商，在加密配置文件中的提供商数组中加入一个类型为 `kms` 的提供商，并设置下列属性：

-   `name`: KMS 插件的显示名称。
-   `endpoint`: gRPC 服务器（KMS 插件）的监听地址。该端点是一个 UNIX 的套接字。
-   `cachesize`: 以明文缓存的数据加密秘钥（DEKs）的数量。一旦被缓存，就可以直接使用 DEKs 而无需另外调用 KMS；而未被缓存的 DEKs 需要调用一次 KMS 才能解包。
-   `timeout`: 在返回一个错误之前，kube-apiserver 等待 kms-plugin 响应的时间（默认是 3 秒）。

参见 [理解静态数据加密配置](https://v1-14.docs.kubernetes.io/docs/tasks/administer-cluster/encrypt-data)

### 实现 KMS 插件

为实现一个 KMS 插件，您可以开发一个新的插件 gRPC 服务器或启用一个由您的云服务提供商提供的 KMS 插件。您可以将这个插件与远程 KMS 集成，并把它部署到 Kubernetes 的主服务器上。

#### 启用由云服务提供商支持的 KMS

有关启用云服务提供商特定的 KMS 插件的说明，请咨询您的云服务提供商。

#### 开发 KMS 插件 gRPC 服务器

您可以使用 Go 语言的存根文件开发 KMS 插件 gRPC 服务器。对于其他语言，您可以用 proto 文件创建可以用于开发 gRPC 服务器代码的存根文件。

-   使用 Go：使用存根文件 [service.pb.go](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.pb.go) 中的函数和数据结构开发 gRPC 服务器代码。

-   使用 Go 以外的其他语言：用 protoc 编译器编译 proto 文件： [service.proto](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/v1beta1/service.proto) 为指定语言生成存根文件。

然后使用存根文件中的函数和数据结构开发服务器代码。

**注意：**

-   kms 插件版本：`v1beta1`

作为对过程调用 Version 的响应，兼容的 KMS 插件应把 v1beta1 作为 VersionResponse.version 返回

-   消息版本：`v1beta1`

所有来自 KMS 提供商的消息都把 version 字段设置为当前版本 v1beta1

-   协议：UNIX 域套接字 (`unix`)

gRPC 服务器应监听 UNIX 域套接字

#### 将 KMS 插件与远程 KMS 整合

KMS 插件可以用任何受 KMS 支持的协议与远程 KMS 通信。 所有的配置数据，包括 KMS 插件用于与远程 KMS 通信的认证凭据，都由 KMS 插件独立地存储和管理。KMS 插件可以用额外的元数据对密文进行编码，这些元数据是在把它发往 KMS 进行解密之前可能要用到的。

#### 部署 KMS 插件

确保 KMS 插件与 Kubernetes 主服务器运行在同一主机上。

### 使用 KMS 提供商加密数据

为了加密数据：

1.  使用 `kms` 提供商的相应的属性创建一个新的加密配置文件：

```yaml
kind: EncryptionConfiguration
apiVersion: apiserver.config.k8s.io/v1
resources:
  - resources:
    - secrets
    providers:
    - kms:
        name: myKmsPlugin
        endpoint: unix:///tmp/socketfile.sock
        cachesize: 100
        timeout: 3s
    - identity: {}
```

1.  设置 kube-apiserver 的 `--encryption-provider-config` 参数指向配置文件的位置。
2.  重启 API 服务器。

注意： 在 1.13 之前的加密功能的 alpha 版本需要一个带有 `kind: EncryptionConfig` 和 `apiVersion: v1` 的配置文件，并使用 `--experimental-encryption-provider-config` 标志。

### 验证数据是否已加密

写入 etcd 时数据被加密。重启 kube-apiserver 后，任何新建或更新的秘密信息在存储时应该已被加密。要验证这点，您可以用 etcdctl 命令行程序获取秘密信息内容。

1.  在默认的命名空间里创建一个名为 secret1 的秘密信息： `kubectl create secret generic secret1 -n default --from-literal=mykey=mydata`
2.  用 etcdctl 命令行，从 etcd 读取出秘密信息： `ETCDCTL_API=3 etcdctl get /kubernetes.io/secrets/default/secret1 [...] | hexdump -C`其中 `[...]` 是用于连接 etcd 服务器的额外参数。
3.  验证保存的秘密信息是否是以 `k8s:enc:kms:v1:` 开头的，这表明 `kms` 提供商已经对结果数据加密。
4.  验证秘密信息在被 API 获取时已被正确解密： `kubectl describe secret secret1 -n default` 应该符合 `mykey: mydata` 格式

### 确保所有秘密信息都已被加密

因为秘密信息是在写入时被加密的，所以在更新秘密信息时会加密该内容。

下列命令读取所有秘密信息并更新他们以便应用服务器端加密。如果因为写入冲突导致错误发生，请重试此命令。对较大的集群，您可能希望根据命名空间或脚本更新去细分秘密内容。

```
kubectl get secrets --all-namespaces -o json | kubectl replace -f -
```

### 从本地加密提供商切换到 KMS 提供商

为了从本地加密提供商切换到 `kms` 提供商并重新加密所有秘密内容：

1.  在配置文件中加入 `kms` 提供商作为第一个条目，如下列样例所示

```yaml
kind: EncryptionConfiguration
apiVersion: apiserver.config.k8s.io/v1
resources:
  - resources:
    - secrets
    providers:
    - kms:
        name : myKmsPlugin
        endpoint: unix:///tmp/socketfile.sock
        cachesize: 100
    - aescbc:
         keys:
         - name: key1
           secret: <BASE 64 ENCODED SECRET>
```

1.  重启所有 kube-apiserver 进程。
2.  运行下列命令使用 `kms` 提供商强制重新加密所有秘密信息。 `kubectl get secrets --all-namespaces -o json| kubectl replace -f -`

### 禁用静态数据加密

要禁用静态数据加密：

1.  将 `identity` 提供商作为配置文件中的第一个条目：

```yaml
kind: EncryptionConfiguration
apiVersion: apiserver.config.k8s.io/v1
resources:
  - resources:
    - secrets
    providers:
    - identity: {}
    - kms:
        name : myKmsPlugin
        endpoint: unix:///tmp/socketfile.sock
        cachesize: 100
```

1.  重启所有 kube-apiserver 进程。
2.  运行下列命令强制重新加密所有秘密信息。 `kubectl get secrets --all-namespaces -o json | kubectl replace -f -`





## 在Kubernetes集群中使用sysctl

**FEATURE STATE:** `Kubernetes v1.11` [feature-state-beta.txt](https://kubernetes.io/zh/docs/tasks/administer-cluster/sysctl-cluster/#)

本文档介绍如何通过 sysctl 接口在 Kubernetes 集群中配置和使用内核参数。

### 准备工作

一个集群。

### 获取 Sysctl 的参数列表

在 Linux 中，管理员可以通过 sysctl 接口修改内核运行时的参数。在 `/proc/sys/` 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：

-   内核子系统 (通常前缀为: `kernel.`)
-   网络子系统 (通常前缀为: `net.`)
-   虚拟内存子系统 (通常前缀为: `vm.`)
-   MDADM 子系统 (通常前缀为: `dev.`)
-   更多子系统请参见 [内核文档](https://www.kernel.org/doc/Documentation/sysctl/README)。

若要获取完整的参数列表，请执行以下命令

```shell
$ sudo sysctl -a
```

### 启用非安全的 Sysctl 参数

sysctl 参数分为 *安全* 和 _非安全的_。*安全* sysctl 参数除了需要设置恰当的命名空间外，在同一 node 上的不同 Pod 之间也必须是 _相互隔离的_。这意味着在 Pod 上设置 *安全* sysctl 参数

-   必须不能影响到节点上的其他 Pod
-   必须不能损害节点的健康
-   必须不允许使用超出 Pod 的资源限制的 CPU 或内存资源。

至今为止，大多数 *有命名空间的* sysctl 参数不一定被认为是 *安全* 的。以下几种 sysctl 参数是 _安全的_：

-   `kernel.shm_rmid_forced`,
-   `net.ipv4.ip_local_port_range`,
-   `net.ipv4.tcp_syncookies`.

>   Note:
>
>   **注意**: 示例中的 `net.ipv4.tcp_syncookies` 在Linux 内核 4.4 或更低的版本中是无命名空间的。

在未来的 Kubernetes 版本中，若kubelet 支持更好的隔离机制，则上述列表中将会列出更多 *安全的* sysctl 参数。

所有 *安全的* sysctl 参数都默认启用。

所有 *非安全的* sysctl 参数都默认禁用，且必须由集群管理员在每个节点上手动开启。那些设置了不安全 sysctl 参数的 Pod 仍会被调度，但无法正常启动。

参考上述警告，集群管理员只有在一些非常特殊的情况下（如：高可用或实时应用调整），才可以启用特定的 *非安全的* sysctl 参数。如需启用 *非安全的* sysctl 参数，请您在每个节点上分别设置 kubelet 命令行参数，例如：

```shell
$ kubelet --allowed-unsafe-sysctls \
  'kernel.msg*,net.core.somaxconn' ...
```

如果您使用 minikube，可以通过 `extra-config` 参数来配置：

```shell
$ minikube start --extra-config="kubelet.AllowedUnsafeSysctls=kernel.msg*,net.core.somaxconn"...
```

只有 *有命名空间的* sysctl 参数可以通过该方式启用。

### 设置 Pod 的 Sysctl 参数

目前，在 Linux 内核中，有许多的 sysctl 参数都是 *有命名空间的* 。 这就意味着可以为节点上的每个 Pod 分别去设置它们的 sysctl 参数。 在 Kubernetes 中，只有那些有命名空间的 sysctl 参数可以通过 Pod 的 securityContext 对其进行配置。

以下列出有命名空间的 sysctl 参数，在未来的 Linux 内核版本中，此列表可能会发生变化。

-   `kernel.shm*`,
-   `kernel.msg*`,
-   `kernel.sem`,
-   `fs.mqueue.*`,
-   `net.*`（内核中网络配置项相关参数），如果它可以在容器命名空间里被更改。然而，也有一些特例 (例如，`net.netfilter.nf_conntrack_max` 和 `net.netfilter.nf_conntrack_expect_max` 可以在容器命名空间里被更改，但它们是非命名空间的)。

没有命名空间的 sysctl 参数称为 *节点级别的* sysctl 参数。 如果需要对其进行设置，则必须在每个节点的操作系统上手动地去配置它们，或者通过在 DaemonSet 中运行特权模式容器来配置。

可使用 pod 的 securityContext 来配置有命名空间的 sysctl 参数，securityContext 应用于同一个 pod 中的所有容器。

此示例中，使用 Pod SecurityContext 来对一个安全的 sysctl 参数 `kernel.shm_rmid_forced`以及两个非安全的 sysctl 参数 `net.core.somaxconn`和 `kernel.msgmax` 进行设置。在 Pod 规格中对 *安全的* 和 *非安全的* sysctl 参数不做区分。

>   Warning:
>
>   为了避免破坏操作系统的稳定性，请您在了解变更后果之后再修改 sysctl 参数。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example
spec:
  securityContext:
    sysctls:
    - name: kernel.shm_rmid_forced
      value: "0"
    - name: net.core.somaxconn
      value: "1024"
    - name: kernel.msgmax
      value: "65536"
  ...
```

>   Warning:
>
>   **警告**：由于 *非安全的* sysctl 参数其本身具有不稳定性，在使用 *非安全的* sysctl 参数时可能会导致一些严重问题，如容器的错误行为、机器资源不足或节点被完全破坏，用户需自行承担风险。

最佳实践方案是将集群中具有特殊 sysctl 设置的节点视为 _受感染的_，并且只调度需要使用到特殊 sysctl 设置的 Pod 到这些节点上。 建议使用 Kubernetes 的 [*taints 和 toleration* 特性](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#taint) 来实现它。

设置了 *非安全的* sysctl 参数的 pod，在禁用了以下两种 *非安全的* sysctl 参数配置的节点上启动都会失败。与 *节点级别的* sysctl 一样，建议开启 [*taints 和 toleration* 特性](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#taint) 或 [taints on nodes](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) 以便将 Pod 调度到正确的节点之上。

### PodSecurityPolicy

您可以通过在 PodSecurityPolicy 的 `forbiddenSysctls` 和/或 `allowedUnsafeSysctls` 字段中，指定 sysctl 或填写 sysctl 匹配模式来进一步为 Pod 设置 sysctl 参数。sysctl 参数匹配模式以 `*` 字符结尾，如 `kernel.*`。 单独的 `*` 字符匹配所有 sysctl 参数。

所有 *安全的* sysctl 参数都默认启用。

`forbiddenSysctls` 和 `allowedUnsafeSysctls` 的值都是字符串列表类型，可以添加 sysctl 参数名称，也可以添加 sysctl 参数匹配模式（以`*`结尾）。 只填写 `*` 则匹配所有的 sysctl 参数。

`forbiddenSysctls` 字段用于禁用特定的 sysctl 参数。 您可以在列表中禁用安全和非安全的 sysctl 参数的组合。 要禁用所有的 sysctl 参数，请设置为 `*`。

如果要在 `allowedUnsafeSysctls` 字段中指定一个非安全的 sysctl 参数，并且它在`forbiddenSysctls` 字段中未被禁用，则可以在 Pod 中通过 PodSecurityPolicy 启用该 sysctl 参数。 若要在 PodSecurityPolicy 中开启所有非安全的 sysctl 参数，请设 `allowedUnsafeSysctls` 字段值为 `*`。

`allowedUnsafeSysctls` 与 `forbiddenSysctls` 两字段的配置不能重叠，否则这就意味着存在某个 sysctl 参数既被启用又被禁用。

>   Warning:
>
>   **警告**：如果您通过 PodSecurityPolicy 中的 `allowedUnsafeSysctls` 字段将非安全的 sysctl 参数列入白名单，但该 sysctl 参数未通过 kubelet 命令行参数 `--allowed-unsafe-sysctls` 在节点上将其列入白名单，则设置了这个 sysctl 参数的 Pod 将会启动失败。

以下示例设置启用了以 `kernel.msg` 为前缀的非安全的 sysctl 参数，以及禁用了 sysctl 参数 `kernel.shm_rmid_forced`。

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: sysctl-psp
spec:
  allowedUnsafeSysctls:
  - kernel.msg*
  forbiddenSysctls:
  - kernel.shm_rmid_forced
 ...
```





## 在 Kubernetes 中配置私有 DNS 和上游域名服务器（旧版本）

1.15的官方英文文档里找不到，可能是其他译者自己加的或者旧版本里的。

本页展示了如何添加自定义私有 DNS 域（存根域）和上游域名服务器。

### 准备工作

-   你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

-   Kubernetes 1.6 及其以上版本。
-   集群必须配置使用 `kube-dns` 插件。

### 配置存根域和上游 DNS 服务器

通过为 kube-dns （`kube-system:kube-dns`）提供 ConfigMap，集群管理员能够指定自定义存根域和上游域名服务器。

例如，下面的 ConfigMap 建立了一个 DNS 配置，它具有一个单独的存根域和两个上游域名服务器：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  stubDomains: |
    {"acme.local": ["1.2.3.4"]}
  upstreamNameservers: |
    ["8.8.8.8", "8.8.4.4"]
```

按如上说明，具有 “.acme.local” 后缀的 DNS 请求被转发到 DNS 1.2.3.4。Google 公共 DNS服务器 为上游查询提供服务。 下表描述了具有特定域名的查询如何映射到它们的目标 DNS 服务器：

| 域名                                 | 响应查询的服务器                      |
| :----------------------------------- | :------------------------------------ |
| kubernetes.default.svc.cluster.local | kube-dns                              |
| foo.acme.local                       | 自定义 DNS (1.2.3.4)                  |
| widget.com                           | 上游 DNS (8.8.8.8, 8.8.4.4，其中之一) |

查看 [ConfigMap 选项](https://kubernetes.io/zh/docs/tasks/administer-cluster/dns-custom-nameservers/#configmap-options) 获取更多关于配置选项格式的详细信息。

### 理解 Kubernetes 中名字解析

可以为每个 Pod 设置 DNS 策略。 当前 Kubernetes 支持两种 Pod 特定的 DNS 策略：“Default” 和 “ClusterFirst”。 可以通过 `dnsPolicy` 标志来指定这些策略。 *注意：“Default” 不是默认的 DNS 策略。如果没有显式地指定 dnsPolicy，将会使用 “ClusterFirst”。*

#### “Default” DNS 策略

如果 `dnsPolicy` 被设置为 “Default”，则名字解析配置会继承自 Pod 运行所在的节点。 自定义上游域名服务器和存根域不能够与这个策略一起使用。

#### “ClusterFirst” DNS 策略

如果 `dnsPolicy` 被设置为 “ClusterFirst”，处理名字解析有所不同，*依赖于是否配置了存根域和上游 DNS 服务器*。

**未进行自定义配置**：没有匹配上配置的集群域名后缀的任何请求，例如 “www.kubernetes.io”，将会被转发到继承自节点的上游域名服务器。

**进行自定义配置**：如果配置了存根域和上游 DNS 服务器（类似于 [前面示例](https://kubernetes.io/zh/docs/tasks/administer-cluster/dns-custom-nameservers/#configuring-stub-domain-and-upstream-dns-servers) 配置的内容），DNS 查询将基于下面的流程对请求进行路由：

1.  查询首先被发送到 kube-dns 中的 DNS 缓存层。
2.  从缓存层，检查请求的后缀，并根据下面的情况转发到对应的 DNS 上：
    -   *具有集群后缀的名字*（例如 “.cluster.local”）：请求被发送到 kube-dns。
    -   *具有存根域后缀的名字*（例如 “.acme.local”）：请求被发送到配置的自定义 DNS 解析器（例如：监听在 1.2.3.4）。
    -   *未能匹配上后缀的名字*（例如 “widget.com”）：请求被转发到上游 DNS（例如：Google 公共 DNS 服务器，8.8.8.8 和 8.8.4.4）。

![DNS 查询流程](https://d33wubrfki0l68.cloudfront.net/340889cb80e81dcd19a16bc34697a7907e2b229a/24ad0/docs/tasks/administer-cluster/dns-custom-nameservers/dns.png)

### ConfigMap 选项

kube-dns `kube-system:kube-dns` 对应的 ConfigMap 选项如下所示：

| 字段                          | 格式                                                         | 描述                                                         |
| :---------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| `stubDomains`（可选）         | 使用 DNS 后缀作为键（例如 “acme.local”）的 JSON map，以及由 DNS IP 的 JSON 数组组成的值。 | 目标域名服务器可能是一个 Kubernetes Service。例如，可以运行自己的 dnsmasq 副本，将 DNS 名字暴露到 ClusterDNS namespace 中。 |
| `upstreamNameservers`（可选） | DNS IP 的 JSON 数组。                                        | 注意：如果指定，则指定的值会替换掉被默认从节点的 `/etc/resolv.conf` 中获取到的域名服务器。限制：最多可以指定三个上游域名服务器。 |

### 附加示例

#### 示例：存根域

在这个例子中，用户有一个 Consul DNS 服务发现系统，他们希望能够与 kube-dns 集成起来。 Consul 域名服务器地址为 10.150.0.1，所有的 Consul 名字具有后缀 “.consul.local”。 要配置 Kubernetes，集群管理员只需要简单地创建一个 ConfigMap 对象，如下所示：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  stubDomains: |
    {"consul.local": ["10.150.0.1"]}
```

注意，集群管理员不希望覆盖节点的上游域名服务器，所以他们不会指定可选的 `upstreamNameservers` 字段。

#### 示例：上游域名服务器

在这个示例中，集群管理员不希望显式地强制所有非集群 DNS 查询进入到他们自己的域名服务器 172.16.0.1。 而且这很容易实现：他们只需要创建一个 ConfigMap，`upstreamNameservers` 字段指定期望的域名服务器即可：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  upstreamNameservers: |
    ["172.16.0.1"]
```







## 应用资源配额和限额（旧版本）

本示例展示了在一个 namespace 中控制资源用量的典型设置。

本文展示了以下资源的使用： [Namespace](https://kubernetes.io/docs/admin/namespaces), [ResourceQuota](https://kubernetes.io/docs/concepts/policy/resource-quotas/) 和 [LimitRange](https://kubernetes.io/docs/tasks/configure-pod-container/limit-range/)。

### 准备工作

-   你必须拥有一个 Kubernetes 的集群，同时你的 Kubernetes 集群必须带有 kubectl 命令行工具。 如果你还没有集群，你可以通过 [Minikube](https://kubernetes.io/docs/getting-started-guides/minikube) 构建一 个你自己的集群，或者你可以使用下面任意一个 Kubernetes 工具构建：

-   [Katacoda](https://www.katacoda.com/courses/kubernetes/playground)
-   [Play with Kubernetes](http://labs.play-with-k8s.com/)

To check the version, enter `kubectl version`.

### 场景

集群管理员正在操作一个代表用户群体的集群，他希望控制一个特定 namespace 中可以被使用的资源总量，以达到促进对集群的公平共享及控制成本的目的。

集群管理员有以下目标：

-   限制运行中 pods 使用的计算资源数量
-   限制 persistent volume claims 数量以控制对存储的访问
-   限制 load balancers 数量以控制成本
-   防止使用 node ports 以保留稀缺资源
-   提供默认计算资源请求以实现更好的调度决策

### 创建 namespace

本示例将在一个自定义的 namespace 中运行，以展示相关概念。

让我们创建一个叫做 quota-example 的新 namespace：

```shell
$ kubectl create namespace quota-example
namespace "quota-example" created
$ kubectl get namespaces
NAME            STATUS    AGE
default         Active    2m
kube-system     Active    2m
quota-example   Active    39s
```

### 应用 object-count 配额到 namespace

集群管理员想要控制下列资源：

-   persistent volume claims
-   load balancers
-   node ports

我们来创建一个简单的配额，用于控制这个 namespace 中那些资源类型的对象数量。

```shell
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/rq-object-counts.yaml --namespace=quota-example
resourcequota "object-counts" created
```

配额系统将察觉到有一个配额被创建，并且会计算 namespace 中的资源消耗量作为响应。这应该会很快发生。

让我们显示一下配额来观察这个 namespace 中当前被消耗的资源：

```shell
$ kubectl describe quota object-counts --namespace=quota-example
Name:                  object-counts
Namespace:             quota-example
Resource               Used    Hard
--------               ----    ----
persistentvolumeclaims 0    2
services.loadbalancers 0    2
services.nodeports     0    0
```

配额系统现在将阻止用户创建比各个资源指定数量更多的资源。

### 应用计算资源配额到 namespace

为了限制这个 namespace 可以被使用的计算资源数量，让我们创建一个跟踪计算资源的配额。

```shell
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/rq-compute-resources.yaml --namespace=quota-example
resourcequota "compute-resources" created
```

让我们显示一下配额来观察这个 namespace 中当前被消耗的资源：

```shell
$ kubectl describe quota compute-resources --namespace=quota-example
Name:                  compute-resources
Namespace:             quota-example
Resource               Used Hard
--------               ---- ----
limits.cpu             0    2
limits.memory          0    2Gi
pods                   0    4
requests.cpu           0    1
requests.memory        0    1Gi
```

配额系统现在会防止 namespace 拥有超过 4 个没有终止的 pods。此外它还将强制 pod 中的每个容器配置一个 `request` 并为 `cpu` 和 `memory` 定义 `limit`。

### 应用默认资源请求和限制

Pod 的作者很少为它们的 pods 指定资源请求和限制。

既然我们对项目应用了配额，我们来看一下当终端用户通过创建一个没有 cpu 和 内存限制的 pod 时会发生什么。这通过在 pod 里创建一个 nginx 容器实现。

作为演示，让我们来创建一个运行 nginx 的 deployment：

```shell
$ kubectl run nginx --image=nginx --replicas=1 --namespace=quota-example
deployment "nginx" created
```

现在我们来看一下创建的 pods。

```shell
$ kubectl get pods --namespace=quota-example
```

发生了什么？我一个 pods 都没有！让我们 describe 这个 deployment 来看看发生了什么。

```shell
$ kubectl describe deployment nginx --namespace=quota-example
Name:                   nginx
Namespace:              quota-example
CreationTimestamp:      Mon, 06 Jun 2016 16:11:37 -0400
Labels:                 run=nginx
Selector:               run=nginx
Replicas:               0 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
OldReplicaSets:         <none>
NewReplicaSet:          nginx-3137573019 (0/1 replicas created)
...
```

Deployment 创建了一个对应的 replica set 并尝试按照大小来创建一个 pod。

让我们看看 replica set 的更多细节。

```shell
$ kubectl describe rs nginx-3137573019 --namespace=quota-example
Name:                   nginx-3137573019
Namespace:              quota-example
Image(s):               nginx
Selector:               pod-template-hash=3137573019,run=nginx
Labels:                 pod-template-hash=3137573019
                        run=nginx
Replicas:               0 current / 1 desired
Pods Status:            0 Running / 0 Waiting / 0 Succeeded / 0 Failed
No volumes.
Events:
  FirstSeen    LastSeen    Count From                    SubobjectPath Type      Reason        Message
  ---------    --------  ----- ----                    -------------    --------  ------        -------
  4m        7s        11    {replicaset-controller }              Warning   FailedCreate  Error creating: pods "nginx-3137573019-" is forbidden: Failed quota: compute-resources: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
```

Kubernetes API server 拒绝了 replica set 创建一个 pod 的请求，因为我们的 pods 没有为 `cpu` 和 `memory` 指定 `requests` 或 `limits`。

因此，我们来为 pod 指定它可以使用的 `cpu` 和 `memory` 默认数量。

```shell
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/rq-limits.yaml --namespace=quota-example
limitrange "limits" created
$ kubectl describe limits limits --namespace=quota-example
Name:           limits
Namespace:      quota-example
Type      Resource  Min  Max  Default Request   Default Limit   Max Limit/Request Ratio
----      --------  ---  ---  ---------------   -------------   -----------------------
Container memory    -    -    256Mi             512Mi           -
Container cpu       -    -    100m              200m            -
```

如果 Kubernetes API server 发现一个 namespace 中有一个创建 pod 的请求，并且 pod 中的容器没有设置任何计算资源请求时，作为准入控制的一部分，一个默认的 request 和 limit 将会被应用。

在本例中，创建的每个 pod 都将拥有如下的计算资源限制：

```shell
$ kubectl run nginx \
  --image=nginx \
  --replicas=1 \
  --requests=cpu=100m,memory=256Mi \
  --limits=cpu=200m,memory=512Mi \
  --namespace=quota-example
```

由于已经为我们的 namespace 申请了默认的计算资源，我们的 replica set 应该能够创建它的 pods 了。

```shell
$ kubectl get pods --namespace=quota-example
NAME                     READY     STATUS    RESTARTS   AGE
nginx-3137573019-fvrig   1/1       Running   0          6m
```

而且如果打印出我们在这个 namespace 中的配额使用情况：

```shell
$ kubectl describe quota --namespace=quota-example
Name:           compute-resources
Namespace:      quota-example
Resource        Used    Hard
--------        ----    ----
limits.cpu      200m    2
limits.memory   512Mi   2Gi
pods            1       4
requests.cpu    100m    1
requests.memory 256Mi   1Gi


Name:                 object-counts
Namespace:            quota-example
Resource              Used    Hard
--------              ----    ----
persistentvolumeclaims 0      2
services.loadbalancers 0      2
services.nodeports     0      0
```

就像你看到的，创建的 pod 消耗了明确的计算资源量，并且正被 Kubernetes 正确的追踪着。

### 高级配额 scopes

让我们想象一下如果你不希望为你的 namespace 指定默认计算资源使用量。

作为替换，你希望用户在它们的 namespace 中运行指定数量的 `BestEffort` pods，以从宽松的计算资源中获得好处。然后要求用户为需要更高质量服务的 pods 配置一个显式的资源请求。

让我们新建一个拥有两个配额的 namespace 来演示这种行为：

```shell
$ kubectl create namespace quota-scopes
namespace "quota-scopes" created
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/rq-best-effort.yaml --namespace=quota-scopes
resourcequota "best-effort" created
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/rq-not-best-effort.yaml --namespace=quota-scopes
resourcequota "not-best-effort" created
$ kubectl describe quota --namespace=quota-scopes
Name:       best-effort
Namespace:  quota-scopes
Scopes:     BestEffort
 * Matches all pods that have best effort quality of service.
Resource    Used    Hard
--------    ----  ----
pods        0     10


Name:             not-best-effort
Namespace:        quota-scopes
Scopes:           NotBestEffort
 * Matches all pods that do not have best effort quality of service.
Resource          Used  Hard
--------          ----  ----
limits.cpu        0     2
limits.memory     0     2Gi
pods              0     4
requests.cpu      0     1
requests.memory   0     1Gi
```

在这种场景下，一个没有配置计算资源请求的 pod 将会被 `best-effort` 配额跟踪。

而配置了计算资源请求的则会被 `not-best-effort` 配额追踪。

让我们创建两个 deployments 作为演示：

```shell
$ kubectl run best-effort-nginx --image=nginx --replicas=8 --namespace=quota-scopes
deployment "best-effort-nginx" created
$ kubectl run not-best-effort-nginx \
  --image=nginx \
  --replicas=2 \
  --requests=cpu=100m,memory=256Mi \
  --limits=cpu=200m,memory=512Mi \
  --namespace=quota-scopes
deployment "not-best-effort-nginx" created
```

虽然没有指定默认的 limits，`best-effort-nginx` deployment 还是会创建 8 个 pods。这是由于它被 `best-effort` 配额追踪，而 `not-best-effort` 配额将忽略它。`not-best-effort` 配额将追踪 `not-best-effort-nginx` deployment，因为它创建的 pods 具有 `Burstable` 服务质量。

让我们列出 namespace 中的 pods：

```shell
$ kubectl get pods --namespace=quota-scopes
NAME                                     READY     STATUS    RESTARTS   AGE
best-effort-nginx-3488455095-2qb41       1/1       Running   0          51s
best-effort-nginx-3488455095-3go7n       1/1       Running   0          51s
best-effort-nginx-3488455095-9o2xg       1/1       Running   0          51s
best-effort-nginx-3488455095-eyg40       1/1       Running   0          51s
best-effort-nginx-3488455095-gcs3v       1/1       Running   0          51s
best-effort-nginx-3488455095-rq8p1       1/1       Running   0          51s
best-effort-nginx-3488455095-udhhd       1/1       Running   0          51s
best-effort-nginx-3488455095-zmk12       1/1       Running   0          51s
not-best-effort-nginx-2204666826-7sl61   1/1       Running   0          23s
not-best-effort-nginx-2204666826-ke746   1/1       Running   0          23s
```

如你看到的，所有 10 个 pods 都已经被准许创建。

让我们 describe 这个 namespace 当前的配额使用情况：

```shell
$ kubectl describe quota --namespace=quota-scopes
Name:            best-effort
Namespace:       quota-scopes
Scopes:          BestEffort
 * Matches all pods that have best effort quality of service.
Resource         Used  Hard
--------         ----  ----
pods             8     10


Name:               not-best-effort
Namespace:          quota-scopes
Scopes:             NotBestEffort
 * Matches all pods that do not have best effort quality of service.
Resource            Used  Hard
--------            ----  ----
limits.cpu          400m  2
limits.memory       1Gi   2Gi
pods                2     4
requests.cpu        200m  1
requests.memory     512Mi 1Gi
```

如你看到的，`best-effort` 配额追踪了我们在 `best-effort-nginx` deployment 中创建的 8 个 pods 的资源用量，而 `not-best-effort` 配额追踪了我们在 `not-best-effort-nginx`deployment 中创的两个 pods 的用量。

Scopes 提供了一种来对任何配额文档追踪的资源集合进行细分的机制，给操作人员部署和追踪资源消耗带来更大的灵活性。

除 `BestEffort` 和 `NotBestEffort` scopes 之外，还有用于限制长时间运行和有时限 pods 的scopes。`Terminating` scope 将匹配任何 `spec.activeDeadlineSeconds` 不为 `nil` 的 pod。`NotTerminating` scope 将匹配任何 `spec.activeDeadlineSeconds` 为 `nil` 的 pod。这些 scopes 允许你基于 pods 在你集群中 node 上的预期持久程度来为它们指定配额。

### 总结

消耗节点 cpu 和 memory 资源的动作受到 namespace 配额定义的硬性配额限制的管制。

任意消耗那些资源的动作能够被调整，或者获得一个 namespace 级别的默认值以符合你最终的目标。

可以基于服务质量或者在你集群中节点上的预期持久程度来分配配额。





## 设置 Pod CPU 和内存限制（旧版本）

默认情况下，Pod 运行没有限制 CPU 使用量和内存使用量。 这意味着当前系统中的任何 Pod 能够使用该 Pod 运行节点的所有 CPU 和内存资源。

这个例子演示了如何限制 Kubernetes [Namespace](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/)，以此来控制每个 Pod 的最小/最大资源限额。 另外，这个例子演示了当终端用户没有为 Pod 设置资源限额时，如何使用默认的资源限额。

### 准备工作

一个集群。

### 创建 Namespace

这个例子将使用一个自定义的 Namespace 来演示相关的概念。

让我们创建一个名称为 limit-example 的 Namespace：

```shell
$ kubectl create namespace limit-example
namespace "limit-example" created
```

可以看到 `kubectl` 命令将打印出被创建或修改的资源的类型和名称，也会在后面的命令中使用到：

```shell
$ kubectl get namespaces
NAME            STATUS    AGE
default         Active    51s
limit-example   Active    45s
```

### 对 Namespace 应用限制

在我们的 Namespace 中创建一个简单的限制：

```shell
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/limits.yaml --namespace=limit-example
limitrange "mylimits" created
```

让我们查看一下在该 Namespace 中被强加的限制：

```shell
$ kubectl describe limits mylimits --namespace=limit-example
Name:   mylimits
Namespace:  limit-example
Type        Resource      Min      Max      Default Request      Default Limit      Max Limit/Request Ratio
----        --------      ---      ---      ---------------      -------------      -----------------------
Pod         cpu           200m     2        -                    -                  -
Pod         memory        6Mi      1Gi      -                    -                  -
Container   cpu           100m     2        200m                 300m               -
Container   memory        3Mi      1Gi      100Mi                200Mi              -
```

在这个场景下，指定了如下限制：

1.  如果一个资源被指定了最大约束（在该例子中为 2 CPU 和 1Gi 内存），则必须为跨所有容器的该资源指定限制（limits）。 当尝试创建该 Pod 时，指定限额失败将导致一个验证错误。 注意，一个默认的限额通过在 `limits.yaml` 文件中的 *default* 来设置（300m CPU 和 200Mi 内存）。
2.  如果一个资源被指定了最小约束（在该例子中为 100m CPU 和 3Mi 内存），则必须跨所有容器的该资源指定请求（requests）。 当尝试创建该 Pod 时，指定的请求失败将导致一个验证错误。 注意，一个默认的请求的值通过在 `limits.yaml` 文件中的 *defaultRequest* 来设置（200m CPU 和 100Mi 内存）。
3.  对任意 Pod，所有容器内存 requests 值之和必须 >= 6Mi，所有容器内存 limits 值之和必须 <= 1Gi； 所有容器 CPU requests 值之和必须 >= 200m，所有容器 CPU limits 值之和必须 <= 2。

### 创建时强制设置限制

当集群中的 Pod 创建和更新时，在一个 Namespace 中列出的限额是强制设置的。 如果将该限额修改成一个不同的值范围，它不会影响先前在该 Namespace 中创建的 Pod。

如果资源（CPU 或内存）被设置限额，用户将在创建时得到一个错误，并指出了错误的原因。

首先让我们启动一个 [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)，它创建一个单容器 Pod，演示了如何将默认值应用到每个 Pod 上：

```shell
$ kubectl run nginx --image=nginx --replicas=1 --namespace=limit-example
deployment "nginx" created
```

注意，在 >= v1.2 版本的 Kubernetes 集群中，`kubectl run` 创建了名称为 “nginx” 的 Deployment。如果在老版本的集群上运行，相反它会创建 ReplicationController。 如果想要获取老版本的行为，使用 `--generator=run/v1` 选项来创建 ReplicationController。查看 [`kubectl run`](https://kubernetes.io/docs/user-guide/kubectl/v1.6/#run) 获取更多详细信息。 Deployment 管理单容器 Pod 的 1 个副本。让我们看一下它是如何管理 Pod 的。首先，查找到 Pod 的名称：

```shell
$ kubectl get pods --namespace=limit-example
NAME                     READY     STATUS    RESTARTS   AGE
nginx-2040093540-s8vzu   1/1       Running   0          11s
```

以 yaml 输出格式来打印这个 Pod，然后 `grep` 其中的 `resources` 字段。注意，您自己的 Pod 的名称将不同于上面输出的：

```shell
$ kubectl get pods nginx-2040093540-s8vzu --namespace=limit-example -o yaml | grep resources -C 8
  resourceVersion: "57"
  selfLink: /api/v1/namespaces/limit-example/pods/nginx-2040093540-ivimu
  uid: 67b20741-f53b-11e5-b066-64510658e388
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx
    resources:
      limits:
        cpu: 300m
        memory: 200Mi
      requests:
        cpu: 200m
        memory: 100Mi
    terminationMessagePath: /dev/termination-log
    volumeMounts:
```

注意，我们的 Nginx 容器已经使用了 Namespace 的默认 CPU 和内存资源的 *limits* 和 *requests*。

让我们创建一个 Pod，它具有一个请求 3 CPU 核心的容器，这超过了被允许的限额：

```shell
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/invalid-pod.yaml --namespace=limit-example
Error from server: error when creating "http://k8s.io/docs/tasks/configure-pod-container/invalid-pod.yaml": Pod "invalid-pod" is forbidden: [Maximum cpu usage per Pod is 2, but limit is 3., Maximum cpu usage per Container is 2, but limit is 3.]
```

让我们创建一个 Pod，使它在允许的最大限额范围之内：

```shell
$ kubectl create -f https://k8s.io/docs/tasks/configure-pod-container/valid-pod.yaml --namespace=limit-example
pod "valid-pod" created
```

现在查看该 Pod 的 resources 字段：

```shell
$ kubectl get pods valid-pod --namespace=limit-example -o yaml | grep -C 6 resources
  uid: 3b1bfd7a-f53c-11e5-b066-64510658e388
spec:
  containers:
  - image: k8s.gcr.io/serve_hostname
    imagePullPolicy: Always
    name: kubernetes-serve-hostname
    resources:
      limits:
        cpu: "1"
        memory: 512Mi
      requests:
        cpu: "1"
        memory: 512Mi
```

注意到这个 Pod 显式地指定了资源 *limits* 和 *requests*，所以它不会使用该 Namespace 的默认值。

注意：在物理节点上默认安装的 Kubernetes 集群中，CPU 资源的 *limits* 是被强制使用的，该 Kubernetes 集群运行容器，除非管理员在部署 kubelet 时使用了如下标志：

```shell
$ kubelet --help
Usage of kubelet
....
  --cpu-cfs-quota[=true]: Enable CPU CFS quota enforcement for containers that specify CPU limits
$ kubelet --cpu-cfs-quota=false ...
```

### 清理

基于使用的该示例来清理资源，可以通过如下命令删除名称为 limit-example 的 Namespace：

```shell
$ kubectl delete namespace limit-example
namespace "limit-example" deleted
$ kubectl get namespaces
NAME            STATUS        AGE
default         Active        12m
```

### 设置资限额制的动机

可能由于对资源使用的各种原因，用户希望对单个 Pod 的资源总量进行强制限制。

例如：

1.  集群中每个节点有 2GB 内存。集群操作员不想接受内存需求大于 2GB 的 Pod，因为集群中没有节点能支持这个要求。 为了避免 Pod 永远无法被调度到集群中的节点上，操作员会选择去拒绝超过 2GB 内存作为许可控制的 Pod。
2.  一个集群被一个组织内部的 2 个团体共享，分别作为生产和开发工作负载来运行。 生产工作负载可能消耗多达 8GB 内存，而开发工作负载可能消耗 512MB 内存。 集群操作员为每个工作负载创建了一个单独的 Namespace，为每个 Namespace 设置了限额。
3.  用户会可能创建一个资源消耗低于机器容量的 Pod。剩余的空间可能太小但很有用，然而对于整个集群来说浪费的代价是足够大的。 结果集群操作员会希望设置限额：为了统一调度和限制浪费，Pod 必须至少消耗它们平均节点大小 20% 的内存和 CPU。

### 总结

想要限制单个容器或 Pod 消耗资源总量的集群操作员，能够为每个 Kubernetes Namespace 定义可允许的范围。 在没有任何明确指派的情况下，Kubernetes 系统能够使用默认的资源 *limits* 和 *requests*，如果需要的话，限制一个节点上的 Pod 的资源总量。

### 接下来

-   查看 [LimitRange 设计文档](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/admission_control_limit_range.md) 获取更多信息。
-   查看 [资源](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) 获取关于 Kubernetes 资源模型的详细描述。
